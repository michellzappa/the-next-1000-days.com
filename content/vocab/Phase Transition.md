---
title: Phase Transition
summary: Critical point where a small change in a parameter or condition causes a significant shift in the system's behavior or performance.
---
In the context of artificial intelligence, phase transitions often describe abrupt changes in the computational complexity or performance of algorithms as certain parameters cross a critical threshold. For example, in constraint satisfaction problems (CSPs), there is typically a phase transition point where the problem shifts from being mostly solvable to mostly unsolvable as the constraint density increases. This phenomenon is analogous to phase transitions in physical systems, like water freezing into ice. Understanding these transitions is crucial for optimizing algorithms and predicting their behavior under different conditions. In machine learning, phase transitions can occur in training dynamics, where slight changes in hyperparameters can lead to a dramatic change in model performance, often seen in neural networks' learning curves.

Historical Overview:
The concept of phase transitions in AI gained prominence in the early 1990s when researchers began systematically studying the behavior of various algorithms under different conditions. This was particularly noted in the study of CSPs and random graph theory, where abrupt changes in solvability and complexity were observed.

Key Contributors:
Significant contributors to the understanding of phase transitions in AI include Scott Kirkpatrick, Bart Selman, and Henry Kautz, whose work in the 1990s on the properties of CSPs and SAT problems helped elucidate the nature of these transitions. Their research demonstrated how algorithm performance could be critically dependent on parameter settings, paving the way for deeper exploration into the theory and applications of phase transitions in AI.