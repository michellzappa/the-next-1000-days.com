---
title: "Foundation Model"
summary: "Type of large-scale pre-trained model that can be adapted to a wide range of tasks without needing to be trained from scratch each time."
---
Foundation models, epitomized by models like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), are transformative in AI for their ability to leverage vast amounts of data to learn general representations that are applicable across many domains. These models are characterized by their large scale, both in terms of the data they are trained on and their parameter count, allowing them to achieve state-of-the-art performance on a variety of tasks with minimal task-specific tuning. The concept of foundation models introduces a shift in AI development towards more efficient, scalable, and flexible models that serve as a base for further specialization through techniques such as fine-tuning or transfer learning.

Historical overview: The term "foundation model" became widely recognized in the AI community around 2021, following the publication of influential research and discussion papers by institutions like Stanford University. These discussions highlighted the emergence and impact of large-scale pre-trained models in advancing AI capabilities.

Key contributors: Significant contributors to the development and popularization of foundation models include research teams from OpenAI, Google, and various academic institutions. OpenAI's development of models like GPT-3 has been pivotal in demonstrating the capabilities and potential applications of foundation models across different fields and tasks.

