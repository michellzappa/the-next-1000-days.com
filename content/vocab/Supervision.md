---
title: Supervision
summary: Use of labeled data to train ML models, guiding the learning process by providing input-output pairs.
---
Detailed Explanation
Supervision in the context of AI, particularly in supervised learning, involves the training of algorithms using a dataset that contains both input data and the corresponding correct output. The model learns to map inputs to outputs through iterative training, adjusting its parameters to minimize the difference between its predictions and the actual outputs. This method is fundamental in tasks such as classification and regression, where the model must predict discrete labels or continuous values. Supervised learning requires a large, labeled dataset and is used in applications ranging from image recognition to natural language processing and medical diagnostics, where accuracy and reliability are critical.

Historical Overview
The concept of supervised learning has roots in the 1950s with the development of early machine learning algorithms. It gained significant traction in the 1980s and 1990s with the advent of more sophisticated algorithms and increased computational power. The term "supervised learning" became widely recognized as a distinct field within machine learning during this period, particularly with the development of neural networks and support vector machines.

Key Contributors
Key contributors to the development of supervised learning include Arthur Samuel, who coined the term "machine learning" and developed early algorithms in the 1950s, and Vladimir Vapnik and Alexey Chervonenkis, who introduced the theory of support vector machines in the 1960s. More recently, researchers like Geoffrey Hinton, Yann LeCun, and Yoshua Bengio have made significant advances in supervised learning through their work on deep learning and neural networks.