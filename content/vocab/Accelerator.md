---
title: Accelerator
summary: Hardware designed to speed up specific types of computations, such as those needed for AI model training and inference.
---
Accelerators are specialized hardware components or systems that enhance the performance of computing tasks by offloading certain processes from the central processing unit (CPU). In AI, these typically include graphics processing units (GPUs), tensor processing units (TPUs), and field-programmable gate arrays (FPGAs). Each type of accelerator is optimized for different aspects of AI workloads. GPUs are highly efficient at handling the parallel processing requirements of deep learning algorithms. TPUs are custom-designed by companies like Google to accelerate tensor operations, which are crucial in neural network calculations. FPGAs offer customizable hardware that can be tailored for specific computational tasks, providing flexibility along with acceleration.

Historical Overview: The concept of an accelerator has been part of computing since the 1970s, but its application to AI specifically gained momentum in the 21st century as deep learning and other AI techniques required more computational power. The use of GPUs for AI acceleration, in particular, became popular in the early 2010s, largely driven by their effectiveness in speeding up the training of deep neural networks.

Key Contributors: While many companies and researchers have contributed to the development of accelerators, notable contributions come from NVIDIA, which pioneered the use of GPUs for deep learning, and Google, which developed TPUs specifically for its own AI workloads. These innovations have significantly influenced how AI systems are designed and deployed, enabling more complex and powerful AI applications.