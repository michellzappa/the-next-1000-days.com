---
title: "Supervised Classifier"
summary: "Algorithm that, given a set of labeled training data, learns to predict the labels of new, unseen data."
---
Supervised classification is a cornerstone of machine learning, residing under the broader category of machine learning techniques. It involves training an algorithm on a labeled dataset, where each instance is paired with the correct output (label). The main goal is to enable the model to accurately predict the output of new, unseen instances based on the patterns it learned during training. This approach is widely used in various applications, such as spam detection in emails, customer sentiment analysis, and medical diagnosis, where the outcomes are known and used to train the model. Supervised classifiers can be broadly divided into two categories: regression models, which predict continuous outcomes, and classification models, which predict discrete outcomes. The effectiveness of these classifiers largely depends on the quality and quantity of the training data, the choice of algorithm, and the feature selection process.

The concept of supervised learning and, by extension, supervised classifiers, has been around since the advent of artificial intelligence as a field in the mid-20th century. However, it gained significant popularity with the rise of digital computing and the availability of large datasets in the late 20th century. Early examples include linear classifiers like the perceptron, introduced in 1958, and later developments in the 1990s and 2000s, such as Support Vector Machines (SVM) and neural networks, which have greatly expanded the complexity and application areas of supervised classifiers.

- Frank Rosenblatt, who introduced the perceptron, an early neural network, in 1958, was a pivotal figure in the development of supervised classification.
- Vladimir Vapnik and Alexey Chervonenkis, who developed the theoretical foundation for Support Vector Machines in the 1960s, have also made significant contributions to the field. Their work on the Vapnik-Chervonenkis (VC) dimension has been fundamental in understanding the capabilities and limits of supervised learning models.

