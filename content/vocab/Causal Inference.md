---
title: "Causal Inference"
summary: "Process of determining the cause-and-effect relationship between variables."
---
Causal inference is a cornerstone of data science and statistics that focuses on understanding the cause-and-effect relationships between variables. Unlike mere correlation, causal inference seeks to establish how changes in one variable directly affect another. This involves statistical methods and models that can control for confounding variables and biases, enabling researchers to make robust predictions about how interventions will affect outcomes. It's crucial in fields ranging from medicine, where it might be used to determine the effectiveness of a new drug, to economics, where it could analyze the impact of policy changes. Techniques like randomized control trials, instrumental variables, and propensity score matching are commonly used to infer causality from data.

The formal study of causal inference began to take shape in the 20th century, with significant developments in the latter half, although the basic concepts of causality have been discussed since ancient philosophy. The introduction of the potential outcomes framework by Neyman in the 1920s and further popularization and expansion by Rubin in the 1970s, along with Judea Pearl's introduction of causal diagrams in the 1990s, have been pivotal.

- Jerzy Neyman and Donald Rubin were instrumental in the development of the potential outcomes framework, which is foundational for modern causal inference.
- Judea Pearl has been a key figure in advancing the field through his work on causal diagrams and the do-calculus, providing a formal language for expressing and identifying causal relationships.

