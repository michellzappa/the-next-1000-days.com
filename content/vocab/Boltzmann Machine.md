---
title: Boltzmann Machine
summary: Stochastic recurrent neural network used to learn and represent complex probability distributions over binary variables.
---
Detailed Explanation:
A Boltzmann Machine consists of a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off. It is designed to learn internal representations of data by minimizing the difference between the input data distribution and the distribution produced by the network. The machine uses a process of simulated annealing, which involves gradually reducing the randomness in the system to find low-energy states that correspond to the best fit for the observed data. Boltzmann Machines are particularly significant in the context of unsupervised learning and form the foundation for more advanced models like Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs). They are applied in various domains, including feature learning, dimensionality reduction, and collaborative filtering.

Historical Overview:
The concept of the Boltzmann Machine was first introduced in 1985 by Geoffrey Hinton and Terrence Sejnowski. It gained popularity in the late 2000s and early 2010s with the resurgence of interest in neural networks and deep learning, largely due to advances in computational power and the development of more efficient training algorithms like Contrastive Divergence.

Key Contributors:
The primary contributors to the development of Boltzmann Machines are Geoffrey Hinton and Terrence Sejnowski. Geoffrey Hinton, a renowned figure in the field of deep learning, has made numerous contributions to neural network research, while Terrence Sejnowski is known for his work in computational neuroscience and machine learning. Their collaboration on the Boltzmann Machine has been pivotal in advancing the understanding of stochastic neural networks.