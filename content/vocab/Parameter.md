---
title: "Parameter"
summary: "Variable that is internal to the model and whose value is estimated from the training data."
---
Parameters are fundamental to machine learning models as they directly dictate the behavior of the model by defining how it processes input data to make predictions or decisions. Unlike hyperparameters, which are external to the model and set prior to training, parameters are learned from the data. For example, in the context of a neural network, parameters include the weights and biases of the network that are optimized during training to minimize a loss function. The quality and complexity of a model can often be directly tied to the nature and number of its parameters, affecting its ability to generalize from training data to unseen data. Proper initialization and optimization of parameters are crucial for the effectiveness and efficiency of machine learning models.

The concept of parameters in machine learning evolved alongside the development of statistical models and computational learning theory, gaining prominence in the latter half of the 20th century. As early machine learning models were developed and refined, the distinction between parameters (learned from data) and hyperparameters (set prior to learning) became more defined.

While it is challenging to attribute the concept of parameters to specific individuals due to its foundational nature in statistics and machine learning, pioneers like Geoffrey Hinton, Yann LeCun, and Yoshua Bengio have made significant contributions to the understanding and application of parameters within deep learning models.

