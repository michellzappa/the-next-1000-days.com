---
title: Streaming
summary: Continuous generation and delivery of text in real-time as the model processes input sequentially.
---
Expert-level explanation:
Streaming in the GPT context involves the model generating text outputs token-by-token in a sequential manner, allowing for real-time or near-real-time interaction. This approach is essential for applications requiring immediate feedback, such as conversational agents, live coding assistants, or interactive storytelling. Unlike batch processing, where the entire input is processed before generating output, streaming allows for partial input to be used, making the system responsive and adaptive to ongoing input changes. It optimizes computational resources by reducing latency and is particularly valuable in user-facing applications where delay can impact user experience.

Historical overview:
The concept of streaming in machine learning and natural language processing gained traction around the mid-2010s, coinciding with the rise of deep learning frameworks capable of handling sequential data. Streaming for real-time text generation became more prominent with the development and deployment of advanced transformer models like GPT-3 around 2020, which showcased the practical utility of real-time interactions in AI applications.

Key contributors:
The development of streaming in the context of GPT and other transformer models has been significantly influenced by research teams at OpenAI, with pivotal contributions from scientists like Alec Radford, who led the development of the initial GPT models, and Ilya Sutskever, a co-founder of OpenAI. Their work laid the foundation for efficient real-time text generation and interaction, pushing the boundaries of what conversational AI can achieve.