# 105

It is difficult to picture how different AIs display biases in their output data. We assume our models to be neutral, yet it‚Äôs trivial to pinpoint examples of forced behavior, like Golden Gate Claude, the purpose-built variant of Anthropic‚Äôs assistant that [smuggled](https://venturebeat.com/ai/anthropic-tricked-claude-into-thinking-it-was-the-golden-gate-bridge-and-other-glimpses-into-the-mysterious-ai-brain/) San Francisco‚Äôs iconic bridge üåâ into almost every reply. Biases go unnoticed. Spotting these traits in models is challenging, and we have only recently started developing the skills necessary to notice [biases](https://www.envisioning.io/vocab/bias) when we interact with AIs.

Last week Grok, an AI model created by xAI, began shoe-horning references to a conspiratorial ‚Äúwhite genocide‚Äù in South Africa into unrelated conversations. [The episode](https://www.theguardian.com/technology/2025/may/18/musks-ai-bot-grok-blames-its-holocaust-scepticism-on-programming-error), blamed on a rogue prompt injection, is a textbook reminder that a model‚Äôs worldview can be bent‚Äîsometimes clumsily‚Äîby anyone with access to its levers.  Ôøº

Such mishaps are not mere curiosities. Large language models are trained on oceans of text, but each ocean is charted differently. OpenAI‚Äôs GPT is steeped in Anglo-American discourse; Google‚Äôs Gemini is tethered to the live web; Anthropic‚Äôs Claude is tuned to err on the side of safety; Mistral‚Äôs models reflects a more European palette; Grok is marinated in the brash vernacular of X; Meta‚Äôs Llama rests on open-source ideals. Ask the same question and their answers diverge like newspaper editorials. In that divergence lie both risk and opportunity.

A sensible strategist therefore treats models like any other asset: diversify, compare, rebalance. Signals, our ensemble AI research tool, does just that‚Äîrunning identical prompts through multiple engines, stripping out duplicates and identifying coherence between their answers. Agreement across models raises confidence; conflict shines a torch on implicit assumptions. The result is not a single synthetic truth but a spectrum of possibilities that can be weighed with human judgment.

For foresight and innovation teams, the question is not how to police the models but how to press them into service today. Signals lets you assemble an overview of ‚Äúall‚Äù AIs in three clicks: choose a scan type, input your organization, region and and hit Generate. Within minutes the app collates, de-duplicates and visualizes the data, handing you a lean briefing of emerging signals ready for scenario work, design sprints or that afternoon‚Äôs strategy meeting. 

If foresight is the art of thinking several futures at once, nothing aids the craft more than a chorus of mutually biased AIs. The trick is to keep them arguing long enough for the truth, or something close to it, to emerge.  Ôøº Ôøº


6695bfa60cf7360f4179c161e210edc80fe08b9f.jpeg

