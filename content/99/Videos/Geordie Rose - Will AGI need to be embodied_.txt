hey everyone welcome back to the podcast i'm especially excited about today's episode because we're talking to jordy rose who is the founder of one of the world's first quantum computing companies is a company called d-wave that many of you may already have heard of now jordy really is a hard tech pioneer at the time he founded d-wave back in the 90s next to no one was paying attention to quantum computing at all and he's also been ahead of the game in a completely different area and that's reinforcement learning in fact at the time i first met jordy back in the mid-2010s he was already very focused on the potential that reinforcement learning would have for augmenting the capabilities of existing ai systems which is why he then went on to run another company called kindred ai which developed one of the first concrete applications of reinforcement learning in industry majority's ultimate focus is now on agi something that he's working on at his new startup sanctuary.ai now sanctuary's approach is based on a really exciting and unusual thesis which is that one of the easiest paths to true agi is going to be to build embodied systems so ai's that actually have a physical form and structure and they can interact with a real physical environment so we're going to be talking about that thesis as well as other questions around what happens when agi hits how humanity can best prepare itself for that event and the broader ai alignment and ai safety problem so i'm really looking forward to diving into this conversation i hope you enjoy it as well hi jordy thanks so much for joining me for the podcast uh thanks for having me should be fun yeah i'm really excited to do this uh obviously we've spoken before uh quite a few times about agi i'm looking forward to this conversation to have like kind of a a broader chat not only about your your background but also what you're working on right now which is very agi relevant for context you were one of the pioneers of applied quantum computing which is or was your main area of focus at a time when almost no one was paying attention to it i'd love to start there so can you can you tell the story of how you became interested in quantum computing and how that led you to start working on what would eventually become d-wave uh yeah you bet so i um i was at ubc in grad school doing physics and the uh the area in which i was doing research was called condensed matter theory which is the study of materials at at low temperatures and in my case the uh the particular stuff that i studied was a material called the molecular magnet and the if you can think of a molecular magnet as being kind of a lattice of tiny little magnets and those tiny little magnets um flip around a lot and how they do that uh it gives these materials their properties and it turns out that that as you cool a material like this there's a very abrupt transition between these little magnets acting like quantum mechanical objects uh somewhere around 300 millikelvin so that's 0.3 degrees above absolute zero so very very cold temperatures but the the behavior of these things is remarkably different below this temperature and uh it occurred to me back then so this would have been like late 1990s that you might be able to build a computer out of something like that and above at that time there was some academic research about uh using materials like this is potentially new kinds of probes of crossovers to things like compute computational complexity problems and things so things that were not traditionally viewed as physics problems and again around that time there was some work at mit to take the uh these basic ideas that arose from the study of materials and and uh and write them in the language of uh this thing called quantum computation which back in the late 90s was nowhere near as commonly understood or talked about as it is today back then it was uh it was an academic thing almost entirely um so i i i got obsessed by this idea of could you actually build a computer out of something like this and uh um that ended up being the inciting motivation for starting d-wave so when i when i finished my graduate work at ubc i i founded d-wave uh together with a few other people and we started thinking about uh how would you go about actually building a a quantum computer and that was in 1999 when we founded the company well yeah and since then d-wave obviously has become known as like a leader in the quantum computing space you've also gone on to focus more and more on machine learning which now is basically your entire focus and specifically this question of how to build agis i think in our conversation i think one of the most important or interesting through lines has been you have this interesting embedded systems thesis about sort of what it would take to get to an agi would you mind exploring that thesis a little bit kind of unpacking it and why you're thinking about agi in that way sure yeah i would i would characterize my my technical interest is not really about machine learning because i think uh we haven't said anything controversial yet so i'll start cognition of the sort that all animals have including humans uh learning is a part of but it's a small part uh most of what we do is not learned and the the the vast bulk of the uh the uh the equipment that we bring to bear on the problem of getting around the world is learned in a sense it's learned over evolutionary scales by evolution trying different things and the ones that are you know able to succeed in making copies of themselves through time uh those uh trials if you will the the the experiments and hyper parameter settings uh that are successful um make it through and get passed on so my my view and i say like my is kind of the royal my because there's kind of a group of us who've been pursuing these types of ideas now for more than a decade is that the uh the correct lens through which to look at general intelligence is uh through the lens of the the biological lens so we have this example of what we think of as general intelligence which is human-like but we have lots of other general intelligences around us virtually every biological organism is general in some sense they have to deal with the peculiarities and weirdnesses of the real physical world we all do everything from bacteria to flatworms to oak trees to us all share the same physical universe and we have to find our way through it so there is a sense in which uh all evolved creatures have a type of general intelligence and the the thing that binds them together the reason why i view that way is that they all have a situation in the physical world so all of these things have what you could think of as a body a vehicle through which their actions are taken and a vehicle through which information about the world comes into them through their senses and uh this picture of um a a thing that's situated inside an environment would be common to anybody who studied reinforcement learning because this is kind of the central premise of of the world view that reinforcement learning represents is that you have you have observations which are partial in the sense that you can never know exactly what's out there and then you have uh as an action space which are the sorts of things that you can do and in a physical object that means movement uh usually i mean there are there are some there's some uh things that we can argue that you could do you could take actions that aren't movement um and that's fine but the basic premise still holds is that you uh you sense the world around you you have partial understanding of what it is and then you you move or you take an action and then uh the the fitness of the sequence of actions you take somehow matters and in a biological system it's related to survival and you know the other things that we need to do to get ourselves around the world without killing ourselves or otherwise making a mess of things so the the premise that we've always followed is that the the clearest path to cutting through this extraordinarily difficult problem is to mimic uh biological systems and the types of intelligence that they need to navigate the world and so we build robots and we build control systems for robots and these control systems for robots to the extent that we can mimic the um the types of functionality that you'd imagine you'd need not one-to-one you know we're not copying the brain but we're thinking about what properties of the brain are required for you to for example know how uh to reach out and grasp an object or to know how to walk on uneven terrain um or to know how to reason about the world all of in the ways that we believe we do or we we suspect or you know the sorts of ways that humans or other animals do it so that's basically the thing i think it's it's it's kind of obvious and i think the reason why uh you know we are a little bit ahead in this game if we are is that we committed to it uh early you know we've been doing this now for um for about 10 years uh we have one uh exit from a product that is was used use these ideas in some way and we continue to work on it and try to generalize these ideas actually yeah you were just telling me before we we started here that your last exit kindred actually is pretty recent and i think it'd be useful to provide a little bit of context on that because it is so core to this whole story yeah sure so the the origins traced back to my last days of d-wave so i had a team around me that was working with our customers and everything that anybody at that point did with the d-wave hardware had to do with uh with a type of ai problem which is sampling over probability distribution so there was a particular technical thing that we thought we might be able to do well and we were in contact with a bunch of people who were thinking about that problem more or less all researchers uh and they tended to be the the kind of the cream of the crop because this was an exotic piece of technology and um people were thinking kind of about the bleeding edge so one of the members of my team uh suzanne gilbert um hadn't had a series of ideas about how you turn some of these advanced ai ideas into control systems for robots that would be uh biologically inspired quote-unquote uh which which means that they would be intended to be control systems that would allow a system to navigate the real world and uh i thought they were brilliant ideas that needed to flourish and dwave was not a robotics company and never would be so i encouraged her to to leave and and start another company so that she founded kindred in 2014 and in the months that followed she called me up and asked me if i would be interested in coming and running it with the the premise being that we would build a new type of control system for robots that could dramatically extend what they could do and potentially even in some limit create general intelligence so i was absolutely fascinated by this as as a thing i've always thought that it's kind of obvious that uh the uh that the sort of thing we call intelligence is much more important than what we call computation computation is a tool it's a thing that you use to achieve some objective but the setting of the objectives or the somehow the figuring out of what questions to ask is clearly more important than the tool so i was more interested in that so i left and uh we ran it together for four years up until 2018 and uh we we we brought it through three rounds of financing up to a 100 million dollar valuation in um would have been late 2017 uh we had supreme of the crop in terms of investors and a very very good technical team including some of the founding members who are who are absolutely excellent and uh we've we turned the the spotlight of all this technological stuff we've been working on on a on a problem that really resolves down to one technical issue which is can you build a robot that can manipulate and grasp real world objects so this sounds really easy right because any human can do it without thinking about it but this idea of doing it in the real world turns out to be at the heart of why it's hard to build robots that can you know make their way around the physical world so we focused on that and in particular on a specific implementation of it in the e-commerce distribution center ecosystem there's a series of places in these large distribution chains where things that are not easy to specify in advance because they change all the time you know what you buy is not the same this week as it is next week and there are millions and millions of these things that you can buy so building a robot that could manipulate and grasp any of them and we did that it was the first robot to use reinforcement learning in production this was one of the ideas that suzanne one of many ideas that she had woven together in this kind of cognitive architecture that she developed uh and uh it was very successful last i heard they'd done more than a hundred million successful uh grasps of objects in the wild which is considerable you think of it as an episode in reinforcement learning yeah positively labeled episode um and the uh the company was acquired for uh 200 pounds a couple days ago uh which in canadian dollars yeah 200 million pounds uh which is about 350 million canadian um by a british uh a british company so the the way that the just getting back to the agi question so how does this intersect so the underlying ideas that were used to generate that that product um can be used to potentially use the same ideas but building up a robot that differs in the sense that the robot is a general purpose robot so this is a this is a new thing that doesn't quite exist yet although we've been working on it pretty hard for many years is the idea of building a physical system a a robot that isn't designed to do one thing but it's designed to do anything a human can do and if you can apply the same kinds of ideas that suzanne developed in the kindred context in the context of a general purpose robot then you can deploy it in any place that does work so it can make clothing it can serve you coffee at starbucks it can deliver your food it can make you your food it can take care of you when you're older so the idea of building one robot that can do all of those things is the central premise that is is behind it and related to our approach to body cognition this idea that you want to be able to build a physical body for something like a mind mind being a robot control system that allows the physical robot to be able to do and be in the world in the same way that we are and navigate it and understand it in the same way we do and what is the what's the nature of the world model that gets built by i mean to the extent you can talk about it obviously a lot of the stuff is going to be under wraps but um are there are there things you can say about that world model especially in the context of you know today's ai systems which are very narrow like we look at even a system like gpt3 which has a lot of really impressive few shot and zero shot behavior getting to the point where we have like one system that can do a whole bunch of different things does that require like a paradigm shift beyond the gpt3 like scale deep learning massively is there is there something else going on under the hood here well i mean i so i've always thought that the uh you you get what you pay for right so if you if your objective is to build uh something like gpt3 where you uh you start from the premise that there are no priors essentially you know what you're doing is just taking all the the data that you can find and then and then processing that data and extracting the statistics of the data so if you start from that premise that's what you'll get you'll get a system that if you do it right will be uh absolutely great at that thing which is learning about the statistics of the data that you send it now that to me is a a very interesting thing but it has nothing to do with general intelligence and the the and again let controversy alert uh there are way different ways to think about how you understand what intelligence is and then how you go about building it so i know i have utmost respect for those guys i think that what they've done is technology it's it's it's just an amazing thing you know that gpt3 thing is like alien technology you know you don't even know what it is yeah uh you kind of explore it experimentally uh not from first principles almost so the um uh i felt very weird actually when i started playing with it because it is really one of those uh you know the blind man in the elephant thing where it's unclear what it is even after you experiment experimented with you know i understand what it is you know i understand how it was built and all of that but but what the artifact that it created was something very very strange and this idea of it being alien technology appeals to me because it's kind of like if aliens built something and we didn't know why and they just gave it to us and it was that i wouldn't be surprised yeah anyway so so the my my approach uh or our approach to how to do this is is very different so ours is almost on the other end of the spectrum where if you're going to build a robot that actually can do things in the real world in the way that you'd expect a squirrel or a you know a bird to uh the idea of learning over data with no priors makes little sense outside of an academic environment so there are people like rich sutton who i admire and respect almost more than anyone else in this field who take this perspective that what we really want to do is uh okay if he calls it the bitter lesson or something like that is that we want to avoid um we want to avoid putting priors into systems and try to learn everything from scratches to the extent that we can and if we do that then we're kind of limited by how smart we are and our computational power and if you take take it for granted that we're going to be as smart as we can be the limit is computational power and our ability to do things in the with intelligence in the world is ultimately limited by our access to compute cycles so that's one perspective i don't take that perspective at all my my perspective is that we have uh as much evidence as we need about how uh the priors need to be structured in order to hang learning on them where it's needed so machine learning and particularly things like supervised learning have a role in building a a cognition for example visual systems converting pixels into representations clearly that's going to be something connectionist it's going to be a neural net of some sort but what you do with the resultant representations is clearly in my view not just about learning you know there is something going on in our minds that we know quite well you know if you if you think about the intersection with neuroscience there's lots of clues about the ways things have to be uh i'll give you one clear example of this so we uh and it goes to your original question about like how do we think about the world models that we built so the first thing is the robots have world models so that's that's one thing is that the the the internal life of one of these cognitions has a simulation of reality in it not at all obvious it's not the way that all uh you know approaches to ai work of course but in this type of a system a control system for a robot the internal model of the world has to be high fidelity representation of the actual world so think about the the matrix right the movie so the people in the matrix at the beginning have such a high fidelity uh immersion in this simulated world that they can't tell it's not real and a robot needs to have something very much like that an immersive internal model of the world that is not only about what you can think of as representations or in the game language the scene graph of the environment the things that are kind of platonically there and their properties but also the the the renders of those things which visually you can think of was what you're seeing so step back a minute and if you've got say an object on your desk or you're staring at something there's a thing philosophically speaking how do you think about this i this this what's going on so there's a thing there you believe that but what your eyes are seeing are a render of that thing the light bounces off of it and we get some picture of it on our retinas and then something happens something happens in our head uh the the way that our internal models work in these robots that we build and also to a certain extent the ones that kindred although to a lesser extent it wasn't as necessary is that there's a an internal model of the world you can think of as like a video game which is a one-to-one representation of what the thing is actually looking at as it looks around and it and the pixels that hit its cam the light that hits it count its camera resolves into something that it might be able to recognize like a cup that generates a hypothesis that there actually is a cup there in its internal model and in the video game such situation you could think of as a cup popping into existence in your video game exactly where the robot thinks it is and that cup is then compared to the input from its pixels and another important point about our perspective is that unlike nearly everybody else who does this sort of thing to us the simulation the internal model of the world is the primary model it's not the senses so uh it's very easy to fool yourself into thinking that what your brain imagines is happening is what you are feeling touching smelling seeing and hearing that's not the way we approach the problem at all the way we approach the problem is that what you think you're seeing smelling touching and hearing is actually the inside model of you the model you have in your head touching the simulated stuff in your head and your senses are a secondary thing which ensures that that model doesn't go out of lockstep with your surroundings so this this this peculiar notion that you know people have referred to sometimes as like uh you know we we exist entirely inside this piece of bone we take it literally and the the senses then become checks they don't become primary they're secondary they're they're kind of like the anomaly detector signal that you use to ascertain whether or not your internal model of the world is in accordance with what your senses are telling you so this flipping of the of the of the game from senses or primary and we should be building supervised learning systems to see if there's a cat in front of a robot to know the internal model is primary i have a model that there's a cat at that position and now my senses are telling me whether or not i'm right that flip is a core part of how we think you have to think about uh cognition in this in the context of the body cognition if you're going to build a robot that can make its way around the world it needs one of those things and you need to look at it in that way or else you're going to get what we have today like look around you where's the robots they're nowhere unless you count cars which you might but this premise that we have machines that can do a bunch of stuff is just not been uh seen and like in quantum computing the ev the evidence in front of your face that you do not have it is evidence that the state of the art and the way that people are pursuing the problem is wrong so it's not that the ways that robots are built are somehow going to gradually get better and somehow magically going to turn into something else that won't happen you know robots have been moved around the same way as you know barring the kindreds and the covariance and other people who are trying to do things right the same way ever since there's been machines there's something new that's needed and so the our what we're working on is is the extension of the kindred hypothesis to the general case to try to build a machine that has a mind uh of a sort and um it's so it's able to navigate the world in the same way that a biological creature would and be able to do all sorts of things that are completely beyond the scope of any machine that's ever been built that's that's so fascinating and it sounds like there are basically two different layers then to the to the thesis here the first one being okay we need an embodied system because that's one part of the evolution prior let's say like we we know that we've evolved to have bodies and that must be part of this and then another part is this almost like i don't know whether to call it like metacognitive or almost buddhist or whatever you want to call it but this perspective on the difference between numina like the object that really exists and then our perceptions of it that seems like it's its own separate prior is that sir how you're thinking about is it two different uh camps or are they are they also linked in a way that maybe i can't think of well they're very they're definitely linked because the the fundamental reason for thinking about the world in terms of models and not senses is connected to function and if you can think of it like this like how do you actually pick an object up and it's not as simple as you think so if you're a sense first person i'll just give a silly example but it matters in robots so let's say a sense person sends first person and i've got this thing called a cup right so i'm sensing it by looking at it now when i reach out to to grab it my hand includes the object so for a while while i reach out to grab it my senses can't tell it's a cup yeah so how do i know it's still there and how do i so this is a silly example but it's an example of of of the of the the first crack in a sense first uh uh perspective if you're trying to build a robot that can navigate the world if i if i'm solely running say clarify's cup detector on my robot at some point in the grasp procedure that cup detector is not going to fire anymore and the robot is not like us right the robot doesn't know quote unquote it's cup because you didn't tell it that there's something called object persistence which is the fact that we know in physics uh f equals m a and all of that means that that thing is probably not going to move unless something else happens that i probably can detect like you know a gust of wind or something so the the the the that that very simple example which is that the sense first approach to the world clearly is wrong in our case for something even as simple as reaching out and grasping an object uh means that you have to add something else and so if you're a roboticist wanting to solve that problem you might say yeah well we'll just we'll just put an entry in a database that says somewhere that if i see a cup somewhere i'm going to leave it there but then what you have is a cascading hack that ends up not scaling if you start just adding pieces like this one at a time you're never going to pick that underwear up out of the bin because there's going to be something some edge case your brittle system that you tried to hand code will not work so instead let's say we we take the the the model first view you bake all of the priors of the world into your model because we know what they are there's this you know this it's it's absolutely peculiar to me that we've known newton's laws for hundreds of years we know how the world works we don't need to learn it we it's given to us at least at the scale in which humans act you know maybe quantum or general relativity there's some questions but the physics of cups are absolutely known we don't need to learn anything about the physical world we already know everything there is to know so if you can build a simulation that can essentially just be an f equals m a solver that's good enough uh you can pick things up in simulation you can do things like the prior is the object stays there until it's moved and now my senses are just verifying to some extent that the object is still there so if my hand partially occludes the cup i don't care that my senses can't tell it's a cup it can still see some pixels and so if i if my my model says there should be a cup there and i can still sort of see a little bit it's enough so this idea of putting the model first solves an enormous swath of problems in the uh navigation of the world so they're back to your question about priors so the cognitive system that runs in a robot that's supposed to be like a humans contains dozens of things like this where you have you specify in advance i know all of these things it might be an engineering challenge to run them in real time like for example the types of physics engines that run in uh like say the phys x engine that nvidia has or others of its sort they're not designed to solve physics what they're designed to do is solve the physics that you would look at if you're running a game so there are engineering challenges in going from the physics models that are the sort of you know the right ones uh that include things like friction and surface stuff and all that uh but they're engineering challenges and so you know like everything else i've ever done in my career the the the question of how you peel back a really complicated problem that isn't obvious how to solve is you first divide it into pieces and then you take each of the pieces seriously and then you do it right a priori you don't use all the stuff that people have built before if it's wrong and uh you know in quantum computing and in in agi call it agi i don't even know what to call it but this idea of building reasonable control systems for robots that the dogma the state of the art has failed and because of that following the same path that people have laid out before will fail also now you'll get successes like kindred where you can make a few billion dollars here and there and that's fine but that's not that's not what i care about what i care about is trying to solve this problem how do our minds work well enough to build them i view that as being the single most clear holy grail of all technology you know if you can solve this problem it's bigger than any other problem that humans have ever conceived of everything we've ever done or thought believed dreamed it all lives inside this thing we've got to understand it why don't we it really bothers me that we're all all seven billion out of us are carrying this thing around with us and we don't know how it works doesn't that bother everyone else i mean it's a travesty we need to understand how this thing works if we if we're gonna if we're gonna call ourselves intelligent as a species i think one of the tests of whether you get to call yourself that is the species has to understand where that thing comes from and right now we don't so let's let's let's change that i think it's time for uh for our the our community uh machine learning researchers ai researchers to really take and roboticists as well to really take for for uh seriously the question of can we actually solve this problem and if we if we were how would we do it so we have our own angle uh it's not the only one uh there are the data driven approaches of you know rich sutton and elias sutscover and these other folks who you know are doing it a different way i think that it's time for us to to solve this problem and when we do there's going to be unlocked potential of the sort that's never happened in human history i mean the steam engine is going to look like a bump in the road compared to our ability to harness this particular type of technology we should do it and uh the time is now you know people are always thinking oh it's 20 100 years out those ideas that something is 20 years out have no weight in my mind because the the the good friend of mine eric ladazinski a guy who's i used to work with a d-wave he said this thing and it stuck with me said if you can do it in 20 years just do what you would have done then now and that that's kind of a really deep thing is that if you can figure out what the right things to do are you're not necessarily bottlenecked by a span of decades so you just have to be smarter about how you attack the problem be efficient and and solve the right kinds of problems in the right order and you'll get there a lot sooner than people thought i'm not saying that agi is easy it isn't it's way harder than anything else i've ever worked on like it's orders of magnitude more difficult than quantum computing like orders of magnitude it's not even in the same ballpark but uh i think it's also concomitantly more important like i said a computer is just a tool it just answers the questions that we post to it if it's built correctly but this thing that we're trying to understand which is how we understand the world what are we what is do we have a purpose if so how could we find out what it is um is is there is there any meaning to any of this and if so what is it what is the eventual outcome of all of this if you go far enough into the future all these questions are things we should be able to answer and um you know that that kind of answering those kinds of questions trumps computation in you know some massive way well and this ties into i guess some of the bigger questions about the future of agi like one of the common concerns is that humans don't yet know the answers to a lot of those questions what gives us meaning what do we want what's our morality and as we start to look at embedding some of those moral frameworks in machines either in the form of priors or in the form of a data-driven approach like the one openai has taken as you've highlighted it it sort of forces us to have the the philosophical rubber meet the road are you are you concerned about like for example what's been called the alignment problem this idea of aligning machines with human values is that something that you think is surmountable or that we'll have to face soon well i mean there is this this is a tough one because there's all sorts of ways that you can create technology that ends up having uh unforeseen consequences you know we see it all over the place you know like social media is a prime example social media in and of itself is neutral you know it's it's a technology that allows people to communicate in a certain way but like everything else in any con sufficiently complex system it's very difficult to figure out what's going to happen when you mature a technology like that and it's going to have negative consequences as well as positive ones nothing is ever cut and dried you know everything is gray the looking at social media is a case study of how a powerful technology has good and bad parts and how to how a society should uh deal with this sort of a thing i think is a good way to start having the discussion so one of the things that occurs is that you know take the social media example so social media has brought home a point that i think we sort of suspected but is made clear that uh the human mind is extremely vulnerable to certain uh column idea pathogens so if you yeah if you introduce an idea in the right context you can get people to believe it no matter what it is so i've often when i was when i was younger and even to a certain extent now you wonder about something like nazi germany like how could that happen how could all of those people uh agree to some things that you know any sane person would say how how are we allowing this to happen this has to end so how does that how does that how does that work and i think that the the the social media experiment has shown us that the mind of humans evolved in a particular type of landscape where it was very important to us to be part of the tribe if you were if you were kicked out of the tribe for through most of our evolutionary history you'd die because we need as a social animal with very little physical ability we need to cohere and work together in order to attain so social media hijacks this it creates tribes where your belonging to the tribe overwhelms your cognition so your irrational thinking about something gets overwhelmed by your need to belong and if a million people seem to be saying the same thing you are almost powerless to disagree and this is not just about uh say the right-wing trumpist stuff the left wing is just as bad there are all sorts of idea pathogens that that every political stripe um adheres to without thinking through the consequences of them and often they sound good on the surface but if you start thinking about them they rip apart so the social media thing is is both good and bad now if you think about like this idea about how to treat ai as it emerges so ai is going to be an extremely powerful thing that i think in our lifetime will lead to machines that we would consider to be sentient so we will have discussions about whether the systems that we build within our lifetimes i'm not going to put a date on it because you know you can't predict the future but my sense is that the problems are not insurmountable in some period of time that we can we can count so uh let's say that happens we're in a very different space where we're no longer the the top dogs in terms of you know some things like you know ability to reason or something like that so then what and i think it's the same thing is that we have to step back and ask um what is what is a salve or a bomb that gets that helps us navigate the good and the bad of the future and as much as i hate to say it i think this really traces back to um to people learning at a very early age that there's a lot of value in skepticism and not believing what you're told so if there was one thing that i think would be uh the antiviral against all idea pathogens it's to never believe anything you hear so if you start from this premise that no matter what anybody tells you whether it's your mom or a political figure or your advisor when you're a phd student or your teacher when you're in grade school everything they tell you question it and don't believe it and then figure it out yourself so that second part is hard but i think this is the this is the inoculation against the the idea of pathogens is that think that anything anytime anyone is telling you something they're trying to get something from you that isn't in your best interest even if it's just for you to agree with something so don't you'll put put your put your own brain before the brain of the tribe and don't believe anything you hear even if it sounds good on the surface and think it through and make up your own mind and even then you know you're not going to necessarily get good outcomes with ai i'm not so naive to think that you introduce something like this into human society that things couldn't go sideways but i think that the the thinking for yourself part is the is the key to inoculating against all uh bad outcomes because i give you something you could use it for call it good or bad those are relative terms but let's say there's some general sense of what that means uh you have to be able to reason about your situation well enough to be able to make good decisions about the technology its use its context and all that so for me you know my mental model of the world is that we are about to share the planet with a a cambrian explosion of uh new kinds of entity they're not going to be like us they're going to be as alien as gpt3 or as humanlike as robots that are supposed to be like us um there's gonna be thousands of these things running around and we are no longer any illusion that we had that were kind of quote unquote in charge which i hate as that's the one of the worst idea pathogens there is this idea of control um being a good thing is uh is gonna go away and it will have to because the we won't have a choice and it's not bad you know the this idea of being in control is such a vague abstract notion that i i'm not sure even what it means you know i hear this all the time as an objective for how to deal with emerging ai technologies is that we have to be in control that's really not very good as an idea on several levels again if you peel it back it doesn't make any sense you're not in control now i don't control whether i pay taxes go down the list of everything you do today and you tell me where you have agency it's an illusion and so the question isn't whether or not you're giving up control the question is whether or not the future is better than the present that's the key question and i view the emergence of these new kinds of technologies as being a kind of flourishing it enables us to do things that we couldn't have done before and a a human-like ai can be put inside a body that can survive space if we really want to go and populate mars and thrust this is a key piece of that story you know can you put a physical human body on mars sure can you send them to the next star system probably not so the the the the flourishing of the kind of thing that we are if we want that to to go on which i think everyone does we don't want this great experiment in cognition to end so how do we do that well we we we flourish we diversify we we we branch and all this branching stuff doesn't mean that you're lesser it means you're greater you created all of this stuff that the future is going to hold you were the the progenitor of this massive explosion in things like us you know if you believe that being alive is a good thing if you believe that being conscious is a positive and you have the power to give dumb inert plastic and metal that gift shouldn't you you know i think if the if you think about the world as being made up of things that think and enjoy and live and things that don't and we could give the things that don't the same gift isn't that a moral imperative that we do that so this this business about the the the moral and ethical considerations about ai i think often end up being too provincial you know we think about today's technology where we're talking about is like bias and data and stuff like that i'm not interested in any of those conversations to tell you the truth because that the the real question isn't about the bias of data the question is about when we get to the point of creating something that thinks what is that going to be like because then then we're talking about like you know the uh the a nuclear bomb versus like try like a spark you know the difference between the two in terms of its impact on human society they can't even be compared they're on different scales yeah the notion of an intelligence explosion i think it's really interesting when it especially in the way it intersects with what you're describing here which is like yeah we have these moral agents which are you know ais that are advanced enough that have cognition we do have to think of them as morally valid agents and i guess the the question is like so when it comes to an intelligence explosion one of the concerns is that an intelligence explosion might actually lead to the eradication of human consciousness or cognition that the coexistence of human modes of thought and super advanced ais that might be able to self-improve that might be able to kind of reach heights of intellectual capacity and just rich cognitive consciousness that we can't even imagine might be impossible like are you concerned about that about the coexistence uh sort of not being a possibility or an option for whatever reasons yes but the the again the what i think the right way to think about this is you're choosing amongst options so it's it's not a simple matter of saying you know there could be a bad outcome if this particular thing happens because if you ask is there bad outcomes in any potential future the answer is yes if we didn't create ai and we just continue on a certain path what's the chance that we're still around in a couple hundred years i don't think it's very good yeah so the the the i think again the right way to think about this is not is there a potential bad outcomes of potential of past that we could take that's wrong because there are bad outcomes in all of them and it's a fallacy to think that just because you can point to one bad outcome in one then the that that's the outcome is first you know guaranteed and somehow the only bad thing that could happen there are a lot of bad things that could happen again it come it comes down to what do you value so i value uh the thing that we are you know i would rather be alive than you know a rock and by the way not everybody believes that there's this great book i always recommend to people called uh it's by thomas legatti it's called the conspiracy against the human race so in it he argues very coherently that consciousness is actually one of the worst things that could you could possibly possess as a piece of matter uh and and gives arguments for why that is but if you take that argument aside and you don't believe it and you think that being conscious is a good thing then presumably we want more of it now this thing about intelligence explosion by the way i don't believe that the conventional use of that term will ever come to pass i think that what we call intelligence is situational intelligence is the ability to achieve goals in a specific type of environment and it's not a number so this is super important it's not a thing that just grows it's it's a tool that you use to get what you want and so what you want in a biological system is ultimately driven by you know the evolutionary pressures in a machine it's much more complicated because at least at the beginning uh that's set by human engineers so if we want to you build a reinforcement learning system that learns to play go we bake in the prior that winning that game is what we want we don't ask the machine you know how do you feel about playing go you tell it and the kind of intelligence in that example is the ability to achieve the goal which is to play go well it has nothing the intelligence itself has nothing to do at least at some level of analysis with the goal itself so i think that this intelligence explosion is not a good idea i don't think it will actually happen in a sense of escalating capability i think what will more likely happen is you'll get the cambrian explosion type of thing where you have thousands of different kinds of intelligence which are you know algorithmic structures for achieving different kinds of goals and they'll all be suited to different niches so gpt-3 as an example is great if you're uh if you're a a creative person trying to come up with names for products so at some point in the future probably everybody in marketing is going to go away and there's going to be an ai based on the statistical properties of language and the something we know about persuasion and all it will do is create ad copy and it will be designed such that that's what it does and even though you could say wow this thing is really sophisticated and boy does it do that well it's going to be an alien intelligence that doesn't ever want to turn the world into paper clips or any other of these ridiculous scenarios it's structured such that that is what it does and they're going to be other kinds of ais built that have other goals and they can get better at achieving those goals but unless something fundamental changes about the way that we think about machines changing the goals is a human exercise so if you have a human want to change the goal of the machine to let's go shoot everybody that's not a failure of the technology don't blame that on ai or cognition blame that on the person who thought that was a good idea and so i think that the the the again it comes down to this business that the the technology is gray you know it will create huge opportunities and flourishing for some people it will create absolute bad things for others it's always that way every technology ever in the history of the world has been like that the question is where does it end up in the balance and i think that investing in understanding our minds in the balance is going to be the single best thing that has ever happened to the human condition uh it will be for all of the potential negatives that could happen and i'm sure some of them will come to pass and many of them we won't be able to predict in the main we will think that we were absolute barbarians before this transition point happened when we finally figured out how it works it's interesting yeah i mean we certainly do think that about our our past selves in a lot an awful lot of different contexts so in some ways it wouldn't be it would be such a shock to find yeah like 50 years later when we have this technology thinking back oh my god we used to do dialysis we used to put people in in prisons for crime and things like that before we had better solutions um there's even more more even more uh radical there was a time when our descendants didn't have language right so we don't we don't even think about those those people as being human but they were in the sense that they were our descendants you know you go back far enough in time and are directed that you know your great great great great great grandmother didn't have language and so we view these great dividing lines in our progression um you know as being kind of like things that are done with you know we okay now we're now we're we've learned how to speak in in you know in python okay we're done yeah no there's all sorts of other dividing lines and i think the dividing line between a an entity that can understand the way that it's its internal working of the of the way that it it processes the world and before is as great of a divide as an entity that doesn't have language and it does maybe even a greater divide and the fascinating thing which makes me believe in the simulation hypothesis by the way is why is it that we live right at the time when that transition is happening it seems unlikely but here we are yeah no i agree and i think that there's there's an interesting through line to what you've been talking about um including with the reference to sort of the concentration camp guard in nazi germany let's say when you tie it back to this idea you know we say oh well our ancestors didn't have language they weren't even human and the moment we say that we kind of define this out group and we feel i don't know if disgust is the right word but there's like it's like three percent disgust of like oh man we came from the simeon origin and and how low and so on whereas you know today morally we will be reprehensible to our future selves or whatever entities end up showing up you know 50 years 100 years from now when this technology's around and we realize oh my god like this is how we've been treating each other um the moral norms of you know just 10 years ago have completely shifted and this idea that we that we found this like one true moral framework that's going to be you know this can hold true for all time although i'm sure the concentration camp guard in nazi germany sure felt that they were on the right the right track i'm sure that joseph stalin's closest associates felt that they were on the right track everyone seems to have this absolute moral confidence and yet the one thing that seems to keep happening is that those moral values get flipped over every couple of years or decades is that something that like that you think is is gonna is ever going to change are we destined to keep seeing this like this moral shift even as we ease into ai as ai starts to maybe even think about moral norms well i i don't feel uh discussed personally to our our predecessors if anything uh i feel uh admiration for all living creatures because step back and consider that every every creature that's ever lived has an unbroken chain of successful reproduction going back to the beginnings of life on earth that is an absolutely staggering thing you know you you the the the bug that you swat that you know that that got in your way that entity has an unbroken chain of successful reproduction going back to the beginnings of life on earth that particular bug now if that doesn't give you some like feeling of reverence for your fellow creatures i don't know what will so if i think back to our our our ancestors you know those guys had a lot to deal with you know the primates there's a lot going for them but there's also a lot not going for them uh it's hard to be a primate and uh the they somehow made it through and then they somehow got to the point of being able to develop language and so i i feel like um kind of sympathy for them in their pre-language days uh not discussed and and i think that the way that we're going to think about the who we are today you know when we go down a few generations and think back to this is again it's going to be more of a sympathy thing is that there are things we don't know today that are going to become clear and in hindsight it's going to be you know they didn't really know all of these things and uh we'll give them a pass because they weren't they just didn't know and the you know the whether or not this kind of a consistent revolution of of of continual reveals keeps going into the future where every time you kind of step up as a thing something true is revealed about the nature of the universe that you didn't know before whether that continues forever i doubt it there's probably some limit to knowledge that you can have and at some point you become as omniscient as an entity can become uh and then there is nothing else you you've reached some kind of intellectual nirvana where you kind of comprehended everything there is to know about everything uh but we're so far from that now that uh it's kind of just a kind of an intellectual exercise to think that far in the future but in terms of like our ai transition that we're about to go through the the one thing that we're going to need to have as a species to make it through this is empathy is that we need to we need to have these ideas of circles of empathy so you and everyone else cares more about the people around you than everything else you don't care about the bug you don't care about even if you say you do you don't care about people in minnesota unless you live there or you have relatives there it's just a fact of human nature that we care more about our children and our spouses and our families than we do about the rest of the world and right now machines are in virtually no one's circle of empathy nobody cares we don't consider them to have any uh status i suppose in the conversation about uh what it means to empathize and that's has to change at some point when these machines start being a little bit more like us and then a lot like us and then maybe even more than us the empathy that we need to develop for them as you know fellow pieces of bundles of matter that move through the world uh is gonna have to grow and it's gonna have to grow not for altruistic reasons but for survival reasons is it in in a in a in a landscape where you have thousands of intelligent things wandering around all with different goals the one thing that's going to make everything work is figuring out how to work together and the the you have to give up these ideas that you're going to have control because you don't even know no matter who you are i don't care who you are every single human in the world does not have freedom in the sense of being able to do whatever the hell you like you live within a set of rules and those rules you can think of as losing control and you do it for a reason it's because it's better for you to be within these these bounds it's better for you personally to be within these bounds no matter who you are so the the this idea of circles of empathy is something that i it keeps coming back to me in terms of what how we have to think about the world is empathize with your fellow matter it doesn't have to just be things like you know dogs and cats and squirrels and your kids the the world around you is this very complex mysterious thing somehow we have a property that most of the matter around us doesn't but that could change when it does the world is going to become a very strange place in order for us to find our place and that we have to we have to rely on the angels of our better nature and not our the demons of our evolutionary ancestry uh in order to help guide us through it i wonder it's too in that respect i mean it really seems like the the um embodied cognition strategy seems like it would be important to the extent that humans are i imagine we'll have a harder time like developing empathy for gpt 5 than for some physically embodied system that like we can at least relate to because it has a physical form um is that part of the calculation there as well yeah well again going to this analogy of an alien intelligence so let's say you know the aliens from zebulon 5 come down and they're like clouds of gas can we have empathy for them if so why i mean they don't have bodies like we do what makes it so that we would think that they were like us so and now ask the question of gpt3 running around inside the uh you know the voltage potentials of a bunch of computers what is different about the alien from zebulon5 and these voltages running around in a computer why do you feel differently about those two things and so i think a lot of this has to do with the mental models we have what it means to be a person or a thing you know we have we have no problem again our evolutionary history tells us why we have no problem ascribing personhood to a dog because we've lived with dogs for thousands of years they've been our companions going back you know to the dawn of civilization and even before the uh the the the fact that we would have trouble ascribing personhood to something as abstract as a as a computer program is more a failing of our imagination than it is about some fact about the world so the the um you know the future will have a lot of things like that call them alien intelligences that we wouldn't even think usually about ascribing anything related to what we think of intelligence too but maybe we should and there will be things that are embodied like little robots wandering around talking to us doing things um and the the whether we ascribe personhood to them is a big challenge and i think a matter of again it's in this gray area you know if your driverless car can take you from one place to another uh is it really like us maybe sort of but probably not but now let's say it starts talking to me and i have conversations with it is that what we mean where is the line like what is the property where we say okay now this thing is over the line i have to start thinking about it like a fellow being is it the ability to feel pain or discomfort what does that even mean because you know we live inside our brains right all pain and discomfort is is a bunch of electrical signals traveling on your nerves to this wet meaty thing inside your skull so presumably you could ascribe the same qualia or you know perspective to an ai if it was sufficiently like us to say this ai is in pain or suffering or somehow is not doing well should we feel sympathy or empathy i think the answer is yes and again why it's because if we have what we think is a moral compass it doesn't only apply to us that is a very toxic idea and it's at the root of all of the worst ideas in human history you mentioned it before this idea of the other it's very easy to say i'm okay i'm fully human but you over there who don't look like me you're not fully human you don't have rights i don't care how much you're suffering because you're not you're not like me this this this deep seated thing that we seem to have which we've overcome to a certain extent but not all the way is not just about say racism or sexism that's one manifestation of the hatred and fear of the other but it's not the only thing and in some not too far away future uh we're going to have to start thinking very deeply about what it means to really be a moral and ethical person or entity it's not just about things that look like you it's about a much broader uh circle so what's in the circle and i don't know and i don't think that the circle is a bright sharp line it's gray certain things you know what are clearly in the circle certain things maybe are clearly outside but the middle is a very difficult thing and it moves over time you know we wouldn't have been having this conversation 500 years ago if we were burned at the stake uh but it you know now maybe it's not not just a bunch of um you know talk over beers now we're talking about being able to potentially build things like this and it's a real conversation about the future of what uh what a good and fair society looks like not just for us but for for all entities that should be inside the circle do you think that we need a theory of consciousness that's reliable that we have confidence in before we can get to that point or can we do it more empirically uh no i think you i think that's something that would be part of this is that you'd say you know okay so we think of the world and we we are our thing in the world in a particular way maybe we decide some moral philosopher thinks that particular thing is is necessary in order to have the properties that we want to ascribe to you know the things inside the circle so you have to have certain properties that are reflective of this idea that we have an internal model of a certain sort so presumably if you're good enough to be able to build one of those things you understand it well enough you could ascribe a measure to uh anything like a dog or a cup or whatever and you could literally go in and take your consciousness meter and apply it to that thing and say okay this score is 0.3 that's not enough because it needs to be one or else it's not it's not a person now maybe that's a way that you could do it i think in the short term uh it's unlikely that anyone would accept any prem any measurement that you proposed no matter how well-founded it was so if i came out tomorrow and i said okay here's how we measure consciousness we do blah blah blah blah blah blah and here's the answer no matter how potentially true it was or defensible no one is even going to pay attention for one thing because you know the what difference does it make this is just some weird thing just like a bunch of other weird things you know most people who study consciousness i think again are well-meaning but it's it's the sort of thing that's really hard to convince somebody that you've got a good answer no matter what it is uh at some point maybe you'll have this idea that there are certain things that are that are have it and some that don't i'm i'm more i suppose like intuitively pan psychic when it comes to this the idea that the these things every every property of the mind that we have names for are uh exist on a spectrum in many dimensions they're not a fixed yes or no you have a little bit or a lot of it in a lot of different ways uh whether you're an oak tree or a human you you possess every single property that we think of in terms of uh you know the the various words that we use to describe the internal experience but you have them maybe in different amounts and potentially in different dimensions uh and by the way that doesn't make us greater than it makes us different then and this this idea that the biological creatures are different than and not greater than is a really key idea as we transition into this next phase where we now have this this new form of life like machine life didn't exist before now it will um the the idea that we're not necessarily the best at everything but that doesn't mean that we're less than so we can make something that can can calculate the product of two integers way faster than any human can does that mean we're less than no just means we're different we just don't we can't compute products of integers as fast as a computer can uh we're better at other things and even if we weren't who cares if there were machines that were better than us at everything it doesn't change who we are it's kind of like that cucumber grape experiment it's like you know if you give if you give the monkey the the cucumber oh he's happy but if you give the other monkey the grape he's pissed because the other monkey got the grape and why'd you give me the cucumber so i think that that the this idea that you can be content with what you have even if you're not the best at everything is a very important idea you know like i could never play in the nba you know i just can't there's no way i ever could i could have i could have trained every minute of every day of my life and i would never have been able to do that there are humans on the planet who are just better than me at basketball and if you go down at every single human endeavor every single one there's somebody alive today that's better than me at all every single one does that make me feel worse about my life no and it should when we're talking about machines in some future where there's a machine that's better than you and everything it's no different than now you're already not the best at everything you do and so this this idea that the the being better then is somehow like an objective is ridiculous it goes back to this earlier thing about like why are we here what's our purpose and all that so i maybe have gone through an evolution am i thinking about this as i've gotten older but being better at everything is not the purpose of your life it's just not because you're going to fail if the purpose of your life was to always win at everything your life is a failure by definition if you want to be the olympic gold medalist in freestyle wrestling at 82 kilos which i did for a long time the fact that i didn't achieve that and i never will if that was the defining element of my life means that everything that i've done in my life is a failure just because i'm not the best but that's the wrong way to look at things like the the the path through your life has ups and downs you're gonna have things that you view as successes and things that you view as failures all these things are relative they're not absolute and you can do all of the things that you've ever done in the background of having a bunch of intelligent machines running around nothing is stopping you from still doing what you're doing even if there's a machine that's better than cleaning your carpet than you are it doesn't matter in fact the machine being better at cleaning your carpet than you are frees you up to not have to worry about cleaning your carpet or you know if the the robot dentist is better at keeping your teeth healthy than the human dentist the human dentist doesn't have that work anymore but by god you've got great teeth and in the in the main what's more important even if you're a dentist you could do something else you know there there's so there's an enormous infinite number of things that you can do that don't necessarily boil down to who's the best at it in fact there's everything is like this um that i i don't have any uh concern at all for the the future in terms of like work replacement things being better than us i mean let's just accept that it's going to happen and figure out how to make the best possible outcome and and there's a ton of very very cool outcomes that could come of this well it's a fascinating topic and a very exciting vision for the future that you have and i'm i'm glad you you stopped by to share it we're going to have to do this again there's just too much to talk about uh but thank you so much jordy for making the time i really appreciate it well thanks it was it was fun uh have me on again and i'll go on for another hour i'm i'm sure we will yeah i'm sure we will actually is there do you want to share any links i know sanctuary is sort of uh sort of secretive right now but are there any links that you want or that you can share for that or any of your other work uh well so we we've got gone to great lengths to not have anything i did uh anything that you can find about sanctuary is not what we do uh at least anymore so the the you know there isn't any um the i'm not hard to find myself personally um if anybody wants to get a hold of me uh there's a lot of people especially in the the um the machine learning community who know what i get a hold of me so just ask somebody if you want my email or whatever and uh i'll respond perfect and i'll share your social media as well i think you're you're on twitter right i am yeah it's for my my uh the only reason i'm on twitter is to follow ashley vance who's absolutely hilarious uh there's really no reason i my outlet for watching you know muppet videos and and cat memes that's that's probably mentally healthier than the reason most people are on twitter these days so uh you're definitely to be commended on that great well thanks so much shorty