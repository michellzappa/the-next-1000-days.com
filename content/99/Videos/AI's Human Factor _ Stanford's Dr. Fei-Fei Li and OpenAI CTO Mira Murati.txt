while we'll be covering a bunch of things on safety which is a highly relevant thing especially in the uh uh this kind of new foundational models universe because both um uh feife and Mira are accomplished technologists uh massively beyond the scope of of safety both building amazing things and have been part of the historic contributions I thought we'd start with a kind of a more broad question which is what's currently most exciting you so fei-fei I will in AI right okay I'll start with you Mary I'll go to you and then we will dive into safety while so many things are exciting me but I guess I'll just say something that's on my mind right now we are finishing the review process of a paper that is coming out which is which is redefining a North star for robotics I I feel like this moment working in robotic learning feels a bit like back in the days of image nowadays where we're really imagining what can be done that would make robotics dream come true and this particular paper that we are hopefully fingers crossed were were in the rebuttal review stage um we'll lay out a benchmark of a thousand robotic tasks that are inspired by actual human activities so it's a scale we've never seen in robotic research anywhere is it is it too much of a pressage to say what like one or two of the most interesting of those tasks are I wanted to actually all of the Thousand tasks comes from American labor Brewery survey of American time usage and their equivalent of European government agency where we look at what humans do in their daily activity and we actually did a quite a bit of study on what people want for robots to help for example most people do not want robot to open your Christmas gift apparently we still want to do that ourselves but almost everybody want the robot to clean the toilet so that's rank really high packing kids lunch is actually ranked fairly high oh interesting so so we're talking about real human activities instead of you know how in robotics we tend to see toy examples yes this is going to be a thousand real tasks awesome yeah what's most exciting to me these days is seeing how far we can push the Paradigm that's driving all AI development in the field which is you know this combination of large neural networks with a ton of data and a vast amount of compute and in the past few years we've seen this formula drive a ton of progress in AI research we saw that with gbd3 and then again with codex and the first generation of Dali so from open ai's perspective we're trying to build these General systems that can think of the world in a similar way that humans do so systems that have kind of a robust concept of the world so if a AI system sees an avocado or reads the word avocado the concept that gets triggered is exactly the same and it's exciting to see that we've got systems that you know have achieved some sort of linguistic competency and understanding of visual concepts and and will continue to push this Paradigm ahead so let's shift to safety because both of your organizations in different ways are uh kind of some of the most important industry leaders on what we encounter with thinking about AI safety and so um so uh Mira why don't we start with you just so we mix it up a little bit and not always go in this order but we'll vary say a little bit about open ai's approach what kind of being this kind of Novel organization organized around a 501c3 and organizations like this how what you're trying to do to identify what safety is and how to create those Norms both in your own action but also catalyzing other industry as you know our mission is to build a general system and figure out how to deploy that beneficially to the world and you know they're just one word but beneficially but figuring out how to robustly do that is actually an immense challenge um it's hard to predict the future it is hard to predict all the ways in which these systems might create harmful biases or other risks that we can't even imagine but at the very least we can try to get as much understanding as possible gather as much knowledge as possible and leave the options open and that's open AI strategy we are trying to deploy these systems continuously but in a controlled way that means an API so gpt3 was first deployed through an API to a small group of users and then eventually we broaden up access as we understood how to get a good handle on the risks but I think it's really hard and that's why that's actually part of the reason why we decided to deploy gpt3 because if you are in the research lab you may think this is going to be the most prominent risk based on what we see but when the model comes into conduct with the real world that gets tested and we found out in several several times that we've deployed that we were wrong and the most prominent risk was something else for example with gbt3 we were convinced that misinformation was going to be the the most important risk and it is very important but in practice we saw that spam was actually a much bigger risk that we had to focus on and the same thing happened with Dali as well so I think it's really important for these models to come in contact with real users with the real world and understand where the friction is where the limitations are and iteratively building mitigations and the mitigations we're building are not are probably not future proof but it's it's a place to start and it gives us enough knowledge of where to go but at the same time we need to think about how the complexity increases as the models become more and more capable people so for example with language models now we oversee the output of of the model by a human for sensitive use cases and that's not something that would scale with more powerful and advanced models so then we have to come up with techniques to help humans evaluate the output of these models and open AI has been working with other language model developers to coordinate on some of the standard practices to figure out how to deploy language models safely very very same question but as opposed to open AI human-centered artificial intelligence Institute role of universities with industry governments plus the work you guys are doing yeah great great you know safety is one of those words like Health everybody wants it but it's really hard to Define it um so uh we're in the Hat of the director or co-director of Stanford human Center AI Institute which we call Stanford High we think a lot about what we really want for AI for future Ai and we come to the word human-centeredness we focus on infusing the human centeredness into every stage of AI research development education policy work obviously we're University so we don't deploy products but we hope that what we would Inspire as applications will have a impact in productization Downstream so so AI is not one thing AI you know designing AI system is is is really stages of work decisions and we believe that every stage of this AI development we need to infuse the ethics and human-centered values into this simplest way to put it how do we Define a problem for example do you is your goal to you know replace humans without consideration of all the social implications or augment human capability that is before you write a single line of code you already are thinking about human values the data where does it come from how do you collect it how do you ensure data Integrity how do you annotate it there is a whole bunch of you know from fairness to privacy to to just a whole bunch of issues and and considerations then the algorithm itself you know how is it safe is it secure does it you know is it biased and then the decision making using the algorithm the inference the the the the human how does it assist the humans or or um you know in other for inform the humans so every stage of AI development is needs human consideration and Stanford high is really trying to embody that process and in this and it's not even a side product a central product of that process is the people we educate the students the undergrads The Graduate students when they come out of this you know years of working or learning from Stanford highs courses and lab work they become technologists or uh Business Leaders or policy thinkers who understand the human centeredness of AI so let's now dive a little deeper in each uh specific organization so for Dolly and gpd3 the front end of the apis and part to kind of understand what the the possible risk slows look like so that you can begin to train those how do you take that information and iterate to a Beyond human you know kind of safety model how do you take that and also help you know kind of lead the way in kind of thinking about this safety within the industry so for for gpt3 for example um initially we opened up access to use cases that we felt we had the right mitigations in place so that means very very uh almost like on the first few days uh the the allowed use cases were around search as well as classification but we were not quite comfortable with open-ended generation and so we worked with industry experts from different domains as well as other researchers to Red Team the model a bit further we worked with other trusted users as well to understand possible ways in which the model would fail the expectations of the user and there are many so from there we try to to building mitigations both from the model perspective but also the tools that come after deployment from the module perspective one of the central things with gbt3 but also other generative language models is the fact that the model will make up stuff and it will not admit when it doesn't have expertise in a specific topic or when it doesn't know the answer so obviously that's a problem and in other ways it could it could also mislead you with the answer and so we want you to figure out how do we make the model more robust more reliable and we use the feedback that we got from our users on the API and we used reinforcement learning with human feedback to make the instruct GPT models which is a series of models that are far more reliable and do the thing that the operator actually wants them to do so not only they're more helpful and safe but they're actually useful and so there are default models in the API today and our users prefer them rather than the base model so this is one way where we use deployment to actually make the models safer more reliable and more effective as well and it is very interesting because it's the first time that safety moves from the theoretical domain into the Practical domain and it merges with capabilities and by doing that it kind of forces a standard in the industry because not only it's safe but it's actually more effective and more useful and then uh similarly what are the things where um you know High Stanford High um has pioneered some things and specifically I'm just gesturing at the ethics review board but go anywhere you like with the question yeah just to follow up right um it's actually being a very encouraging Trend to either companies are really thinking about the ethics and the the the the safe ways of developing and releasing products in the meantime in Academia we are also thinking for example a typical way of of putting some guard rails in our research in our research activity is traditionally if the research involves human subjects we have IRB review boards this has been critical across Academia in U.S universities and also International universities that a lot of medical research and human subject related research gets reviewed by a board of expert faculty and so on but when a I was starting to you know um become a really big chunk of University Research we don't have an equivalent of a IRB you know at the beginning it was okay it's theoretical it's just you know but very quickly we realized wait a minute um you know there's so many examples of AI research that we can for example face recognition you know we we see globally the biases and the harms of bias so how do we um how do we put guard rails in the design of AI research and to ensure even the most technical researchers and students have in mind the ethics and the social impacts of their work so at Hai one of our functions is to facilitate multi-disciplinary research and read you have been a a board member of our Institute so we talk a lot about that it's really important we encourage researchers to do interdisciplinary research but we need a new way a new IRB to guide our researchers to think about ethical and social implications so Hai was as far as we know the first organization University organization in the country that formed what we call an ethics and Society review board ESR that reviews all Grant applications and as we assign grants to our researchers we ensure that the grants work with the ESR board to um to articulate the understanding and potentially mitigation Solutions of the social and ethical issues of the research so that's one example of how much we put that emphasis in another example is our work with the policy makers we work directly with Regulators policy makers and frankly the Civil Society um you know different people multi-stakeholders to here and to communicate the implications of AI to hear their thoughts to uh to facilitate dialogues we particularly focus on bringing industry and civil rights organizations are federal and state and local governments and policy makers to the same table create a forum a safe neutral Forum to have these conversations because we all care about Innovation and guard rails and we need this dialogue and add a little bit about the national research Cloud too because it's part of enabling universities uh and other entities like this in order to be able to fully participate yeah yeah one of the things that we recognize is the magic one of the magic sauce of America in the past Century is our ability as a country as a people to innovate we have an incredible ecosystem thanks to our entrepreneurial World our um our higher education the the labs the researchers also the federal the role federal government has played in incentivizing research whether it's NASA or DARPA or NSF and all that but as AI took off in the past decade what we observe is that the resources are quickly concentrated in a few companies with compute and data and talent the three critical resources for doing incredible AI Innovation and which is great and we see incredible things coming out of open AI Microsoft coming out of Google deep bind coming out of Facebook but that's not enough to live the entire nation to educate more AI talents to you know stand up in the face of global competition we need to ensure this ecosystem remains healthy so that it's not just concentrated in in Industry per se and right now under Biden and administration the White House and the the Congress have mandated a task force study group a 12-person task force to figure out what this National research Cloud will look like I'm actually one of the members of this task force along with other people from industry Academia and government and hopefully very soon we'll come up with our Rapport and push this bill forward to establish this National resource so Mary I'm going to ask you I have obviously a thousand questions for both of you and depending on what the questions are from the audience I may ask some more but I'm going to ask you the last question before I turn the audience which is one of the things that GPD 3 and Dolly have both done is shown a path for really amplifying human creativity it isn't just the downsides that is part of the question around safety and so with but also the in a sense helping us be more human be more creative say a little bit about the the kind of the lessons you know from Dollar in GB3 and what their potential is for this human amplification yeah exactly so from gbt3 early on we were surprised to see the ability of the model to generate creative and even touching poetry so one of the prompts that we gave gbt3 was to generate a poem in the style of Pablo Neruda that talked about Maxwell's equations and we were quite surprised that it had this ability to pick up on the elements of Maxwell's foundational electromagnetic equations but also do that in the style of the love poems of Pablo Neruda so that was really beautiful and a lot of people were playing with poetry and the creative side of of gbt3 and I think we saw that even more with Dali Dali maybe because it's images and also so the form factor in which we made it available through daily Labs everyone was just having so much fun with it even at open AI we would spend hours just generating daily images and this really just shows how Technologies like Dali can democratize high quality creation of images and ideas and can push them so much further and often you know we we get asked well does this in some way dilute the the human creation the original human creation and what what happens in the future there is this almost instinctive human reaction to protect our own original creations and I think that it's if we look back in history it's actually not so different from um you know what happened in 16th and 17th century where uh you know there weren't that many people that could afford paintings and so things were quite binary you were either Rembrandt or nothing and there wasn't so much of a nuanced appreciation it was either a great painting or not and so I think as as we get tools like Dali or gpt3 maybe there's going to be a more nuanced appreciation for this co-creation and the different appreciation for the original human human Creations for example today you know you can have a skilled artist that went to art school that can try to to create a replica of the night watch and to my undiscerning eye it will look great but we still value a real on brand differently so so but you know you can say okay this is sort of a Elite's point of view what what about the the broader impact um and I think that's actually not so different from the effect of globalization um because it's really an exchange of ideas a cultural exchange and of course there are sort of unwanted and undesired effects of globalization but overall it does create more diversity and it does create more prosperity and we've seen this in history with Western Europe the 19th century being one of the most fertile and diverse from liberalization of of cultural exchange and by contrast the after the the collapse of Roman Empire it was quite the opposite um and and so there is actually an interesting book about this um called creative destruction that talks about uh this perceived fear that human Talent will be diluted in a way um but actually if you look at it long term and the global effect the Global Effect is one of diversification one where we end up with more ideas in total more prosperity and we will continue to develop more information and create artistically scientifically and also in a social context and I think we have time for one question from the audience um I did also ask that last one because if you haven't had a chance to play with Dolly and do your own printout it's one of the things that open eyes help provision us with so it's a fun experiment um you know just to the last point that he alluded to um you know first it's very heartening the thoughtfulness and thoroughness of you know preempting and mitigating potential impact of launching some of these Technologies and I'm wondering some of the effects cannot be simulated until you put that out there in the real world and probably at scale so how do you think about you know the the the the the risks and issues at large-scale societal effects and potential second order effects as well and what do you think you know is the role that that we as technology companies can play in that and and probably related to that is you know um you mentioned governing boards as an oversight and I'm wondering how do you address the tension between the the process interaction and the pace of innovation we were colleagues in Claudia if you remember we've had oversight boards internally and you know things get slightly slowed down as a result of um so would love to hear your thoughts how how you think about this okay I'm sorry actually Miriam you're right so we're with deployment we're just scratching the surface and we're putting these models out there seeing what people are doing with them seeing the risks and building tools to mitigate them but more importantly bringing the feedback back to the model development stage to build models that are more robust and I think the alignment technique is really core to that but more broadly uh it's very important that we collectively are thinking about the governance systems of AI because obviously the the developers of these models need to be a diverse group of people but even that is not enough and we have to think about AI sort of like electricity and so we have to think about the deployment and the governance of these systems more broadly in a in a global setting and and bring in input from all all different fields and also great question about Innovation versus regulation or guard rails I think we all get that question and it's always wonderful to think about it first of all I I don't believe they're mutually exclusive they're at odds with each other I do believe as everything else in life we have to strike the right balance if we go to the extreme route that there should be zero you know guard rails Innovation might come out to fail us all so it's not necessarily a world we want to live in and of course the other side of the extreme is also not good in the case of AI I think it's it's time for every organization from Individual researchers all the way to the government to have that dialogue to think about how this can be done and if it's done right actually good guard rails can encourage good Innovation I'll give you one example of um smart camera machine learning work in healthcare environment which is part of my lab's work where we actually talk about safety we actually use cameras to help doctors to monitor safety of patients now the the intention is all great but immediately we get into privacy issues we get into password issues you know who are you watching what are you watching this comes back to us as an important social human feedback to push us to innovate more my team of students who are used to just doing deep learning algorithm are now using a differential privacy algorithm to push for um you know privacy protected machine learning and then suddenly they realize differential privacy algae rhythms are too slow it's never used in videos where we're using cameras suddenly that guard rail push the regulatory push is actually incentivized Innovation we're published in papers that's pushing differential privacy machine ml algorithms to deal with the large-scale data so this is a perfect example of when we care about these guard rails they come back to motivate us to innovate better technology and I think there's plenty of examples in AI that we can do that and that everybody is better off if we do that and with that you can see why both Mira and fei-fei are people that I also constantly learn from um and so let's give them a hand [Applause]