thank you for the absolutely lovely introduction um so I did give a version of this talk um it's changed quite a bit between then and now in Porto last year um so thank you so much for coming to this talk slot um as Adele said my name is Jody Burell and I'm currently working as a developer advocate in jet brains all nicely branded today I've been a data scientist for almost 10 years and a big chunk of my career I actually spent in NLP kind of a funny story about that one of the jobs that I worked at in NLP when I left we were starting to work with early large language models like Bert and I have friends who are still working there and one of them reached out to me a couple of months ago and said the CEO has been in contact with our team and he wanted to know what we're doing about working with a I and he's like dude we've been doing that for the last like seven years so confusion abounds so prior to my career as a data scientist I actually did my PhD in Psychology so you can imagine that I've been watching this space with a lot of interest but I've also been watching it with a lot of concern and one of the biggest areas where I've sort of been having concern is about the messaging around large language models so what I want to do in this talk is take a much more measured look at this talk uh sorry at this area and cut through the hype a bit so for the past two years no big surprise we've been in a full-on AI hype cycle I'm actually surprised you came to my talk about llms because I'm quite sure everyone's sick of hearing about them at this point so we've had claims that range from Models like Lambda showing sentience to speculation that huge s of the white collar job market are going to be replaced by generative models to even doomsday claims that we have an AI apocalypse on the horizon so it's been an absolutely overwhelming flood of opinions idea and speculation so naturally for most people it's really hard to know what to think about these models are they actually useful or are they just fun toys are they just a gimmick what can we realistically use them for and are they really going to replace or destroy us so what we're going to be doing over the next 40 minutes is cutting through the hype and looking at the actual applications and limitations of these models we're going to see the context and the science behind these models and we're going to challenge some of the more outrageous claims such as we're on the path to artificial general intelligence or AGI with these models and obviously with a 40 to 50 minute time slot I've had to leave out a lot um I think we should have a few minutes for questions at the end so please do send them through the app or come and find me after afterwards so for a lot of people models like chat GPT 3.5 and 4 felt like they came out of nowhere but naturally they're part of a long history of research in this field specifically in the area that I mentioned of natural language processing so the interesting thing about these models is that the generation of text was not the original aim of earlier models in this field it was to build models that could automate tasks with text that require huge amounts of man labor things like text classification or summarization so let's have a look at how this journey evolved so large language models all belong to a type of model called neural Nets and these were originally proposed of as a way of artificially mimicking the human brain so research in this area started in the 1940s and when in stops and starts up until sort of the 1980s where a number of important technological limitations were overcome however the practical application of neural Nets was limited even up to the start of the 21st century and this is because these models have really intense computing power requirements so researchers found the larger they made these models the more they can learn about their input data and the more accurate or sophisticated their predictions can be but this requires the use of processing units that are really efficient at doing huge amounts of matrix multiplication hence you hear all these mentioned of linear algebra in this area so this leads us to our first breakthrough on the path to large language models which was the development of Cuda so Cuda allowed us to transform gpus into allpurpose matrix multiplication machines and therefore make the training of large neural Nets much more feasible as you increase the size of neural Nets they also become unsurprisingly much more data hungry so therefore the next big advancement was the development of a data set called common crawl this is a huge dump of frequently linked pages on the internet and the development of common crawl meant that researchers now had sufficient Text data to start training more and more complex language models and the final step was Innovations on the modeling side so despite early predictions that this would be an easy problem working with text turned out to be quite complicated even with all of this nice computing power and all of this data that we now had so language is really complex and it's highly context dependent so it took some time to develop a type of neural net that could actually capture these sort of relationships between words and this also happened in 2007 the same year that common CW was developed with the development of a type of neural net called long short-term memory networks or lstms we're not amazing at naming things in this area you may have noticed that not a very catchy name so these models were one of the first to really understand words in their context sentences and therefore they were able to vastly outperform previous models on a range of tasks so of course they were better at uh text generation than previous models but they were also one of the first general purpose natural language processing models they were capable of doing a range of tasks like text classification summarization and translation and in fact you may have noticed that Google translate suddenly got a lot better in 2016 this is because we started using lstms under the hood however a major limitation of lstms is that they're forced to process words in a piece of text sequentially so what they do is they process the first word they remember something about it they process the second word they add that information to this memory and so on and so forth so this caused a number of problems but the biggest of these is it led to problems with scaling these models limiting how big researchers could feasibly make them so the models that solve this are of course Transformer models you would have heard this term everywhere this type of model avoids this sequentially Pro sequential processing and therefore allows models to get really huge and a really cool thing about Transformer models is as part of their training they learn really rich internal representations of how language works and therefore they're of course generalist models they're able to do a range of NLP tasks that they were not explicitly trained to do so this has kicked off the current generation of text processing models and this is why we can have the large in large language models so the branch of Transformer models that evolved into generative or Auto regressive models are the generative pre-trained Transformers or gpts and the first of these came out in 2018 so gpts have been a very successful type of model and pretty much every single one of the many many many many large language models released in the last three years have a GPT based architecture including Heavy Hitters that you would have heard of like chat GPT GPT 4 Falcon mistal 7B Gemini and Claude so now that we have an understanding of the recent history of large language models and some of the foundations that they're built on let's have a closer look at the GPT models so we're obviously not going to look at every variant I just showed you we'd be here for the rest of the talk doing that but what we're going to do is have a look at the group of models developed by open AI because they were the ones who came up with the first GPT model so one of the most interesting problems for natural language processing is machine translation and this was the first major use case for Transformer models so in order to do translation between two languages Transformer based model needs two components an encoder and a decoder so the role of the encoder is to learn everything it can about the source language and send that information over to the decoder now decoder's job is a bit more complex it needs to learn everything it can about how the target language functions but then it also needs to use that information sent over by the encoder and try to predict word by word the most likely matching sequence in the target language so you can see that in this example here let's say we have the sentence I have visited Italy in English and we want to translate it to German so encoder will have learned a whole bunch about how the English language functions now decoder will have learned a whole bunch about how German functions so our sentence is first processed by the encoder and then word by word the Dakota will generate the most likely matching sequence in German however something that researchers quickly realized is that Dakota units all on their own are pretty useful so as we discussed a Dakota's whole job is to learn the ins and outs of how a target language works and use that knowledge to predict the next word in a sequence based on an input so researcher started to think well what if we cut out this Source language we cut out the encoder and we just Train encoders by asking them to predict the next word in the same language what we can do is train these models by showing the millions of words in the same language and get them to adjust their predictions based on whether they can successfully predict the next word or not and we then end up with as a result with a model that has a pretty good idea of the next most likely words to come in based on a given text input so we can see that with this example here we have the two words as an input I have and the model has learned the probability of the next word and the next word after that and so on and so forth now interestingly this was initially not a huge break for through because of its ability to generate text but because it's solved the problem of where to get enough data to train these really huge models as I said the bigger you make neural Nets the more data hungry they get so the problem with training most models this is machine learning models across the board it's not uh unique to llms is you often need to manually prepare the data in some way it might be labeling data or pairing things up for example and obviously if you want to make really really really huge data sets this gets really costly or potentially unfeasible but this problem is solved if you're basically using a sentence as both the input and the output you just need to cut the sentence at a point use the beginning of the sentence as your input and you use the next word as your Target and this gives you a ready-made training set which scales very easily so this was the basis of the GPT models and along with that beautiful scalable Transformer architecture is the secret of how these models have been able to grow so big so the way these models work is they roughly increase their size by stacking decoda units it's not exactly how they work but this is the general idea and what this does is it increases the number of model parameters so this is the size of the models so the first GPT was 120 million parameters which seems quaint at this point the second was 13 times that size at 1.5 billion parameters and you can see the models have just kept growing and growing up to gp4 which is estimated to be at around a trillion parameters and I left GPT 4 out o out I just can't keep up with all this stuff so I was like you're staying off this slide so the models have grown over successive generations to be better and better at a range of natural language tasks and we can get a sense of how the models have evolved by seeing how they each respond to the same prompt complete this sentence Belgium is apologies to any belgians in the audience I'm not taking the piss but uh you know this is just the one I've been using for for about a year so okay so gpt1 is really good at generating text which is grammatically correct but there's no real sense of context so this is what gpt1 came up with for our prompt which the best you can say about it is it's grammatically correct now gpt2 is a bit more polished than gpt1 but it still tends to come up with really output so here is gpt2 attempt so gpt3 is where the models start learning not only grammar but they start encoding some information about the words that they're trained on so here's gpt3 is attempt it's gives us a nice little sentence which not only makes sense but it actually gives us on context uh on topic information and then finally when I put this into chat GPT 3.5 it wrote a whole bloody essay I had to cut it off but it kept going on about multilingualism and comic books would have gone on forever so now we know where llms have come from and we've explored how they work under the hood a bit let's turn to how these models are being perceived in both the tech world and Beyond so as we've discussed these models now exist in a hype cycle which makes claims about them that goes far beyond their original purpose as generalist natural language processing models and the most sensationalized of these claims as I said at the beginning is that llms are showing signs of artificial general intelligence or AGI so these claims of AGI have run the gamut and they've been backed up actually by some of the most respected researchers in the field like Jeffrey Hinton one of the so-called three Godfathers of AI or backed up by papers from Microsoft research so with all these people throwing their weight behind the idea of lm's showing AGI how can we be sure that these model haven't developed intelligence so let's go back to May of 1997 when then World chess champion Gary Kasparov was playing deep blue IBM's uh chess playing Ai and this was the second time that they' faced each other so in the second game of the match deep blue made an unexpected move and this rattled Casper of and it made him believe that the model was far more sophisticated or advanced than it actually was so thrown off his game he actually ended up losing this game then he lost the third then he lost the fourth he lost the whole match and the Press went crazy speculating that if we could solve chess with artificial systems then surely AGI was just around the corner but as you can see from this incredible you know quality of this screen capture uh that was almost 30 years ago and we can see how those predictions panned out the problem with trying to assess intelligent in this manner is that it confuses the output of an AI syst system with the mechanism that the system took to get there so we can use these skill-based Assessments in humans because we know that in humans the ability to demonstrate significant skill in an area is reflective of an underlying Raw General ability machine learning or AI models they don't work this way they try to optimize for their training goals and if they can take shortcuts to get to good performance on those goals they absolutely will the mistake is thinking that the pathway required to learn some skill for a machine learning model including llms requires the development of an underlying intelligence this illusion of skill-based intelligence is so well known it actually has a name the kaggle effect after the well-known machine learning competition website so on this website people compete to create an algorithm that can best complete some sort of task and the winning Solutions are so good at doing this specific task that while they're doing it they seem to show a sort of intelligence but as soon as you try to get them to predict on some sort of example that's too far outside of what they were trained on they fail they show brittleness and this is a problem plaguing the current assessment of intelligence in llms they focus very strongly on how these models perform in specific tasks ignoring how intelligence is actually defined and measured in humans and this is because while researchers in these areas are very very well established in their fields their fields are things like Computer Sciences physics mathematics or engineering not psychology so let's have a look at an example that shows why these skill-based assessments are a problem so if you remember back to the beginning of last year one of the biggest hype topics especially after GPT 4 came out was that this model was able to solve medical and law exams and there was all these claims that this model was going to replace doctors and lawyers and then they came for us the model started solving leak code problems and then they started saying these models are going to replace programmers so let's take a closer look at this last claim so there's really great example circulating on Twitter around the time that GPT 4 came out so there this guy called Horus Haye and he tested out GPT 4 with some coding problems from a website called code forces and what made code forces really good for this particular little experiment is the date that each coding problem um was released is actually timestamped so so what he did is he collected 10 puzzles that were released at the time that gp4 was being trained so potentially it could have been in their training data he ran them through GPT 4 and well' you know gp4 got all of them right congrats it's coming for our jobs but then what he did is he collected another 10 puzzles that were released after gp4 was trained these were of an equivalent level of difficulty but this time GPT 4 got every single one of them wrong so what happened well this raised a lot of suspicions that GPT 4 was only able to solve these puzzles because they're in the training data and he just memorized them so another researcher Sayes kapore tested this explicitly by asking gp4 about a code Force's puzzle that was available at the time it was trained and obligingly gbt 4 vomited up the complete Source confirming that it did indeed have code Force's puzzles in the training data so I think this is a really neat example that these skill-based assessments of intelligence in llms can be wildly misleading instead we need to look to how well these systems can solve tasks they've never seen before that is how well can these systems generalize so the way this was approached by franois chol he's a very well-known AI researcher he works at Google he defines this in a hierarchy of generalization so what we do is we start at an absence of generalization this includes systems that know all of the Poss possible outcomes in advance such as if you've got an algorithm that can play tic tac toe it will be able to play it by knowing all the possible configurations that it can play with in advance we then move to local generalization this is where a system can make inferences on examples it hasn't seen before but only if those examples are similar enough to things it was trained on and machine learning models fall under this and this is an illustration of the kaggle effect broad generalization reflects a human level of in a single broad activity domain so a famous example of this is the wnc coffee test this was proposed by unsurprisingly Steve wnc of Apple Fame and he proposed that if you can have a system that can go into a kitchen and can complete all the steps required to make a cup of coffee without human intervention that would show broad generalization and full self-driving cars would also fall under this C categorization we then have extreme generalization this is essentially human level intelligence and this is basically where you have a system that can solve problems it hasn't seen before even those that only share some abstract commonalities with things it has seen before and it can solve problems across the scope of what you'd expect humans to be able to solve so you might be thinking at this point I'm focusing a lot on human level abilities but we're talking about artificialist systems don't we want them to go beyond what we can do to be better than us and this leads to the final level which is universality this is the ability of a system to solve any problem in the universe beyond even problems that have any relevance to us the thing with universality is it shouldn't really be considered as an initial goal for an artificial system for a couple of reasons the first is as developers we know that all problems need a scope in order to be useful and the current scope we have is being able to automate tasks that we ourselves do manually for artificial systems the second is we haven't really even been able to solve up to Broad generalization so should maybe focus on these lower levels before we aim quite so high so if we come back to intelligence we can actually line up Chet's conceptualization of generalization with the actual effect uh sorry accepted definition of intelligence in humans so this conceptualization says that human int is that humans have a general ability to learn called G or general intelligence and we use our general intelligence to learn broad activities such as how to cook or drive a car and within those broad activities are tasks that we can complete such as whisking eggs or using an indicator and you can see that an artificial system's extreme generalization aligns with G in humans broad abilities align with broad generalization and specific skills align with no generalization or local generalization so given this overlap it seems fair that we can derive lessons from measuring intelligence in humans which is an established field and apply them to measuring intelligence in artificial systems so as an aside before I go on I want to say I am a psychologist I am aware that intelligence is a controversial field this is not being proposed as a silver bullet it's more giving us a framework to move Beyond these skill focused assessments of intelligence in these systems and try to get a sense of how well a system can truly generalize so Chalet also promoted a method of maybe designing or building a system with artificial general intelligence so he defines it as follows an artificial system should be able to demonstrate the ability to solve a task so far so familiar but it does this by using knowledge encoded in a skill program relevant to that task and the skill program is generated by a humanlike intelligent system so these skill programs are refined on input about both the situations they encounter and how effective the response has been and the intelligence system itself learns over time through exposure to more and more tasks so Chalet also argues that if we're talking about a humanlike intelligence system we should also presume that such a system is designed with the same priors or skills that humans are innately born with and this is Elementary geometry and physics arithmetic and an understanding of the agency of others so skill programs will be able to encode their experience with tasks and remember how they solve such problems in the past and tasks themselves can vary based on generalization difficulty so the generalization difficulty of task is quite Central to this system design it's basically how different tasks are from things that the system has already seen so obviously tasks that are much more different or much more different are quite different to things that the system has seen before are going to be much harder for the system to solve and it will need to be able to generalize more in order to successfully solve them so this gives us is a starting point a conceptualization of how we might build a system with artificial general intelligence so chal's work has been followed up with researchers from Deep mine these are actually the precursor to open Ai and were then acquired by Google so they again start from the idea that an intelligent artificial system must be able to generalize but they simplify this Dimension by breaking it into narrow and general systems and to this they add a second dimension performance and they measure this by the percent of people that it can outperform on a particular task or range of tasks and the way that they kind of break it up is they have a range from no people or outperformed to people unskilled in that task to 50% of people 90% 99 and then finally all people are outperformed by that system so let's first have a look at systems they classify as having narrow generalization so in terms of systems that can out perform no one they put calculators and other systems that need to be entirely manually operated under emerging narrow AI they put GOI or good oldfashioned AI these are these old school I know I love that term um these are systems those kind of old school ones that needed to be encoded specifically with rules and then at the higher ends they have systems like grammar that can outperform 90% of people in terms of spelling spell checking and grammar tasks they have our old friend deep blue that can out form 99% of people at chess and then they have a system called Alpha fold that can predict a protein's 3D structure better than 100% of people even skilled scientists but as impressive as these are these are narrow systems as we know the true path to AGI is in systems that can generalize so let's see what the Deep mine team puts here well so far they only have two systems and interestingly they classify chat GPT as emerging AGI I strongly disagree with this as I've just shown you Chet's definition which is different from what these authors have put together is that the scope of generality includes all tasks that a human would be reasonably en uh expected to encounter and even chat GPT 4 40 um is still Limited in what it can do the other interesting thing is is that the Deep M team were unable to find other examples on the higher ends of generalization so what this shows is this these researchers really believe that we have quite a long way to go before we get to AGI or as they're calling it for some reason artificial super intelligence I guess AGI got you know not sexy enough and they had to change the branding so coupling this with Chet's definition of how we might build such a system I think there's really enough evidence here that we are quite a long way off this goal so what does this mean does it mean that llms are not useful well of course they are but only when they're applied to the problem domain that they were designed to solve which as I've been saying all throughout this talk is natural language problems so we've touched a bit on what natural language tasks are throughout this talk but let's take a step back and Define what they really are so this problem area is pretty borrowed and it encompasses anything that you might want to do with natural languages natural languages are languages like Dutch English Mandarin ones that evolve naturally so as I mentioned at the beginning of this presentation these are problems we've been trying to solve for decades and llms are simply the latest and so far most sophisticated tool we've had to solve these problems so let's have a look at some examples of natural language problems this is just a taster but it will give you an indication of what this problem domain looks like so llms are good at doing the following they're good at translating between one language and another but only if the model has being trained on sufficient examples of both language they're good at text classification where they can infer the topic of a piece of text and assign it to a category they're good at summarizing longer pieces of text into shorter ones and they're good at answering questions or question answering as it's called where a model can provide an answer BAS B on a question passed in as an input so we're going to focus on this application for the rest of the talk so llms can answer questions in a few ways they can of course answer it during during using their parametric knowledge as we saw with the slide About Belgium so during training sufficiently large llms will be able to encode knowledge that's trained sorry that's contained in their training data so depending on the model depending on the training data llms may be able to answer questions accurately based on this parametric knowledge alone llms can also be further trained to better answer questions in a specific domain and this is a technique known as fine-tuning so the idea is you create a data set which is comprised of high quality inputs questions and the expected output and you further train the model so that it returns outputs that are more more in line with this data set and after further training on this data set the llm will be better equipped to answer questions in this specific domain and then finally there's one of the most talked about Methods at the moment which is retrieval augmented generation or rag so this is where additional context relevant to the inut input prompt is pulled in from some Source external to the llm and this is then incorporated into the prompt and helps the llm more accurately answer the question so let's focus on rag a bit more deeply and see how a rag pipeline might be used to answer a question who won the American Super Bowl this year so I pick this question deliberately not because I'm a huge fan of American Sports but because it's a relatively recent event so most models are not going to be able to answer this question based on their parametric knowledge alone so instead they're going to have to rely on pulling in this information from somewhere else so how can we answer this question using a rag pipeline well just like when we use an llm normally we create our prompt you know in this case a question about the Super Bowl however rather than passing this directly into the llm we can first gather some additional information so this information can come from a variety of places it could actually come from a web search engine and if you've used chat GPT 4 that's actually what it's doing sometimes under the hood it's actually pinging Bing and pulling in information but if you're dealing with sensitive documents you're going to want to store that internally some way in some sort of database so how do we then retrieve information from our database at question time at the time we want to get our llm to answer our question and do this in a way that's efficient well the first thing we need to do is divide our documents into chunks we then feed these chunks into a second what's called an encoder model and as a result we convert these chunks into document embeddings and these are then stored in Vector database so if you've heard things like web8 or pine cone all these deor databases people are talking about this is why people are using them at retrieval time the prompt is also converted into a document embedding using that same model and the most similar document trunks are retrieved from your vector database so we can see from for the Super Bowl query we have two chunks one telling us that the cansas City Chiefs one against the San Francisco 49ers and a second telling us the score we can now build our augmented prompt we instruct the llm to ignore its parametric knowledge and only use this additional information we retrieve from our Vector datab uh yeah do document database when answering the question and then voila the llm is now able to accurately answer our question so let's now jump over to pie charm and see how we might actually be able to build our own simple rag pipeline for question answering okay sorry it's going to be slightly awkward all right so what we're going to be dealing with in this demo if I can find my mouse can you see my mouse there we are okay so we're going to be dealing with is a really large PDF which basically contains all of the pie charm documentation I thought this was very clever I was doing a demo of pie charm using pie charm documentation so you know patting myself on the back for that so this is a really huge PDF it has almost 2,000 pages and because it's not indexed in any way it's just a plain PDF we're actually going to have a lot of trouble actually searching through that if we want to answer questions so we can build a question answering rag application by ingesting this documentation and searching over it using our llm so how are we going to do this we're going going to do this using an open source package called Lang chain now Lang chain is a really really powerful application for extending the functionality of llms Beyond just sort of simple input output systems and at the moment it's only officially supported in Python and JavaScript but because it's been such a popular package it's been ported actually to a lot of languages in uh Community projects so I've seen Java I've seen C so if you're interested in using this and you're not working in python or JavaScript you may want to have a look around for one of those Community projects okay so let's now go through our application so obviously the first thing that we need to do is choose an llm that we're going to use so that's what we do here I'm going to be using an open AI model so we have the Syntax for doing that there then of course we need to load in our gigantic documentation that we're going to search over in my case I'm using a PDF but you don't need to use a PDF Lang chain supports a lot of different formats so if you want to use some other type of text format you can have a look through their documentation and see what they support so now we have read in our documentation we need to split it into chunks just like I showed you in the previous slide so we split that based on character and then we have our chunks and then we need to convert them into document embeddings and store them in the vector database in this case we're using a chroma database and we use a particular embedding model that we want to use for that so once it's in the vector database we need to retrieve it when we want to ask a question so this is what we do here in this method we would create create our retriever use that on our database and we can set different parameters like potentially the number of chunks that we want to retrieve each time we ask a question from the vector database and then having put together all those components it's actually really simple as you can see all we need to do is bundle it together in a little application so this is called a chain in Lang chain and you can see we're building a retrieval QA chain don't worry if you didn't get all of this I'm going to be providing the code at the end of this talk so you'll be able to play around with that yourself so now that we've created our app we need to be able to instantiate it with different parameters and there are as you can see a lot of different levers that you can pull when creating rag pipelines so I'm not going to go through all of these The Talk would go 20 minute over and Adele would be very angry with me but again you can go through this notebook at your own Leisure and see the different parameters you might be able to tweak in order to optimize your own pipeline however the most important things I want to point out to you is the model that I've chosen to use to answer the questions I've used chat GPT 3.5 so I've already shown you how powerful this model is it's going to help us get good performance on a range of natural language tasks including question answering and I'm also retrieving five chunks per question in order to answer my question okay great so we built our application that was easy so now we can start asking questions so the first question I'm going to ask is is what are the options for debugging with py charm and you can actually see that it gives us a really nice answer this is the summary given by the llm tells us about creating break points tells us about stepping through code tells us about evaluating expressions etc etc so if we go down here we can actually see the chunks that our application has used to answer this question so we can see this first one is very relevant this second one very relevant but if we scroll down the last two chunks are actually not that relevant they more seem to be from overview pages that just mentioned debugging and this is where you can see that trade-off you have to do with your hyper parameters it might be that five chunks is too many and there are different kind of levers you can pull in terms of that so now that we have been able to ask one question we might want to ask a follow-up question we have a chat after all so maybe we want to ask have you left out any other types of debugging but we need the application to understand what it's already been asked so we can do that in a really simple way here we can basically pass in the first question that we asked and also pass in the result that it gave us that little answer that it talked about break points and the following and then we assign that as one of the parameters when we invoke the application and you can see it's given us a follow-up answer it's now telling us about debugging in specific use cases like JavaScript or specific python frame Frameworks so to finish the demo I want to show you something kind of wild I was so excited when I put this together so like I talked about llms are generalist natural language models so this means that the same model can often do multiple tasks and one of those tasks is translation so because I'm using a sufficiently powerful model I can actually do queries in different languages so German to English translation is supported by this model so I can ask how can you install py charm in German what my application then does is it goes away and it finds me English language document chunks that are relevant to this sorry I'm so excited and so you can see this is relevant installation guide you can install pycharm doing the following and then what it gives me is a relevant summary in the original language so I just think this is so cool um like I said I'm going to give you the code please go away and play with it yourself but for now I'm going to end with a boring cautionary tale just for a couple of slides I'm sorry where is my mouse there we are okay so this is the point where I tell you like a boring data scientist that after showing you this really cute little neat rag act that app that um working with rag in fact working with llms in deployment is not really simple and comes with a lot of gotches and pitfalls so let's start with problems you might have with rag so do you remember all those levers I was talking about that you can pull like the you know number of chunks that you return per query well all of these can massively impact the performance of your rag app so this can include important things like the size of the chunks how many how you create the document embeddings what model you use how you retrieve from the database and how you create your prompt how you integrate that information into the prompt and then with llms generally not all llms are suitable for all tasks so while they are generalist models different models may have specializations in different things they may be trained on specific data sets so like a really simple example is let's say you want to use a model for some reason for translating fan to Dutch because fan is quite a small language if you if your model has never actually been trained on Fran text then your llm is just not going to be able to do this translation it still needs the underlying data it's not magic it doesn't just create that information from nowhere and if it does it's a hallucination so if you don't pick the right model for the task or if you don't tune your applications correctly this can lead to poor performance it can lead to poor quality anwers and as I said it can lead to hallucinations and I guess this leads you with the final question how do you know whether the llm that you want to use is going to appropriate for your use case well this gets into a really huge and complex topic which I'm not going to have time to get into I'm just going to touch on which is llm measurement so a number of benchmarks exist for measuring how well llms can perform on natural language tasks generally and you can see the performance of various models on leaderboards like this one created by hugging face for open source llms however how a model will perform on your specific task is is really going to depend on the domain and the use case and potentially you may need to go and seek domain specific benchmarks or even create your own benchmarking data set so in conclusion llms are powerful but still limited models they're not on the brink of developing AGI but nor are they without application and only part of an empty hype cycle the trick is picking The Right Use case carefully tuning and building your deployment and making sure you measure your performance carefully and if this sounds familiar it's the same boring problems we've been dealing with in software development and machine learning for decades thank you very much