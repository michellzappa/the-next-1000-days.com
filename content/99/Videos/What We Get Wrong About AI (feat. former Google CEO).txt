Time to talk about AI. Right now, we're 
in this weird moment where lots of smart   people agree that we're on the cusp of 
this truly world-changing technology but some of them seem to be saying it's 
going to kill us all, while others are   saying it's more profound than fire... "You know, I've always thought
of AI as the most profound technology, more profound than fire or electricity..." It's clear at this point that something big
is happening. But my problem is, it's all just so vague. I want to know: How specifically 
would AI kill me? Or how would it dramatically   transform my life for the better? In this video, 
that's what I'm going to try to figure out, what the most extreme bad and good possible features 
with AI actually look like, so that you and I can   get ready. And more importantly, so that we can 
be a part of making sure that our real future goes right. "Artificial intelligence -" "artificial 
intelligence -" "artificial intelligence"  "the benefits vastly outweigh the risks" "eventually they will completely
out-think their makers -"  "AI to begin to kill humans -" "AI has the potential to change society" "and a lot of people can be
replaced by this technology" "Is this depressing?
I don't see why it should be..." "This will be the greatest technology
humanity has yet developed." To understand why you're seeing
so many mind-blowing AI tools all of a sudden, you need to understand how they 
actually work. And to do that we need to play some   chess. This isn't one of those "oh my god, AI beats 
a person" kind of games. In this game, neither of the   players are human. One is a famous chess engine, a 
system programmed by humans with insanely complex   rules for how to play the game. The other is using 
a very different strategy. And that second player   absolutely crushed the first... "It had learned the game without
any of those rules, it just watched enough games to
see what winning looked like." That is Eric Schmidt, former CEO of Google   and chairman of its parent company, Alphabet. Yeah. He was chairman of the company when they created
that second player, AlphaZero. "Before that moment all of the game playing
was done algorithmically, move here, evaluate this, do the math that..." But that's not how AlphaZero worked... "It didn't understand the principles of what a rook and a 
pawn and so forth and so on, it just knew how to   play because it had observed enough
games and it learned how to win." In other words, our best systems
had gone from using human-given rules to win,
to using observation to win. "So you can think of that as moving from algorithms
to learning. That to me was a major major deal." That ability to learn changed everything. It's what
makes incredible tools like ChatGPT possible today. You now know this technique as "machine learning" "Machine Learning!" "machine learning..." The reason that it suddenly feels
like "AI" is everywhere is because of the incredible success of machine
learning specifically. At a basic level, the idea is that instead of giving a
computer a rigid set of rules that says "if this happens, then these are the 
possible outcomes," instead you give a computer a   set of inputs and outputs and allow it to create 
the rules that turn one into the other. Meaning   that it might come up with rules that we didn't 
think of or maybe don't even understand... but making   the AI models that can do all the incredible 
things that you see now just recently became   possible. And it's because the computers training 
them have gotten way more powerful. Look at this   graph: So you see it going up and then around 2009 
the computing power behind AI models just begins   to explode. That change is largely thanks to a 
switch in the physical technology used to do that   training, going from CPUs to GPUs. My favorite 
way to show the difference between CPUs and   gpus is this Mythbusters demo back in 2009. That robot 
right there represents a CPU and it shoots paint   in these little sequential bursts. It can get the 
job done but it's slow. And this robot represents   a GPU so instead of shooting paint one little bit 
at a time it can shoot in parallel. Basically, the   physical tools behind AI are extremely powerful 
now and they're getting even more powerful, fast.   According to OpenAI, the amount of computing power 
used in the largest AI models has been doubling   every three months. This is why you're seeing 
now AIs able to pass the bar exam, make more   realistic images, answer more complex questions. 
It's why this particular type of AI technology is   "the risk that could lead to the
extinction of humans" "AI is a fundamental existential risk 
for human civilization." "How do we know we can keep control?" So we have this technology that can learn.
And it's learning fast. And so of course, in large part thanks to Hollywood, we
imagine that it'll learn to kill us. "My CPU is a neural net processor, 
a learning computer" but as much as these systems appear to
be human, they're not. Why would they   want to kill us? They don't want anything. And yet,
Bill Gates, Sam Altman, and hundreds of other tech   leaders recently signed a 22-word statement that 
shocked me. I'll just read it to you: "Mitigating   the risk of Extinction from AI should be a global 
priority alongside other societal scale risks such   as pandemics and nuclear war." That is an incredible 
statement, that the development of AI is in the   same realm of risk and importance as destruction 
by nuclear war. To better understand why they feel   this way I turn to this survey. This is the 
same one that's been widely reported as "half   of AI researchers give AI a 10% chance of causing 
human extinction." The specific question that they   were asked is, "what probability do you put on 
human inability to control future advanced AI   systems causing human extinction..." So what's going 
on here? Well the surveyors summarized an argument   for why AI might be so dangerous by saying "it's 
essentially the old story of the genie in the lamp,   or the sorcerer's apprentice, or King Midas: You get 
exactly what you ask for not what you want." Imagine this: In the future someone creates a powerful 
machine learning system and gives it the desired   output of a very accurate climate prediction. Then 
the AI, using its self-created rules, figures out   that the more computing hardware it can use the 
more accurate its prediction will be. Then it   figures out that by releasing a biological weapon 
there would be fewer humans taking up the valuable   computing hardware that it needs. So that's what it 
does and then it gives its climate prediction to   no one left. This is the category of thing that the 
researchers mean when they say "a system optimizing   a function of n variables will often set the 
remaining unconstrained variables to extreme   values." In other words, it might optimize for what 
we tell it to do at the expense of other things   that we care about. "You get exactly what you ask 
for, not what you want." The term that researchers   use for this is "specification gaming" and 82% 
of the researchers surveyed agreed   that it was an important or the most important 
problem in AI today. Specification gaming leading   to disaster becomes less likely if we work to 
contain AI systems and we don't let them get   connected to tools that might physically harm 
humans. Like don't give them the nuclear codes but how likely is anything like this to actually 
happen? I honestly don't know and I think neither   does anyone which is a big reason why all of 
those tech CEOs signed that letter and why you   might have heard people advocating for a pause 
on AI development. However, there are real risks   to not moving forward too. There's a fairly large 
and impressive group of people now advocating   for a pause on AI development.
What do you think about that?  I think it's a terrible idea
and the reason for that is that a pause would give time for our 
competitors which starts with China to catch up.   At the moment the US is in a very strong position. 
We have all of the top models, we have the majority   of the researchers, we have the majority of the 
hardware, we have the majority of the data that's   being used. That's not going to be true forever, 
but this is a critical time for us to build this   technology in American values,
liberal values, not authoritarian values." So we've created these tools that
have started to become so powerful  that we're concerned about
how well they might do what we ask   and at the same time every country, every company 
is incentivized to build them first with their   own interests in mind. But why should we want AI 
in the first place? Like what's the goal here?? In my view, the most positive extreme case for AI 
that I've heard isn't how much better or faster   it can do the mundane things that we already do 
it's how it could leap frog us to do things that   we can't. You might be wondering, how? Because of 
how incredibly good machine learning systems are   at pattern matching, they can sometimes give us 
results that we can verify are correct but we   don't totally understand how it got there. It's 
funny, it's the same skill that scares us is the   one that gives this tool such incredible potential. 
And if you're feeling a little bit skeptical here   that's totally fine and understandable, I was too, 
until I heard this example: In 2021, researchers   used machine learning on a problem that had up 
until very recently been called "one of   the most important yet unresolved issues of modern 
science." It figured out the structure of a protein   from just amino acid building blocks. For decades, 
our best effort to do this has been to spend   hundreds of thousands of dollars per protein to 
shoot X-rays at them all in the hopes of learning   just a little bit more about our own bodies and 
make better medicines. This is how we got new   treatments for diabetes and sickle cell disease, 
breast cancer and the flu, but then researchers   fed pairs of sequences and 3D structures that we 
already knew into a machine learning system and   allowed it to learn the patterns between them. 
And the result was just incredible. We now have   predicted 3D structures for nearly all proteins 
known to science, more than 200 million of them. "Deepmind's AlphaFold" "AlphaFold" "AlphFold was able to do in a matter
of days what might take years!" "solving an impossible problem in biology..." I get a little emotional just thinking about this about how many people's lives might actually get 
better because of this knowledge explosion. And this is just one example of what we've already 
been able to do. As machine learning systems get better and better, people have extremely high 
hopes about what we might be able to use them for... "We have lots of problems in the world. Think about 
climate change, for example. Climate change will be   solved to the degree it's solved by using techniques 
that are very complicated and very powerful that   will have as their basis generative AI.
And I think that we want that future." After learning more about AI
and this moment that we're in I think I've figured out why it
feels so confusing and so hard: We're living inside a trolley problem. Down one path is the status quo,
life without AI. But with this incredible new tool we can pull 
ourselves onto another path, one that could   fundamentally change society. But we just don't 
know, at what cost? Will AI give us what we ask   for or what we actually want? In this video, we've 
only talked about the most extreme futures with   AI. In other episodes, we're going to go deep into 
specific applications. We'll go full on Huge If True into AI in music and news and robotics and 
climate and food and sports and more to explore   how these tools might transform our world. It's 
easy to dismiss it as crazy when you hear someone   say that AI might be "more profound than fire or 
electricity" and while the cynical side of my brain   wants to say that it's probably true that most of 
the most ambitious AI efforts will likely fail, the   more optimistic Huge If True side of
my brain just keeps wondering: What if they actually work?