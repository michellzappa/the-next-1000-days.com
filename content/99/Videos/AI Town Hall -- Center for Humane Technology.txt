I'm random Fernando one of the co-founders of Center for Humane technology there are many many hours of content we could share today with you about AI so I've tried to focus on what we think will be of most service to you including the principles Trends themes and you know from nearly 2 000 questions that you all sent in and I also want to address some of the common myths and misconceptions along the way you all represent a very wide range of backgrounds so not everything that I cover will apply to everyone but I promise we'll have plenty of helpful material for for anyone um okay so what we'll cover today are capabilities of general purpose artificial intelligence some backgrounds on how large language models work so these are llms where things are going wrong Ai and the economy deepening disparity and pads forward so first of all in case you haven't seen it yet please watch the AI dilemma by Tristan Harrison is a Raskin if you haven't already that talk covers a lot of interesting material and this talk Builds on that and assumes that you've already watched that so what are we talking about today uh we're talking about general purpose AI so that is AI with uh generative capability so text images audio video Etc the ability to reason and other enhancements and we'll also talk about hallucinations and how that is still a problem um first a quick recap of generative AI so this is an image from the journey 5. it looks great of course and this is kind of where the state of the art is and we're very good at generating these you can see the massive Improvement right so V1 here is from February 2022 and V5 is from February 2023. and there's a massive Improvement in performance uh in that time and you can see you can learn a lot more about this there's a great channel uh on YouTube called AI explained that covers a lot of this material and this example comes from there it's fantastic we can also reproduce voices really well we can make songs right and we can reproduce voices with just three seconds of audio and if we have more samples we can do even better um we've had massive jumps in performance on exams and these kinds of you know tests and actually a lot more has happened even since March 2023 when gpt4 and these kinds of results were released and you can see the Jump Right In uh from GPT 3.5 to 4 on the bar exam went from 10th percentile to 90th and on GRE quantitative went from 25th percentile to 80th so there's a lot of capacity in there and um the other thing is there have been there have been incredible number of plugins released recently so the key Point here is um these Technologies can now interface with all of your favorite apps and it's kind of supercharging the economy we'll talk a lot more about that and kind of the the risks there and it includes apps like zapier which is actually integrated into 5000 apps so through that you can control like a vast space of possibilities and of course that also opens up a vast space of Harms next I'm going to go through a few examples related to reasoning and general purpose AI because this is an area that's often misunderstood um or sort of underestimated and it's a moment it's an interesting moment where there is a ton of low-hanging fruit so researchers are publishing like crazy with consequential new papers coming out every few days so here's one example right um when you give a an input like a prompt or a request the model actually runs through a whole tree of possibilities explores different paths before it answers and it kind of scores those paths and says oh like this path with the the dark green boxes that one scored really well so that's the one I'm going to use and that makes that automatically makes the model a lot smarter another example ignore all the details here but the idea is you can ask an llm sorry to show its work and you can have a separate model rate the intermediate steps so here you can see those intermediate steps colored right the the dark green is like oh this is not I'm not sure about this the red ones are just wrong um and so you can see on the left the sequence is all bright green all the answers were were scored well and on the right side things went wrong and we're able to figure that out and based on those ratings you can figure out whether the final answer is a good choice the point is both of these can improve performance by many factors depending on the task there's a cool thing called code interpreter that can do math and data analysis and visualization interactive graphing basic image editing and Analysis um so this example the there's a data file like a spreadsheet that's uploaded and the user asks hey can you create a bar chart of the top 10 most frequent artist names and you can see here are the results right it kind of does all that work for you creates the the labels and the axes and this is a very basic example there's so much more that it can do that is now being unlocked it's not available the code interpreter is not available to everyone yet but again the AI explained channel has a great demo of it the video is called 12 new code interpreter uses so if you're curious you can check it out there one of the problems that these models have is that there are uh there's a long-term memory problem it can only take in so much input and it can't remember right across different sessions large amounts of information so people have worked on that and there are various workarounds and I'm sorry about that um the basic idea is that you could come back and it would remember things that you said in the past right so in this example um this guy Rick is chatting with the the GPT and the GPT is able to remember that uh what his favorite city is for example then there's these classes of um kind of more autonomous uh gpts so Auto GPT automates the steps needed to complete complex tasks so the user is able to give just a fairly basic high level input and the model figures out all of the rest it figure out the intermediate steps so here um the user says I'd like to order a pizza and the model is listening and it understands the audio and the chatbot says okay what type of pizza would you like the user says okay 11 inch pepperoni pizza regular crust and the AI does the rest so here's what's going on behind the scenes um it looks a little complicated but the idea is you can connect the thing to take away is that you can connect all of these different apps and the llm can figure out how to use them correctly right and so for example here um there's an app called Bubble that allows people to build custom web apps without coding it can connect with the other apps elevenlabs.io creates realistic text-to-speech twilio allows you to make calls and pull audio from a call and the gbt is able to drive this process autonomously that's pretty cool um and the next thing is is learning agents so again you can have there's an example of this thing called Voyager which was exploring this gaming world of Minecraft using gpt4 for planning and developing new skills so it's able to go do various tasks inside of the game the key Point here is that gpt4 is used to do the planning to do the exploring to do the learning of new skills and it shows how you can have different types of autonomous agents tuned for specific tasks and of course you can mix all of these llm specific techniques with traditional programming techniques and data structures and so this again opens up a vast space of interesting possibilities you know kind of cool things you can do but also harms and you'll find this is kind of the recurring theme throughout the presentation uh despite all these improvements llms still have what's called hallucinations so a hallucination happens when an llm produces text that's factually incorrect and typically L M's do this with a lot of confidence and that's kind of the joke right that it it says whatever it says with striking confidence and when it's right that's helpful and when it's wrong it's a big problem um sorry so so why do these hallucinations happen uh there's a few reasons so one is when it's trained it actually doesn't know what the actual truth is it just learns based on the patterns that it sees and it does that fairly intelligently but it doesn't always get it right um the data that it trains on could be noisy or inaccurate in one way or another the models don't actually have larger contextual understanding or they don't understand the world fully they only they only know what they've learned which turns out to be a fair amount but again there are some important gaps and these models because of the way they work they tend to Value fluency and coherence over accuracy sometimes and so it'll it'll it'll give you a response that reads very well but is actually just wrong and to be fair this has improved a lot from gpt3 to gpt4 as an example um but it's very hard to get rid of entirely for many reasons but a big one is that actually when it generates text there's a probability for each piece of text that it generates so it doesn't generate the same results every time so it's not deterministic and that's a big problem also one hallucination can lead to more because previous outputs are factored into future outputs so when it's generating new text it kind of looks back at what it already said and if what it already said was wrong now it can be steered kind of off the path and be very unhelpful and of course there are big risks when you when you put all this together big risks because Society is in this phase where AI is being integrated as rapidly as we can into everything and so uh when we have the hallucination problem and we actually have um models that perform pretty well then there's a lot of risk because we can start to rely on them more and then when they fail they can be catastrophic coordinated failures and actually um this is something that was mentioned by the head of alignment at open AI that when we scramble to integrate everywhere we have to pause realize that the technology is immature and that this this hallucination problem and other problems um we'll talk about those can can all show up in in a large scale across our society because we've run ahead and implemented everything and of course after this was said all the plugins for external apps were released so um people are really trying to to mention the harms but the race is ongoing and we'll talk a little bit about the Dynamics underlying the race as well so key takeaways for this section we're not talking about just chat GPT or generative AI but general purpose AI and here generative AI is kind of a subset of general purpose AI I'm using a definition from the European Union I think it's it's a good one um this is not yet artificial general intelligence or AGI but it is improving very very fast people Define AGI in many different ways but getting sucked into that debate actually distracts us from dealing with harms from the actually very capable AI that we already have so these Technologies can be used with an infinite set of external tools and programming and they're improving very quickly but they still hallucinate confidently and unreliably and in some cases right the performance really is like 10x per year with a combination of software and hardware and new types of training and and new types of prompts all that stuff um is driving a lot of improvements so let's talk a little bit about fear-mongering and hype so this comes up a lot when we when we talk about Ai and first of all understanding the situation accurately is critical this is kind of the fundamental thing um and what we're talking about is general purpose ai's rapid integration across Society not just generative AI sometimes people forget that and they talk about the chatting thing or the image generation thing and actually the the broader capabilities um bring in a lot more risks we have to understand its capabilities and dangers right in order to set the right rules so does hype help or hurt it's mixed hype definitely helps companies because the tech actually is very powerful and it can do all kinds of fun things and and it's highly marketable so of course they're building big campaigns you you'll see this in a lot of advertisements these days right every company every CEO is talking about the benefits and the cool things that can happen and how we can save time and improve productivity and grow the economy and what happens is when these products get released little by little and they actually are kind of cool um the public can get enamored with these exciting new tools and that makes it much harder to get not just regulation because there'll be some kind of Regulation but the question is whether it will be adequate to protect Society from the large scale of harms that we're talking about tech companies tend to there's this Playbook that happens where they build powerful technology it gets broadly deployed and adopted and then there's this like ask for help regulating what have become impossibly hard problems and we're sort of on the path to doing the same thing with AI um so where does hype kind of help us a little bit well sometimes if we can understand these these capabilities well and understand the harms well the warnings about the dangers can actually help to drive public pressure and get the right protective measures in place as an example right the US government tends to establish Tech policy more reactively as opposed to proactively so you kind of need these this large-scale public pressure in order to make adequate change so why fear and danger I mean is this kind of similar to climate right there is big risk at play both in the immediate term and in the long term and we've got technology that has already exceeded the limits of our sorry not sure what's wrong with my throat our physiology our governance and our natural ecosystems and AI has kind of supercharged this whole process so we need Swift action so why is this race happening and I think a lot of this comes down to misaligned incentives so let's talk about that misaligned incentives create a downward spiral to misaligned technology these aren't in any particular order but they're kind of interesting to to look at so they're systemic accountability right we don't want to get fired we don't want to lose face money power or whatever it is that we might be holding on to whoever you put in the CEO spot regardless of how much you think they're a good person will have to fight those pressures there is this kind of accountability right to board shareholders and the economy that everyone has to deal with there's The Usual Suspects of money power social status and recognition these are pretty common um sometimes people find themselves with a certain kind of Lifestyle right this could be money this could be status that they need to maintain the competition is a big one people underestimate the desire to win the desire to be part of a winning team and actually effective CEOs tap into this and they get their teams aligned and executing crisply against a common opponent and by the way this this race for competition this competition um includes the race to publish research the thrill of Discovery this is a big one right we all experience forms of this in our lives we might discover a new recipe or a strategy for our favorite sport and for many technologists and researchers this is actually the deepest drive and these are normally people at kind of lower levels right but a few years ago AI Pioneer Jeffrey Hinton was asked why he was doing the research despite the risks and he said look I could give you the usual arguments but the truth is the prospect of Discovery is too sweet and I think he was just being candid and honest there but that's just something to keep in mind when we think about how this is all playing out cultural conditioning right what did you learn from your parents from your education what do the people around you talk about not being aware of our conditioning creates blind spots and it's easy to chase some of these kind of false incentives if there's this idea in your subconscious that tells you that when you when you get this done finally everything will be good this is kind of a deep point but when people have so much power this kind of self-exploration becomes much more important in practice all these incentives align this kind of sorry all of these incentives misalign and they magnify what's already a rapid amplification and distribution process without sufficient risk assessment and of course that's bad for society so next I want to quickly talk a little bit about how some aspects of how llms work so first of all there's you know this idea of algorithms and transparency so the traditional thing when you think about an algorithm people think of something like this this is a flow chart where you get some input there are some decision points and then you kind of make a decision right the algorithm says approve or reject to this person for a loan for example um and here's a simpler version of that right you take an input make a decision and give some output big number or small number very simple you can look at it and you can understand how it works now on the right side we look at a neural network a neural this is a very simple neural network neural networks are modeled on the human brain and just like our neural Pathways get weaker or stronger over time the connections between these neurons get weaker or stronger as they're trained so there's different numbers that represent the weights um and then so once the model is trained based on the input different neurons light up right inside of the network based on those weights and then you end up with a value so that's the red the red output at the end with llms the output is the next piece of text so that's a very simplified explanation but the point is the the workings of it are quite complex so it ends up effectively effectively being a kind of black box Black Box neural networks are used all the time in in systems like social media and recommendation systems but the new forms of general purpose AI are a lot smarter and this is a simple example it actually looks more like this and actually it's more like a million times this seeing inside is hard but we can so just to give you some intuition for how you could do that um you could visualize right how the nodes light up just like we do fmris of of brains you could do techniques like the step-by-step reasoning that I showed a few slides ago but there's challenges there because sometimes the steps that it reports are actually not accurate and so we can end up developing false confidence that a model is actually doing what we want but it's not um next I want to talk about some ways that recent llms encode a sense of you know meaning or understanding because people often talk talk about them like they're just repeating patterns of words without any understanding so take this example we've got five English words and three kind of axes of flavor right to these words so genuine right is a positive word surreptitious is not so much subtlety uh surreptitious has a lot of subtlety in it enthusiastic kind of doesn't intensity enthusiastic is a very intense um kind of uh word so by understanding those kinds of flavors llms can start to group words or concepts with similar characteristics so here you can look at Dimensions like age size and energy consumption so all the animals will be close together all the villages will be close together and in reality it's kind of incomprehensible right llms have hundreds or thousands of dimensions and so we can only understand you know not three dimensions like here and so we can only understand them using like big tensors and complex mathematics but you get the idea another example position matters so in this case where we put the word only changes the meaning of this sentence quite dramatically right so the models factor that in and they understand the different words the different positions mean different things they also understand which words in the input are important and they also look at the relationships between all of the different words as it's trying to as they try to predict um what the next piece of text is so in this example um the the words are colored based on importance So based on the model's training it knows that although reigned JoJo decided and go are probably important words and the orange words are kind of important the yellow words are less important than the gray word you can probably ignore it and B it uses all that information as it makes the prediction of the next piece of text um the next thing is about developing kind of deeper understanding so in the course of its training a neural network builds up lots of connections and weights between all of its layers when we ask questions different parts of the network light up based on what patterns they've been they've picked up during training so in this example right the early layers might have picked up something about basic sentence structure the middle layers might understand something about syntax and semantics the later layers might understand a more holistic kind of structural level and so again the details are are complicated but this is a very rough way of explaining how llms understand language so well and again remember these networks are massive so they are able to pick up all sorts of things uh that that humans might have a harder time picking up the last piece is that there's an important piece of in the training process models like chat GPT they do a step called reinforcement learning with human feedback so in this step humans help the model to make sure that it's its reward model is actually rewarding the right behaviors and that helps to produce more nuanced responses better alignment with human values because the human feedback right can kind of interject some of those values adaptation to new scenarios that didn't show up in the training data error correction so fixing problems that may have shown up in the training kind of erroneously but because humans are involved just like with the rest of the training right this is another opportunity to introduce bias ambiguity you can get over optimized Etc but just just want to mention this step because I think it's very important so key takeaways llms operate with a lot of opacity they have much more understanding than just repeating word sequences from their training so now okay so so far there's a lot of things about kind of how powerful the the models are but actually there's a lot of things that are also going wrong so let's talk about that um here are some of the current just recent harms right over the last months and you can see across many different domains right really uh dangerous things that have been happening here's some more um and just recently right there was the fake Putin example um where he there was a fake media and the Cyber attack as well so that you can see how these combinations can be quite dangerous this is a chatbot explaining how to make Napalm after the user figured out how to kind of jailbreak it right to kind of break it out of its restrictions here's an example where an AI chatbot chatbot is now integrated right into Snapchat and it's always there right even though your friends might not be and here's an earlier example where this kind of chatbot was helping a 13 year old plan a romantic experience with someone who was 18 years older and so this has all been rolled out right there's 375 million users Snapchat has fixed this particular example but the point is these kinds of examples will always come up this is very immature technology but yet we're shipping these we're shipping this technology widely it's kind of crazy there's another military example if you want to look it up but this is sort of like in summary it's like playing a computer game and and you're able to give um make requests and give orders and and sort of plan plan how you're going to execute um in conjunction with the intelligence of a chatbot um this demo is kind of incredible right but it also gives you an example of uh this is just the tip of the iceberg there's so much more that's happening that is classified and then there's robotics so here's an example from Europe [Music] so you get the idea right the point is we are hooking these up to increasingly more powerful Ai and are we going to be surprised when bad outcomes happen I think that's just it goes from science fiction to like real possibility um a different manufacturer called ghost robotics teamed up with sword International to produce this robot dog with a custom gun in 2021 so again right there's this kind of rush to integrate everywhere and just want to paint these realities there's a conversation of net good versus Net bad right there's incredible potential um obviously we're going to have the ability to have new new drugs new Material Science new Energy new climate Solutions and this is all could be very exciting and very good for us but if we are Reckless with the deployment of AI and we have ai that floods democracies with synthetic media accelerates the development of cyber weapons and disables actually critical infrastructure and then we decentralize right Advanced AI to extreme actors there's a lot of possibility for things to go wrong and we won't get to all the good things because we'll get stuck we won't be able to get through the door and also right all of our societal kind of structures right law religion culture relationships all of these are built on language um and we've now given out the keys right to that manipulation another note that's important is there's a lot of nuance to this but with open source traditionally again we were we were there's a lot of value in open sourcing kind of procedural algorithms where we could see inside see the flowchart but when it comes to neural networks open sourcing is a lot it doesn't mean as much right it's sort of like um you don't get the weights you get the source code you get sort of the the training process but you don't get the weights the weights are sort of where the magic is and so there's some risk people have also gotten weights and when those weights are decentralized people can actually reproduce the functionality of the original model there are also a lot of new models that are able to run on simpler you know commodity Hardware and run very well so we need to find novel solutions to bind power and responsibility again we see the pattern we've seen the same pattern with social media companies where the investments in safety are very small compared to the investments in Innovation it's not that they're not happening they're just woefully inadequate for the scale of what we're releasing into society so then there's this topic of extreme risks and I know there are many people who feel like this kind of discussion of extreme risks detracts from Real World harms that are unfolding right now and that's true in many cases like that does happen um it's also true that in many cases the people who emphasize extreme risks and are really concerned about it they tend to have quite good lives and so for them these extreme risks are the ones that would really upend their lives and it would take them to a situation where they don't feel safe or they don't have reliable income so there's this language oh we're all in the same boat but actually not everyone is equal in that book right many people on that boat already feel unsafe and don't have reliable income and they'll also face those more distant risks so there's this reality right where a lot of resources get allocated differently based on these differences and real world immediate harms often get less attention partly because they're sort of less glamorous right they don't get the headlines as much and partly because they're really hard they involve deep societal problems that are difficult to address so we're going to talk about both today and one of the things I wanted to show people often ask about um they can't visualize this Pathway to extreme risk and so this table shows a bunch of different examples I won't go into them all but you can see many of them are already possible now kind of deception persuasion um political strategies and as we hook these Technologies up to more and more of society there is going to be more and more obvious exposure and we need to be really thoughtful about that and if the technology is taken over by a malicious actor or the technology itself makes a bad decision we've opened the door to very large consequences so all this is driven by what we call multi-polar tribes right these are competitive environments where everyone's engaging in harmful Behavior not because they want to but because they'll lose if they don't so AI companies will lose if they don't rush Innovations out to Market someone else will take their market share so this is kind of one of the challenges um this is a complex topic I just want to put this slide up for those who want to pause the video and look later there are ways to get out of it and so there's forms of this that we need to do and and are in progress as well so key takeaways order matters we can't get to the good stuff if we let Society break down um AI demands novel ways to bind power and responsibility and we have to attend to both today's harms and future arms and we can't just forget uh one or the other so what about the economy right how does this how does this all play out um most of the world uses a mixture of capitalism combined with sets of rules to make sure that it's playing you know somewhat fairly capitalism has done a lot of good things right it's it's built it's given us longer healthier lifespans give us access to food clothing shelter information intelligence you know this kind of virtual intelligence connection and there are some other things about it like it's less sensitive to who's in power because it's like a machine that just runs itself so whether you like the person who's President right now the machine still kind of runs um it leads to to Rapid process uh sorry it leads to Rapid progress and that's driven by a conversion of resources right natural resources or our minds or labor into consumer goods and services so inherently it's about extracting value right from those resources and reallocating that value but the problem is obviously it only benefits people who have the tends to benefit people more who have more resources to start with and so in this process of extraction we've we've exceeded many of our boundaries right nature our health our minds and even the functioning of democracy and Technology accelerates this process AI is now the greatest accelerator we've ever built but but what ideas are we actually accelerating here are some of those ideas um these ideas like growth is good one can own land nature is here to be converted for human purposes and sometimes even other people right are grouped into that right this idea that they're a stock of resources to be converted for one's purposes um these are kind of terrible things that we've done in his in history and still continue to happen in parts of the world um and people are perfectly rational right and of course they're not but but these ideas they're not just ideas they become cultural assumptions and then they become baked into our institutions our policies our laws and actually into the prevailing mindsets that drive behavior and drive technology and so you can see this kind of extractive Paradigm you may have seen this we like to show this a lot um these kinds of assumptions appear in many domains of technology and they they apply to AI VR and web3 social media wherever um anyone yeah anyone who's trying to grow things quickly tends to have this set of um mindsets so what happens right in the economy when companies get good at something and it's useful in a small dose so say like a shoe or a drug or a social connection or some kind of nice drink um when we tie that production and growth to metrics and then we try to do the infinite growth thing it's easy to lose track of our values and so what's going to happen with AI the vast majority of companies are going to look at how AI can accelerate their existing operating models and Investments so that's going to bring incredible and incredible reduction in friction for consumption which also means an incredible increase in harms and extraction so we've got to be really careful here um another concept here is when we turn a tree into Lumber we give up right the way the leaves produce oxygen the way that the uh the The Roots right protect from erosion the the way it provides a habitat for animals or food for animals right all these things we do the same thing with children and we give up right sleep we give up relationships with family and friends um all and and learning all because kids get stuck on devices all of this comes at a cost right billions of air conditioners so we've got two billion air conditioners 1.4 billion cars 1.5 billion cows and when you put that all together with all the other things that we're doing um it adds up right all the emissions add up and it leads to much more energy being stored in the atmosphere and as a result we end up with 19 of the last 20 years of our the hottest on record we've got fires droughts hurricanes that destroy homes and displace people globally and of course so this goes up exponentially and the harms are unequally distributed so of course the poorer people end up shouldering more of the harms that come from all of this this whole process and so it's like wearing a car and we're accelerating off a cliff and AI is going to accelerate this car even faster and sometimes people forget that we're all in the car right even even the wealthiest and most powerful people are in the car and so we have to find responses that are actually adequate not just letting off the accelerator like donating one percent of profits which actually just still driving off the cliff how do we turn the wheel so that involves changing our economic incentives leveraging our finding ways to change our mental operating system and the unsustainable ideas that so many minds are living by so business as usual and certainly not AI accelerated business as usual isn't going to work so there's a useful equation here this is just kind of an approximation but I think it helps conceptually competition and extraction right when we exponentiate them with technology we end up with existential risk right this is a situation where the rules of the game like our regulations can't keep up with what we're building and so we end up destroying the ecological physical and mental foundations that Humanity relies on and you know when you take this to an extreme extent that's when you get this existential problem we can just use the word catastrophic if you prefer but this is kind of the model a useful mental model so because price is always at the center and it magically folds in Supply demand costs and all of these things it has some benefits but as we saw earlier with the example of the tree and the lumber price leaves out many aspects of what makes nature and people really special and it's very dangerous but so what happens is if we want to make a difference we need to affect price and again I'll leave this up um this is something that that as a field as a group right we have to do effectively right we have to have an agenda that we use to drive external pressure so these are penalties to price right through media through policy law education but we can also um inspire better Behavior right we can kind of subsidize a move in the right direction through product and culture change through training through mobilization uh movement building all of these things and when we think about aligning our institutions with our technology we have to remember there are two key faculties that we can't lose sight of sense making and choice making but what happens is increasingly these are mediated and often corrupted by our technology which is driven by perverse incentives and that has horrible consequences so the complexity of the issues we're facing is going up rapidly but actually our own technology is making it harder for us to make sense of this complexity and so there's a wisdom Gap and this is kind of the challenge for all of us to kind of help to help to solve um and there's a lot of work being done in this realm of public interest technology as it's called so I've put some links here there's a public interest technology University Network there's the Council on technology and social cohesion that we're part of the Civic Tech Field Guide so all of these are things you can you can check out uh there are alternatives to GDP GDP was invented um was not invented to do all the things that it's being asked to do now so there are other ways to do this so I've listed some of them here there's also work on alternative economic models by folks like donut economics the new new economy Coalition sacred economics the work that reconnects Circle economy I'll add a slide on this later but there is this is a time to really look at these Alternatives because we are going to need them so key takeaways right this kind of equation helpful to remember a price-centered economic system is really only sensitive to price so we've got to figure out interventions that connect to price and AI demands it's an opportunity but also it's something we're just gonna we're gonna have to do these are conversations that people have been having for a long time with kovit with climate with inequity and this is a time when we we've got to jump um Jump Ahead to do this next is the topic of deepening disparity right leaving behind people who have less and who aren't kind of the common case in society so um let's talk about jobs this comes up a lot right so massive job losses are likely right in the short term um huge productivity games will will happen for anyone who's using Ai and this is the thing everyone's going to kind of get drawn into integrating these Technologies um soon after that we'll have lots of layoffs right because we'll get good at using AI and there will be more impact on cognitively intensive jobs and that correlates with higher education levels um I'll explain a little more about the layoffs in a moment as well just in case that jump wasn't clear and this is a big psychological and Status here right for people who are used to like thinking jobs and and being in this kind of certain portion of society um that would be that would be hard there's a less immediate impact on physically intensive jobs but also keep in mind that robots are being trained and are increasingly capable so in the end there will be fewer jobs more labor competition and lower wages for everyone who's left on the bright side since I know um there is only this kind of bright side I guess is the cost of producing goods and services will drop but of course that's because less humans are involved so when you hear that AI can partly help you with your job that actually will translate to lost jobs and Sam Altman explained this well in one of the talks there's a huge premium on coordination on you know health benefits all of the things that we do to take care of people if you can make one person twice as productive that replaces not two people but more like three or four and so when you hear this question um this kind of statement oh AI will create new jobs that's kind of the wrong point right the question is how many more jobs will be lost and gained what's the net and what's going to happen to all the people who are left out and the answer is not retraining retraining is is way addresses only a very small percentage of that group and also now there's no there's no new frontier to grow into that's kind of obvious like in the past there was knowledge work cognitive work right that we could grow into as we got more industrialized but this time it's not clear what that would be and no one's I haven't heard any good answers for that yet um I'm going to skip through this because there's a lot of detail here but again anyone who's interested can can pause the bottom line is there was some analysis done by open AI um on on jobs and basically there is a vast number of jobs with a large exposure and a large amount of potential Income At Risk and that's kind of the bottom line um and so now let's talk about some of these immediate harms that we're talking that that come up um so let's say we have a sophisticated AI model that analyzes this guy right Michael and tells us if he's eligible for health coverage or a home loan or disability insurance this can go wrong in many ways right first of all the model may not work well this may not be a good model for him the I could hallucinate As We Know um most commonly the problem would be that the model will try to fit Michael's characteristics to pass data that it was trained on so any negative or positive factors that are encoded in that past data will influence the result from Michael which ends up pushing Michael's future in the direction of that past data right and so for example to make it concrete if if Michael's kind of characteristics match in terms of race or ZIP code or education with people who had high Health premiums he's likely to get a high Health premium too even though he might have very different circumstances that the model doesn't understand so that's one aspect there's another aspect which is um so let's look at another example right where we're using again AI to model health insurance premiums this map shows the correlation between coveted cases per capita so those are the blue circles right bigger Circle means more cases per capita and pollution which goes from green to Red in the LA area so the model could learn to predict that people living in highly polluted areas get sick more often and therefore they should pay higher insurance premiums so this is how a smarter AI could actually lead people to unfairly bearing a burden for societal problems like pollution that they can't control so there's this trap right this is a great example of how getting better and better actually causes a different kind of values problem across society and again affects people um there's a very large correlation between people who are poorer or marginalized in other ways and these kinds of very sophisticated narrowing effects and of course all the other things that happen with social media the kinds of harms that have been happening will continue to happen I just wanted to give these two examples and then one last thing about how AI accelerates social defaults and kind of conditioning so when AI is widely integrated across Society it conditions all of our minds right because we end up interacting with it all the time and that has an effect so lots of defaults and stereotypes are actually baked into the training process are baked into the AI sometimes that happens in obvious ways and these are the things that people often talk about uh like gender race economic status age these are kind of more more obvious but a lot is more subtle and so an example would be like accent certain accents are perceived as smarter than others or marital status right where marriage would be deemed as good or stable and the model will learn all of this stuff another example could be how leaders behave how they speak right the structure of their sentences the gender of leaders uh who drives the narrative in movies right certain types of characters with certain types of characteristics and so the AI learns those patterns but when people interact with it people start to learn those patterns as well this is not about hallucinations right this is about encoding social structures social defaults and stereotypes into the data there's a great discussion of this as well as a lot about Solutions in a paper called on the dangers of stochastic parrots so I'd encourage you to check that out just note that the paper was written before the release of gpt4 so some of the model capabilities have grown since then but this aspect is covered really really well and the growth in in AI capabilities is a problem that every paper has and and every talk is going to have as well so recap big job loss is likely more for higher education levels big wealth inequality um AI reinforces past patterns and can over tailor risk assessments and it strengthens societal defaults and stereotypes so we end up with this kind of catastrophic mix of conditions I want to recap we've got frenetic sorry frenetic Innovation where companies are seeing the opportunity of a lifetime we have synthetic media where we can't make sense of the world well reputation and trusted relationships will become extremely valuable in that world we have distributed access every scam requires less time and becomes more profitable malicious actions become easier to do we can't even keep spam calls and texts from our phones so just keep that in mind as we think about the explosion of possibilities that we are now allowing and Rising inequity inequity itself is a catastrophic risk because it breaks the functioning of economic systems and democracy and introduces harmful threats affects mental health there's all kinds of problems the last thing I want to talk about is when we have this kind of innovation and we also need Clarity and we need Clarity on why we're doing it in the first place what our definitions are of kind of thriving and happiness um what problem are we really solving and I think conversations around alignment have to include better access to food clothing shelter medicine Mental Health and the reasons why solving those problems aren't well incentivized we don't want to just be hamsters you know on the Wheel of Life bouncing between wanting what we don't have and disliking what we do have and never being content and this is a lot of what the economy does to us there's many aspects of thriving that we need to understand so a balanced relationship to pleasure pain and meaning gratitude creativity learning balance respecting the flows of life acceptance of challenges that come our way care and kindness this doesn't come up that much in the conversations around technology and it really needs to quality relationships reducing regret um I highly recommend the thriving and centering values modules of our course it's a free course foundations of Humane technology and that content will help you with all types of Technology including Ai and social media so consider checking it out and signing up but if you want just watch the videos which are just on the website you can just click into them a human rights lens is also important right looking at the basic capabilities that humans ought to be able to have we can't lose sight of that in the conversations especially when we talk about AI alignment this has to be a critical part of the conversation as well as who is at the table for those conversations and again the fairness and Justice module of our course has a lot a lot more about this there is a risk where we end up in a world where artificial pet rights become more important than real human rights that sounds like a joke but it can really happen and again that's because people with fewer resources aren't served well in capital-centric economies and as we've seen that's going to become an increasingly larger number of people the last thing I want to touch on is AI and kids because there are a lot of questions about this uh relationships are the entry point so social media phones toys media games all of these are going to be supercharged um with AI right because AI is going to allow much deeper relationships to be built in terms of interactions especially listening and reacting and kind of generating interactive like interesting text and in many of these domains AI is just going to be a tool that's going to increase harms and create huge opportunities to hook kids and create big dollar opportunities I think that's that's a fairly obvious thing that's going to happen there are some good things that could happen so mental health support could become much more accessible um but there's a risk of AI chatbots being tied to the wrong places right so if the chatbots live on social media that could be really dangerous because they have perverse incentives and and again I want to get there's no substitute for like talking to a real human therapist but the model that we have of very expensive therapy and doing that like what once a month or like once a week at huge cost it's not accessible for a very large number of people and so the people who figure out how to get these kinds of models um implemented well there might actually be something valuable there again big asterisks right but just just trying to name some of where there there is hope um in education there's there's broad opportunities to to be helpful the way that kids can learn can be much more interactive and learning is going to be much better that way and the other interesting thing is like so I'm from Sri Lanka and you could have kids in Sri Lanka having access to some of the same stuff like through chat GPT right which is kind of interesting and again there will be always like new levels of inequity where schools that are better resourced will do this in in more sophisticated ways but overall there is some possibility that there could be some narrowing of the gap um and so for example kids who have parents who who can't attend to them as much because maybe they're working two jobs or three jobs like the opportunity to learn um with a partner that's interactive and knows all the material quite well could be could be profound um I've seen some demos of you know Khan Academy and trying to do this kind of thing well so again big asterisks because it's all it all remains to be seen But there are some opportunities lastly there's this point on resilience I think um it's dangerous for kids to become accustomed to getting only what they want only the experiences that they want avatars that look great and sound great and always support them and everything is positive and easy this is a challenge for for adults as well um and so there's this challenge where instant gratification at an all-time high means that resilience is at an all-time low so we've got to be really really careful about all this much further could be said but I just wanted to touch on that so just a quick recap of solutions and people always want to kind of remember that when the within the existing system interventions have to affect price remember that kind of rainbow color diagram right you have to mix all of these different pressures um and incentives to get things to change we are going to need Global coordination and we need safety Investments commensurate to the investments in growth we're going to need heavy input from social sciences and wisdom Traditions to figure out the values problem right how can we align on values we have to figure out how to Center Human Rights and growing disparity all the time that can't be something that's thrown aside or said that it'll just be benefits will trickle down because they really don't they don't trickle down well we need to Center values explicitly and we need to figure out how to transition to new systems so how can you help one is learn more about AI watch this talk again watch the AI dilemma again if you haven't take our course here's a good ml safety course machine learning safety course if you are kind of interested in more advanced stuff our friends at Altec is human have a great reading list that you could check out speak up where you have the opportunity but do it in a nuanced way represent multiple sides of the argument and that's what we've tried to do here um weigh in on forums and discussions speak up where you have agency there's lots more we can say but that is all we have time for today thank you so much for your time and attention uh and now we're going to do breakout groups so Andrew is going to put some instructions in the chat uh for how you can join different breakout groups where you can kind of discuss with other people kind of your impressions and your questions and all that stuff so yeah and we'll try to do more of these maybe um let us know thank you again take care bye for now