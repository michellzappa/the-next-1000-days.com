MODERATOR: In session. We have Sanjeev
Arora from Princeton, who I think needs
no introduction, but has both done a lot
of foundational work in theory of computer
science and learning theory and, more recently, has been
explaining deep learning, and is going to be talking to
us about emergence in language models. [APPLAUSE] SANJEEV ARORA: Thank you. Yeah, great workshop. And yeah, this is joint work
with fantastic collaborator Anuridh Goyal. If you haven't
heard of it so far, you'll be hearing much more. And I just also wanted to
advertise this new unit that was started at Princeton,
which is Princeton Language and Intelligence, which is
devoted to large AI models. Emergence is this
phenomenon that when you scale up LLM from, say, N
parameters to 10N parameters in tandem with data
set scaling, as you saw in the previous talk. It is accompanied with
significant performance gains on many benchmark
tasks, few-shot or zero-shot. And a related view is
that emergence is actually ability for in-context learning
that the model doesn't even need any explicit
training to solve tasks. You just present the task
to it in natural language and it does it. So that's an
alternative definition that some people have. But obviously, if you're looking
at the continuous phenomenon, that's the first one,
the performance gains. And scaling laws are primarily
about cross-entropy loss, and this is the chinchilla law
that, with the numbers spelled out, that was alluded
to in the previous talk. So here's the thing that I was
going around, also DeepMind, and asking people. What do you find
weird about emergence? And one very good researcher
who had a statistical physics background, and so
on, so he really understands technical
things, said I don't understand why
reducing the low-order term. So like this, as
was pointed out, 1.61 is something fixed
related to the language. And then the terms
that are going down are these other ones. And when you increase
N by a factor of 10, N and D by a factor of
10, you change this terms, the additional terms, by
some small amount, like 0.06 goes to 0.03. Why the heck should
that change to-- why did that lead to such big
differences in macro behavior? So we'll get to that. But here are the
plots, which you also saw in the morning session. So for different tasks, you see
that, for a while, as you scale up, nothing much is going on,
and then suddenly, performance starts improving. And actually, in
this were out paper, they also note that
there's an elbow in the performance
for cross-entropy loss at around that time. We'll try to address
that with theory. But I want to also point out
this one phenomenon or this one skill, meta skill,
which is the ability to combine skills on demand. So this is related to
in-context learning, but also the essential function of a
chatbot and interactive agent. So for instance, this
was an actual query, I would give to the
language model and say, like this, I want a
single example of two or three lines of
text that illustrates these different things, which
are captured by NLP tasks often and anaphora resolution, logical
inference, simple physics, and judging sentiment. Can you give me one please? So GPT4 says, the
catch is the mouse. It was very small. The mouse ran into
a hole in the wall and the cat tried to
follow, but got stuck. It looked very funny and
I couldn't stop laughing. So it's addressing
all the constraints that I'd set out on demand. And now, we've seen this before
in the talks and the sparks paper and whatever. - Where were they? SANJEEV ARORA: It was
in the training set. So, you can put
more constraints. So you can say, how about
giving an example in context of hip hop or something
even more esoteric, medieval chivalry or something? Plenty of physics
needed for chivalry. All right, Big Bart, I'm
switching to other models. Big Bart says the
rapper dropped the mic and walked off the stage. He left, so he's, of
course, anaphora resolution. He left the crowd
roaring in approval. Claude Instant, another
good model out there. It something about Lil Wayne
and the bass is reverberating, so that's simple physics
knowledge about the world, and the size exceeded
Dropbox limits. You can continue on with this. You can put more constraints
and see what it does. And the point is-- actually, I'll get
to that in a second. I'll get to that example again. First, let me just say now. So with everything
I've said so far, I want to, of
course, now say it. What are the stumbling blocks
for theory before we go on? So very little mechanistic
understanding of deep nets. Deep learning is still
not understood very well, theoretically. No clean definition
of the problem. What is skill? What is combination? What are tasks? And there's plenty of
work in linguistic theory for the last many
decades, and you end up with some formalizations
like context-free grammars, probabilistic context-
free grammars, Gricean logic-based theories,
et cetera, and probably many more that I don't know. And they end up being
formal and rigid, like you can't say, given
natural text, exactly what's going on, and is it
being generated according to some of these things? Then, suppose you even
solve that problem, you had a formalization
of what is language, and what are skills,
how do you argue that a large subset
of them emerge roughly in tandem as the
cross-entropy loss increases? And of course, we don't have
any mechanistic understanding of deep nets. Third, or fourth, why do
combinations of skills also emerge, that
as it gets better, actually it's able to
combine skills flexibly? And so, this was
my point, that I was about to make on
the previous slide, that suppose the number of skills
is in the thousands even. That's a very
comfortable estimate, if your linguistic
skill is logical, world knowledge, all
those kinds of things. AUDIENCE: Wait, dude. Can you define a skill? SANJEEV ARORA: I won't. (LAUGHS) And I won't need to. Yeah. So just use your
intuitive definition. And so, grant me that there
are 1,000 skills at least. 1,000 to the power of 5
is 10 to the power of 15. And remember, I'm also
going to ask it to generate, in some very restricted domain,
medieval chivalry, which is only a tiny fraction
of the training set. So for sure, with
high probability, if I take a random a
menu of 1,000 skills and randomly pick 5 and a
topic, an esoteric topic, this is almost certainly
not in the training set, nothing remotely like it. So this is a simple way. Like all these doubters, ah,
it's in the training set. It's not in the training set. MODERATOR: Wait,
wait, wait, wait. You're assuming. You're assuming that this
is all sampled randomly. Whereas, you're a human
writing these things and the people that
have been training said they're also humans. SANJEEV ARORA: No, no, no. AUDIENCE: You're on
the manifold of what humans are interested in. SANJEEV ARORA: No, no, no. Take 1,000 skills. You give me the 1,000 skills. It could involve coding,
whatever, anything, math, and I'll take a random subset of
five of them and ask the model. There are 10 to the
15 combinations. Like in order for this to not
be vastly more than the number training set, you would have
that every paragraph is about, I don't know, 10 to the 1,000,
10 to the 5 combinations. That doesn't make any sense. AUDIENCE: I don't know. You're only defining. Some of these skills are
very easy to late augment, for example, others are not. SANJEEV ARORA: Yeah. I'm not saying this means
that the AGI is here. That's not what I'm saying. Yeah. I'm just getting around that
usual objection somebody pipes up. It's in the training set. AUDIENCE: No, no, no. Look, I think this
is-- you cannot just-- SANJEEV ARORA: I don't want to
talk too much more about this. This is not important. AUDIENCE: So you're saying, we
can't have 10 to 15 examples and so on. An example doesn't have
to include only the subset and exclude the rest,
and so on, right? That's fine. Man, too much of
the example maybe. SANJEEV ARORA: But
anyway you do the math, you won't have 10 to the 15. I mean even if
there's collisions. Yeah. AUDIENCE: I think
the point he made was no one's actually tried
to sample five random skills and test which one-- SANJEEV ARORA: I did. AUDIENCE: How did you do it? [LAUGHTER] But you don't have a definition
of what the skill is. SANJEEV ARORA: It
doesn't matter. AUDIENCE: It's just
randomly sampled. SANJEEV ARORA: How
does it matter? AUDIENCE: Well, we don't know
what you're sampling from. SANJEEV ARORA: It
doesn't matter. Like if you have 1,000
skills and you and I agree that these are a
subset of natural skills, if I take a subset of
five, it still the same. AUDIENCE: But I don't know
what the skill is even. SANJEEV ARORA: It
doesn't matter. For the argument,
it doesn't matter, if we agree that it's a skill. Yeah. AUDIENCE: Sanjeev, I think
the question may be more like, did you actually enumerate
the 1,000 skills, and were you able to
feel the satisfaction? SANJEEV ARORA: Yeah. AUDIENCE: OK. That's all. AUDIENCE: There's an
argument, for example, in the images you had,
like two to the N pixels, you have N pixels and/or
two of them possibilities and you don't [INAUDIBLE]. So you see this being
different than-- SANJEEV ARORA: I
think it's different. AUDIENCE: Can you repeat the
exponential complexity of-- SANJEEV ARORA: So
the question was, is this the same as images
with N pixels to that 2 to the N possibilities. That's not why. Yeah, I think so. Let's not point it
too much on this. My point was, I think everybody
agrees that you can give GPT4, crazy things, instructions, and
it does something reasonable, that we agree on. Let's start with that. So this also relates to this
debate about stochastic parrots and AGI, and so on. Now I don't want
to talk about AGI. I mean that's a very loaded
term, at least the stochastic parrots critique. And there was this
very nice discussion that Andrew Ng taped with Jeff
Hinton, where Jeff was saying, look, the one basic thing
we have to do in this debate is to agree on the facts. And the key fact
from his perspective was in this video is,
are these chatbots mere stochastic
parrots, or are they beginning to understand us,
understand us and our world? And it's not clear
what that means, but certainly,
this kind of thing suggests that it's able to
put itself and imagine itself in these scenarios,
which are not in the training set, a very
minimal form of not being stochastic parrot. That's all I'm saying. So with Anuridh and
John Brown-Cohen, who work with people we might know. He's now at DeepMind. We did some experiments
by ourselves. So we were the
Raiders and at least he did it did reasonably well,
and the other models start to struggle. Three skills fine. Five, six, difficult. But these were very-- I mean, I wouldn't say
they were very exhaustive. Are we on the same page? Let's go on. All right, so clarifying
the emergence question, but also a theory overview. Here's a motivating example,
exactly, what are skills? So the old example that
everybody in AI knows, Winograd Schema from 1971. The city councilmen
refused the demonstrators a permit because
they feared violence. Everybody knows what
the sentence means. But if you start thinking about
it, they is actually ambiguous. It could refer to either
the city councilmen or the demonstrators. But somehow, we knew
immediately that it referred to the councilmen. So, well, does the
model understand it? So you can add what's
called a closed prompt, this multiple choice. Who feared violence,
councilmen or demonstrators? And until five years ago, models
would be 50/50 between the two. They had no clue. So they hadn't figured
out anaphora resolution. That's called anaphora. What is it referring to? Question for you,
what skill or skills are used in this example? You're really the one
talking about skills. SANJEEV ARORA: I know. No, no. [LAUGHTER] I mean, I'm just trying
to motivate discussion. Like, what are skills? AUDIENCE: I think it's just a
whole bunch of co-occurrences. SANJEEV ARORA: Skill is a
whole bunch of co-occurrences? AUDIENCE: No, no. The reason that
this works is there is a lot of co-occurrences that
all point to the right answer. There might not be
any explicit skills. Skill can mean it's a mental
construct that doesn't actually exist in the data. SANJEEV ARORA: OK, so skill-- so for example, we agree
that parsing or understanding what is refused, what
is a permit, what does it mean to give a permit
to demonstrators, what does that even-- AUDIENCE: I don't think
you need any of that. No, it's just--
it's correlation. You might not even need words. You might need-- AUDIENCE: What is
not a correlation? Everything is a correlation. AUDIENCE: Yeah. AUDIENCE: So then, we-- but we have to use
some skill definition, so that's his point. [INTERPOSING VOICES] SANJEEV ARORA: You think you are
doing this by correlation, not by-- AUDIENCE: Yeah. SANJEEV ARORA: OK, well-- [LAUGHTER] So you yourself think you
have-- you're not idiot. OK, good. All right. But OK, everybody, I
hope, gets the point, that there's logic being
used, there's world knowledge, there's causality. There's the cause. AUDIENCE: Wait, wait. Logic? But humans are
really bad at logic. SANJEEV ARORA: No, no, no,
not complete and sound. Because what is because? It's causality. You don't agree. All right. So by your definition, then,
you are not using any skills? AUDIENCE: No. SANJEEV ARORA: OK, good. All right, so
that's useful to me. So I didn't know that point
of view, yes, the view that-- AUDIENCE: So can you go back? SANJEEV ARORA: Yes. AUDIENCE: So what
skill are they using? SANJEEV ARORA: Good, good, good. All right, I forgot
to say the punchline. Thank you so much, Kathy. [LAUGHTER] So what skill was used
to answer this question? And the answer is all of them. You have to understand what's
the cause and causality. You have to understand
what a demonstrator is and why they need a permit. You have to understand
when violence happens at a demonstration. Is it caused by the
people who gave the permit or who got the permit? All of that, world
knowledge, everything. Yes, Di. AUDIENCE: In this case,
if we think in this way, we have a lot of
sentences that I face. If we ask a human to annotate
just like what you did, like an expert
going through that and listing all possible skills
content, also [INAUDIBLE],, does that mean that in
the pre-training mode, we already seen all
those type of skills? SANJEEV ARORA: I'll get to that. Yeah. But I think the number of
skills, I think, is small, maybe it's in the
tens of thousands. I don't know. We can discuss. Hundreds of thousands? But I think all skills
are there, but maybe not with enough-- AUDIENCE: Question is answered. SANJEEV ARORA: Good. AUDIENCE: Sorry. SANJEEV ARORA: Yes. AUDIENCE: I remember, and
it's been maybe 30-some years, but I think fifth
grade or something, we just go through and
say there's a noun, there's an adjective. SANJEEV ARORA: Yeah, yeah, yeah. That's parsing. AUDIENCE: If we parse
this, wouldn't there be a token that basically
gave the right answer? SANJEEV ARORA: No, because
they could refer to either city councilmen or demonstrators. It doesn't make sense for them. AUDIENCE: I think the
corpus of human language, wouldn't it always
refer to the first thing that you, or maybe not. AUDIENCE: No, no, no. AUDIENCE: So the [INAUDIBLE]. AUDIENCE: Because
they're skilled here, so there's only one agent. SANJEEV ARORA: What agent? AUDIENCE: The councilmen
are the agents. SANJEEV ARORA: No. There are all kinds of-- how it's solved is not
going to be a concern here. Yeah. AUDIENCE: Change it
to, they like violence. [INTERPOSING VOICES] SANJEEV ARORA: Excuse me? AUDIENCE: What is the
connection of all these fields with your final goal? SANJEEV ARORA: Yeah,
so let's go to that. Unless if there's a
really burning question. Yes? AUDIENCE: Just a quick question. Could you have asked GPT4
to tell him what skills it used to come up with-- SANJEEV ARORA: It
doesn't have that level of self-introspection. No. Yes? AUDIENCE: If you say that like
it use all the skills to solve this example, then what's
the point of [INAUDIBLE] it needs all of them? SANJEEV ARORA: There's
no point of this example, except to illustrate that
the skills are mixed up and it's not easy to separate
out what skill was used. That's the only point
of this example. All right, so now,
let's see what does that have to do with
reduction in cross-entropy. Just to get an insight,
what's the connection? So remember this question
from my colleague, why does this low order term? Given how large N and D are, why
does that small change matter? The insight is this. There's a base entropy. This point was made earlier
in the earlier talks. And then, there's an
excess, and this excess is the one that's going
down from scaling. When you scale N and D by about
10, that excess goes down. That's a factor too. That's exactly what my
colleague was saying. But a 2x reduction actually
is very significant. Let's see with that example. So here's an example. A model that was 50/50. The model is five years ago. A model that was 50/50
between the two examples had log to excess cross-entropy. So if you just present
that to a good model, it would say answer was A, and
so it gets excess cross entropy log of 1. It's deterministic,
which is zero. And the model that's confused
gets excess cross entropy log of 2, a large constant. AUDIENCE: So this is your
working backwards from the fact that it was 50/50. SANJEEV ARORA: Yes. So the perfect model or
human gets logged one. Let's call it human. It gets logged
one, which is zero, and the bad model
gets logged two. So that is the
definition of excess. So this corresponds
to the base entropy. There's no entropy
there, because the human is perfectly confident. And the model is getting
log two, so all of that is excess to one. Yes? AUDIENCE: You know what fraction
of humans get this correct? SANJEEV ARORA: Let's
assume it's one. For sake of argument, let's
say it is actually perfect. AUDIENCE: I'm sorry, Sanjeev. But It was a calculation
you were doing here, which I'm not sure I follow. SANJEEV ARORA: OK. AUDIENCE: I mean, I saw
the working backwards to get log 1 and log 2, but
how it came from the log, from the-- SANJEEV ARORA: It
then come from here. It then come from here. I'm just saying this
is all over here. This log 2 would contribute to
this, not to the base entropy. AUDIENCE: And I'm
not following why. SANJEEV ARORA: Why? So the human has no entropy
in response to this question. Human is-- AUDIENCE: No, I understand. Yes. SANJEEV ARORA: So that's log 1. OK, so that means-- my next slide says this. AUDIENCE: I think I can
actually answer the question. It's because if you think
about the answer there, the token that says A or D
is where this entropy is, but there is a lot of other
stuff in the initial sentence and the answer that doesn't
fall in the category of the correct answer or not. So that's where all the basis-- SANJEEV ARORA: So
maybe let me clarify. The entropy is just
the choice A versus B. That's what you're saying. In that, you see, right? The human would say A. AUDIENCE: No, no. I saw that. SANJEEV ARORA: It's a 1-bit on. That's what I'm saying. I'll integrate it
into the theory. Any other questions? Right. So intuitively, as you reduce
the excess cross-entropy, and you can easily
imagine that in real text. There was something else
that was following here, where if you had
this 50/50 confusion, you would be predicting wrong. This is just distilling out
that confusion in your question. And that's called
a close question. So wait, what happened? So the point here is, what
is excess cross-entropy? So now, you might
say, OK, if we have to reduce excess cross
entropy, let's just reduce that directly. Why are we working with
entropy, cross-entropy? The answer is excess
cross-entropy, by definition, requires a human in the picture. So you wouldn't really
need to ask humans to rate trillions of tokens to
get actual access cross-entropy and then start minimizing it. So that's not how it can happen. AUDIENCE: Just in case it's
useful and not a diversion, could you define
excess cross-entropy? SANJEEV ARORA:
Exactly what it is. So, like you had a
certain place in text, you ask humans what the
next word is and you get some probability
distribution on the next word, so that's the ground
truth distribution. You ask the model, it
deviates, and that difference is the excess cross-entropy. All right, so it shows
that excess cross-entropy forces the model to reduce
its confusion compared to the human. So now, going to
our work, so, one, a mathematical understanding
is possible for LMS right now? As we heard from
Elias' talk, maybe we have three or four years or
five years left to figure this out a little bit. All right, so what
we'll do is it'll be a statistical framework,
so it will not too unfamiliar. But we will be able to prove
generalization to unseen combinations of
skills, combinations that were not seen in the data. Skidding noise is
assumed as an axiom, and it sidesteps the need
for mechanistic analysis of gradient updates. Key assumption, and this is
illustrated by the example, the Winograd example. There were many skills
needed for that example, even to predict that, the
answer to the question. So pieces of text
involve combinations of random k-tuple of skills. I'll define a random k-tuple
and skills in a second, but that's a key assumption. And then the technical
driver of the result will be random
graph theory, which may be some fraction
of the audience would be reasonably familiar with. And that random
graph theory will explain why reduction
in cross-entropy leads to roughly
synchronized improvements on individual skills. So it doesn't say
completely synchronized, but you can see from
the plots that it's kind of synchronized, as
well as on tuples of skills. So it highlights role
of skill-learning and hints at new ways to learn. This whole thinking about skills
is sort of implicit there. You saw, for instance,
in Sasha's talk, they threw in some code data
and it improves performance on some tasks that
are in the evaluation. So it's not clear exactly
what even text is anymore. Code is mixed in and who knows
what else, synthetic data. So our framework is kind
of agnostic about that. It doesn't use anything
about natural language per se, but about
this combinations of skills, which could happen
with images and all kinds of other modalities. So without further
ado, the setup. Usual language, usual modeling,
is not conducive to theory because there's nothing
there to work with, except the cross-entropy loss. [LAUGHTER] So what are our tasks? How are they connected
to one another? We're trying to make
theory about tasks. What are tasks? How are they connected
to one another? Where does linguistics and
knowledge and common sense come in, et cetera, et cetera? Although, yeah, Alyosha
thinks there are no skills, so I'm putting some
doubt about that, but OK. AUDIENCE: Well, you
didn't tell us this good. SANJEEV ARORA: Yes. It's a node in a graph,
you'll be happy to know. Instead of thinking of the
corpus as a very long sequence, I'm going to chop it up
into bits of a certain size, and I call that a test stream. So we're looking at-- this
is a picture for test data. Training data doesn't
make any appearance, because by scaling law, we know
already what the performance is on the test data. So we're reasoning
about test data. So there are pieces of
text of a certain size and piece of text t has a
probability or measure mu 2 of t. Mu 2 is a measure. Oh, sorry, it should
be mutual there too. So that's mutual. And then there
are latent skills. And the answer to the burning
question, what are skills? It's a node in a graph. Just think of it as conceptually
some discrete set, which is, of course, maybe skill
is a continuous notion, but it's a discrete
set and they could involve linguistics, logic,
world knowledge, common sense. You can open up books
on any of these topics and you'll see types
of logical reasoning. You've all seen, like
deductive reasoning, modus ponens, et
cetera, so there's a long list you can get. But there are
probably many others that we don't know of,
because these skills we're talking about, I
want to emphasize these are skills from the
transformers viewpoint. So what I wrote here was
what we humans came up with in trying to formalize it. But really, what
we're looking at is skills from the
classic viewpoint, which, who knows what it is. So this is just
for illustration. And each skill also has
a certain probability, mu one of S. AUDIENCE: Probability
according to what? SANJEEV ARORA: Natural. In this distribution
of text, so we'll see. AUDIENCE: You mean
the probability. SANJEEV ARORA: Let's see. Let's see. Let me just continue
and then you'll see. AUDIENCE: This skill is
relevant for this lesson? SANJEEV ARORA: Let'd see. Let's see. Yeah. I'm going to. So the mixing
assumption is, nature picks a k-tuple of skills,
iid, from measure mu 1. So from this set of nodes,
pick a k-tuple, iid. And then, uses an
unknown process to convert it into a text piece. So that's going to be a black
box in theory, unknown process. It has to be. Who knows what that process is. And uses some other
unknown process to assign it a probability. That's also latent. Yes. AUDIENCE: Isn't the
probability of the text implicit, based on
distribution of [INAUDIBLE]?? SANJEEV ARORA: Well, but
it's an unknown process. Yes. It's determined by that, but
we don't know that process. So it's very different from
usual simple statistical models, where you
would say, ah, you'd it and then you combine
it in a linear way. It's not like that. AUDIENCE: You're
saying the process itself is probabilistic. SANJEEV ARORA: Yeah. So the probabilistic
is this thing, that it's taking the iid. AUDIENCE: But the unknown
process is also probabilistic. SANJEEV ARORA:
Yeah, but that part you can consider it--
yeah, it's probabilistic. Yeah, and you pick one sample. Yes. AUDIENCE: Sounds very
much like a topic model. SANJEEV ARORA: It
does sound like that, except topic model, as I said,
is exactly what I said, linear, and this is not. This is some arbitrary thing. Yes, indeed. How these skills figure in that? We also don't know. Yes. AUDIENCE: This includes the
process that takes the skills, ignores them, draws a random
thing from probability mu 2, and generates a [INAUDIBLE]? SANJEEV ARORA: Correct. Yeah. Yes, ma'am. AUDIENCE: You said this graph is
like the graph for transformer skills, but then-- SANJEEV ARORA:
That's the graphic. Yeah. AUDIENCE: Does nature pick from
the same graph or different? SANJEEV ARORA: So, nature
picks up a lot of skills, and then, this process,
unknown processes, kick in. So unknown processes
are part of nature, but they're unknown to us. AUDIENCE: The collection
of skills, s, is the same? It's like nature has a-- SANJEEV ARORA: Nature knows
a collection of skills. Yes. AUDIENCE: And the transformer
has some collection of skills. SANJEEV ARORA: Oh, sorry. Nature is assumed to know
what transformer skills are. Yeah. Yes? AUDIENCE: Oh, wait. Apart from what skills are,
is there a fixed dictionary of skills that is always the
same or is it possible to-- SANJEEV ARORA: Yeah,
this theory is assuming. So the dynamic theory
is very interesting, but, yeah, this is static. Any other questions? AUDIENCE: Could
you just say given the k-tuple of skills,
nature samples a text from the distribution mu 2? Is that what this is saying? SANJEEV ARORA: No. AUDIENCE: Why? SANJEEV ARORA:
Because you have to-- AUDIENCE: And then assign
it while you build it. SANJEEV ARORA: Yeah. AUDIENCE: Is there a sense
in which each of the skills can be thought of as something
like an attention head and a transformer, like
parsing, is that association between different elements? SANJEEV ARORA: It's agnostic. Yeah. There's no-- I'm not
giving it any meaning, purposefully, because I think
it's fairly general, more than just parsing. You could also apply it to
multiple modalities, images, or whatever, and
they could be mixed. Yeah. AUDIENCE: Just to
help [INAUDIBLE].. AUDIENCE: Yes. It's multiple, what, the
performance or the questions, or what are you trying
to model with nature? SANJEEV ARORA: I'm not-- I mean I'm agnostic. You'll not even get text corpus. Is that what you mean? You know, there are
some things where you're doing a prediction. cross-entropy is a
prediction, loss. And we don't know where
these pieces came from. This was-- AUDIENCE: Is the process
deterministic or randomized? SANJEEV ARORA: It can
be randomized, this one. Yeah. AUDIENCE: Isn't, in a
way, the battle half won because we are
assuming that nature works on top of these tasks
that are coming from artificial? SANJEEV ARORA: We're
not assuming anything. I haven't assumed anything. I didn't say what these
skills are, whatever. This is how the-- AUDIENCE: How do nature
pick k-tuple skills from those latent skills? SANJEEV ARORA: Yes. AUDIENCE: Does that
transform over? SANJEEV ARORA: No, no, no, no. I said, you can even
think of it that way. It may not. It could involve these
linguistics logic. It doesn't matter. But I'm just saying
that, just cautioning that, the set of skills may
be much larger than this one. Any other question? Let me move on, clarify
the scenario enough. Unless if it's-- let's see
if everything gets answered. So here's the thing. So now, how do we test whether
the skill is being correctly learned or applied? So the closed
sufficiency assumption is that nature knows
the set of skills and it's going to insert a
question, multiple choice question, which will test your
understanding of that skill, similar to the Winograd example. And so, in this piece of
text, after it generates it, it inserts some questions. Now, those questions are
surprising to the model. It may not anticipate where
nature would put that question. So it's not required to
predict the questions in the cross-entropy
loss calculation. So the question just
appear and model doesn't have to break them. Then it sees the answers
in the multiple choice, and it can read the answers. And the assumption is that
it's in simple English, so it understands
that, and it just has to predict which
answer is correct. That's the assumption. And so, the close
sufficiency assumption is that nature's questions
via closed prompts are squeezing out the model's
deficiencies in understanding. So the deficiency
of understanding is the excess cross-entropy,
what humans would not have. And this, the average
cross-entropy loss was just squeezed out as excess
cross-entropy on the questions that nature has inserted. AUDIENCE: We can think
of the prompts as a hint? Somehow, you're saying
the hint is very logical. SANJEEV ARORA: It's not a hint. It's a question. But it's saying that-- yeah, so if you think
of it as a hint, you may actually object
to this assumption because you would say,
that was a big hint. But think about the
Winograd example. Who was afraid of violence? Now, maybe you could say,
just asking that question makes you think harder. Maybe, and then so you're
sort of interfering with the process a little bit. AUDIENCE: At least
not an obvious answer. SANJEEV ARORA: It's not. Yeah, it's an assumption, that
basically the models a priori, lack of understanding
or confusion, is squeezed out by
these questions. Yes. AUDIENCE: So the
close prompts are assumed to be
collections of questions that have definite answers. SANJEEV ARORA: Yes. That the human can answer. Yes. You can easily imagine
that the human has a certain probability
of answering, and then it's a cross-entropy. Yes. AUDIENCE: So, as it
emerged here, interaction. Yeah, so having interaction
is a way to generate data might be a
thing of the future. SANJEEV ARORA: Yes. AUDIENCE: How would
that go into your would? SANJEEV ARORA: You have
to extend the theory. This is a more static theory. AUDIENCE: can this assumption
be empirically tested on a bunch of closed
prompt benchmark tasks? SANJEEV ARORA: That's
a good question, but yeah, I haven't tried. AUDIENCE: And it hasn't already
been tested, the [INAUDIBLE]?? AUDIENCE: I would argue that
it has been, in the sense that researchers have
uncovered that part. SANJEEV ARORA: Yeah. That would be my answer. AUDIENCE: Like arguably--
again, depending on how you want
to define a task, you could say that a
given natural language inference is a task and NLI
data sets test that task. SANJEEV ARORA: Exactly. Yeah. Thank you. So Colin is probably one
of the experts in designing these evaluations and so on. So, yeah, at least I'm glad
to hear that he agrees. All right, so now, I'm
measuring proficiency in skills and tuples of skills. So we have this
bipartite graph, where every skill, k-tuples of
skills produce pieces of text. So therefore, each
individual skill is involved in some
pieces of text. all right, so that. So each skill s is involved in
a subset of the text pieces, so that's a sub
distribution on text pieces. So if I look at it
from the point of view of any individual skills, look
at all the pieces that got used in, and that's
the subdistribution. So the observation is that
if the close prediction was successful on t, then all k
skills were correctly applied. I'm not sure if I should
call it observation or that's an
assumption, that if-- because we're saying that
if close prediction was successful, then you
applied the skills. That's the definition Now, what happens when the
close prediction was incorrect? So now, you failed to predict
that it was the councilmen that were afraid of violence. Which of the skills
did you fail at? Did you not understand
what's because? You didn't understand what are
demonstrations, any of those. So all these skills
that we used, we'll assume that they were
all incorrectly applied. Yes. AUDIENCE: You're saying, what
if you apply two skills kind of incorrectly there? SANJEEV ARORA: So
that's what I'm answering, that if the answer
is incorrect to the close question, I'm assuming that
all skills were incorrectly applied. So that's a kind of conservative
bound, as you'll see. AUDIENCE: Sorry. I'm asking is if
you successfully do close prediction, you
could have incorrectly applied two skills that
cancel out of it? SANJEEV ARORA: Right. This is ignoring that. It's assuming that,
at least on average, if you're doing
it correctly, then you're applying the
skills correctly. Yeah. AUDIENCE: Sorry. None of the other N minus
k skills are applied. Isn't that also assumed? SANJEEV ARORA: No, correctly
applied here on this t. That's what I'm saying. AUDIENCE: But the T might. They might have applied
other skills as well. More two k skills on the whole. SANJEEV ARORA: No. So that's what we're assuming,
that the nature has basically given you a catalog of
k skills that would. Maybe where you're going with
is different pieces of text may have different number
k, and that just makes, as you can imagine, the
random graph theory hopeless, so I'm keeping it at one k. So now, the failure rate
of the model on scale s is just the loss
on close prediction from this distribution. So that's the answer to
your question, Alyosha, that this is-- you are basically
reducing it to some kind of statistical measure. That's skill. And similar definition
for task corresponding k prime tuples of skills for
k prime less than k pairs of skills. You take pairs of skills
and see which pieces of text they are part of and
then go from there. AUDIENCE: How do you
combine those skills, then? SANJEEV ARORA: The analysis
doesn't need to know. Yeah. So it's not composition. It's combination-- that those
two skills appear in there. And if the model doesn't solve
the Cloze question correctly, then you assume that basically
all the skills that we use were incorrectly applied. So it's a very
conservative estimate. So you're lower bounding
how good your proficiency is on the scales. You may be more proficient
despite making errors, but if you're making
errors it's assuming you're just not proficient. OK? So yeah. Again, same thing. So just to illustrate,
suppose nature produced this piece of text
using a 5-tuple of skills. Then this piece of text
appears on the distribution for 5 statistical tasks
for each of those 5 skills. 5 choose 2 statistical
tasks corresponding to pairs of skills, 5 choose 3
statistical tasks corresponding to triples of skills, et cetera. OK. So there's a whole
universe of tasks-- SQ-scale tasks,
statistical tasks. OK? And then yeah, as Colin
said, all these tasks that maybe are using NLP are
using some subset of skills. OK. So why emergence? And so I forgot to say
on the previous slide that that picture of
the skills and so on is called a skill-cluster. And you could imagine corpus
having multiple skill-clusters, and I'll address that
later, but for now, we discuss a corpus where
there's a single set of skills and then this-- the process I described
was one cluster. So how does the
model get better? Well, remember, we are
assuming that the cross-entropy on the answers to
the Cloze questions is tracking the
overall cross-entropy. So as the scaling laws kick
in and the model gets better at cross-entropy,
you will also see that it's answering the Cloze
questions much better, which are testing the
model's understanding. So these are the errors on
Cloze questions-- the big X. And as you scale up the
models, cross-entropy improves. And the assumption is that
these errors will also go down. So for instance, like this, OK? Something like that. OK? So as you scale up,
the Cloze questions are getting answered
more and more. All right. And so how does this
affect error rate on tasks related to
skills and skill-tuples? So let me first tell
you the incorrect way to think about it, which is
that this a random graph, so all skills are equally affected. You take any set of places
where the errors are happening, and basically, every skill will
have about the right proportion of edges there, and so on. That's not exactly true. OK? So the reason it's not true
is that these errors depend on the training process,
and details of the training set, and so on. And then there are
these unknown processes that link these
different text pieces. So after training, you cannot
assume that these edges, with respect to where the
errors are, are random. You cannot. AUDIENCE: You mean
uniformly random? SANJEEV ARORA:
Uniformly random, yeah. Even if they were put in
randomly conditional on knowing where the errors are, you cannot
think of them as a random set of edges. AUDIENCE: So using
statistical arguments-- SANJEEV ARORA: We will, yeah. Yeah. AUDIENCE: --that
nonuniform distribution-- SANJEEV ARORA: Correct. It's just not uniform. Yeah. That's the random graph theory. OK. Any questions? All this build-up. Now, I'm ready to
give you the theorems. OK. So why emergence? Oh, I managed to go back. I see. I got confused. I was holding-- it
wasn't making sense. Sorry. OK, good. Oh, this also is there. OK, good. Yeah. So what we want is the
emergence curve, right? The Cisco process-- that
proficiency in task increases. It's a continuous thing. And some of the
previous talks mentioned that the illusion that
it suddenly emerges is just that-- an illusion. And if you do it
properly, you should see just continuous improvement
as you improve the model or scale up the model. And the curve I'm talking about
is set of xy pairs in a plane, in a graph such that when the
excess cross-entropy is theta, then for at least x
percent of skills, the model has classification
loss at most gamma on the Cisco task
associated with that skill. OK? So I've reduced proficiency on
skills to this prediction task. And that's the set of xy pairs. For what fraction of skills-- the classification
loss is less than 1. AUDIENCE: Sorry. What is the dependence on k? SANJEEV ARORA: So k is going
to be in there because you'll see the expression. It depends on k. OK. So this is the first theorem. This is for uniform
distribution-- uniform mu 1 and mu 2, the two measures. And that's really classical
random graph theory. And by the way, this
figure was done by GPT 4 in the last-minute rush. So that's the conceptual
picture, right? These are skills. These are the text pieces. And this graph connecting
them is random, even though there are these
arbitrary measures, and so on. But the measures
are now uniform. So the curve that I
was talking about is-- sorry. This doesn't seem right. I think there's a typo here. Anyway, the curve
that I mentioned is given by this expression. So it depends on theta and k. And then alpha, beta are
like the x and y, roughly, but in this
expression, it's like if x's cross-entropy
loss is theta, then there are 1
minus alpha times N2 skills such that model has
error less than beta times theta on the task associated. So parameterization
involves theta here. OK? And entropy is
the usual entropy. So just to give you an idea,
so suppose theta is 0.1-- the excess
cross-entropy is 0.1-- and alpha is 0.2, beta is 3. Then for at least 0.8
fraction of skills, the model answers incorrectly
in 0.3 fraction of skills. And as I emphasized
earlier, this is the minimum
guaranteed performance because I have a very
conservative estimate of performance. So in practice, the random
graph theory may be pessimistic and [INAUDIBLE] there. OK. So yeah. And you can see that the random
graph theory is coming in because of this for all Y. We
don't know what this set is, where the errors are,
so we have to reason about all Y. So anyway,
so this is just done by a probabilistic method
to show that it holds with high probability for all
Y. The fraction of Y that's violated is 10, or
the expected fraction. AUDIENCE: Sanjeev,
the thing that's been puzzling me the whole
talk is random graph theory can tell us about sudden
emergence of connectedness, or subgraphs, or
things like that, but here it seems like we only
care about k-tuples, right? SANJEEV ARORA: So right now,
I'm only talking about nodes. I'm just talking about skills. AUDIENCE: Yeah, yeah. SANJEEV ARORA: I haven't
talked about the tuples in this theorem. I'm coming to that. So this is just individual
skills, which are nodes. AUDIENCE: Right. OK. SANJEEV ARORA: I'm going
to talk about tuples. AUDIENCE: Right, but then
we want to make a statement about-- it happened suddenly
that all these skills-- SANJEEV ARORA: So for that,
I have give you the picture. AUDIENCE: OK. SANJEEV ARORA: So
this is what it is. You can't wrestle
with this anymore. This is what it is. And so we have to
plot it to see. Yeah. All right. Oh. And then the weighted version
with arbitrary measures is just a more
complicated version of the probabilistic method,
which required some work. So these are the curves, OK? So that's your answer. That's answer to your question. So you're not going to get
some single expression that describes the emergence. It's this collection of curves. And you can reason about the
curves based on the math. Am I out of time? Almost. Yeah. So this is for
different k and theta. Theta is 0.1-- so
different k for same theta. And there's a simple
corollary of the theorem that increasing k shifts the
curve below and to the right. That is, there's
better emergence. Yes. AUDIENCE: I thought
Paul's question was, there's some global phenomenon,
like connectivity, et cetera, that emergence should apply to. So what's that global quantity? SANJEEV ARORA: OK. So this is just
competence and skills. AUDIENCE: Yeah. SANJEEV ARORA: So emergence-- I'm talking about that your
performance on a bunch of tasks improves. And for me, tasks are either
the statistical tasks, or the individual skills,
or the tuples of skills. And I'm showing the
result for those. AUDIENCE: OK. Because if there's
no global phenomenon, then it seems like
we should just be able to write an expression
like whatever to the k power, right? And it should be smooth, right? The probability that I
have all these skills is equal to the probability
that I have one skill raised to the power of-- SANJEEV ARORA: I
didn't understand that. It probably is a
misunderstanding. Let me just finish. OK. So the simple corollary
is that k increasing k shifts the weights, shifts the
curve below and to the right. And that's better
emergence, right? So you have more skills that
have that similar or better performance. And what is higher k? I think of that as a
substitute for data quality or a proxy for data quality,
that more complicated data, right? These days apparently,
the human readers have to have college degrees. So they can write skills. They can use more skills
than a piece of text. So high k is better
quality text. Then emergence scores of
k prime tuples of skills-- I'll just say in one line,
it's a tensorization argument for the theorems. Tensorization argument-- and
that's what gives you that, but the bottom line is that
basically, if the size of theta reduces by a factor of 2,
then whatever performance you were getting
for single skills, you get for pairs of skills. You reduce it by
another factor of 2, then whatever performance
you're getting for pairs is for the whole quadruple now. So as the models are
scaled up, you'll see a better improvement
on tuples of skills. Yes. AUDIENCE: Did you get all pairs? SANJEEV ARORA: No, no, no. Similar. It's this curve-- on
some fraction of tuples, you'll see a
certain performance, but the overall curve of that
is exactly just shifting up. AUDIENCE: So does the
fraction of pairs of skills that you get correct rise
continuously as theta falls? SANJEEV ARORA: That
is not the claim because this tensorization
argument is not exact. Again, all of these
are all underestimating rather than overestimating. And so it's all consistent, but
I don't know how to prove that. Yes? AUDIENCE: Is it a fair summary
of what you're saying-- there's a lot hidden
in the formula, but the emergence of
these complex abilities arises as a phase transition
due to the nonuniform distribution of degrees of-- SANJEEV ARORA: It's
not nonuniform. It's uniform. AUDIENCE: You were
talking about how the distribution over the
number of edges connecting-- SANJEEV ARORA: Yeah. So maybe let me finish. And then take more
questions, if we have time. Yeah. So that's it. And again, so yeah, you see
that if you increase the scaling by 2 orders of
magnitude, then what was the curve for
single skills becomes the curve for the quadruple. AUDIENCE: What's the alpha. SANJEEV ARORA: Don't
worry about it. It's just the
fraction of skills. Yeah. AUDIENCE: Sorry. Sorry. SANJEEV ARORA: Yeah. So the point is you've
only had a constant factor increase in amount of data. And you're suddenly seeing
good performance on quadruples. So you definitely didn't
have a polynomial increase in the amount of data. That's mathematically
equivalent. That's the key takeaway. AUDIENCE: Can you repeat that? Sorry. SANJEEV ARORA: So these curves-- so what I'm saying is when you
increase the amount of data by a factor of 10,
the curve for 1 tuple shifts from here to here. And this becomes the curve
for the pairs of skills. Another order of
scaling can factor 10. And quadruple is doing
as well as single skills. So your performance
on k-couples or tuples is increasing much more than
the amount of data used. It's some kind of a-- yeah. AUDIENCE: Is another
way to think about it-- SANJEEV ARORA: If it's OK,
let's restrict questions to exactly if you don't
understand something rather than follow-up. So OK. And so yeah, this, I
won't say, but yeah. There's an explanation. These scaling laws implicitly
are suggesting phenomena like these, that
as you scale up, you need less data for the
same amount of learning. And that's what's going on-- much less than the naive
counting approach would say. This was an
illustrative experiment that you can learn to recognize
4 tuples of digits, which is 10 to the 4 combinations,
from actually 10 to the 3 labeled examples, or
labeled examples in a very-- so you can read [INAUDIBLE]. And the extensions
of theory-- they are nonuniform distributions. Already mentioned. General skill-clusters-- you
could have different skill clusters, like basic English,
math, logic, whatever, or science. And you could have
different scale clusters. And then there
isn't a clean theory because we don't know if
progress happens similarly on all those clusters,
and at what rate. And so all of that can only
be settled by experiments. OK. Yeah. Emergence in different clusters
may happen at different scales. Mathematically, that's a lot. And the last thing I
want to emphasize again, returning to this magical
ability of chatbots to combine skills on demand-- another name I call it-- procedural thinking, for
want of a better word. In all of these, we're trying
to use 20th century notions to explain 21st
century phenomena. And the procedural thinking is
roughly that described a task. And then you're
identifying some arguments and some procedure
that has to be applied using those arguments. And so that's what we
call procedural thinking. And that's somehow emerging. And of course, it's
just a couple of skills. These are all individual skills. You can count them as skills. So they will emerge
from scaling, but maybe they can emerge
better than that as well. So concluding thoughts--
elementary and plausible accounts explains
phenomena such as learning k-tuple of skills we are
seeing in our training data. Highlights importance
of procedural thinking. And some procedural thinking
appears in our theory, just presumably by statistics,
but probably, there's more going on empirically. And Cloze tasks sufficient to
test most interesting skills? And cognitive scientists seem
to think it's pretty general. It's not completely endorsed. Statistical notion of
skill seems limiting. Proficiency in skills
is on this distribution. And the whole point of a
language model or AI model is that they seem to work
outside of the training distribution. So that seems limiting. So that's just the first theory. So I'll stop there. Thank you very much. [APPLAUSE] MODERATOR: Let's
take one question. And then the others can
ask Sanjeev at lunch. AUDIENCE: So is another
way to think about this that for each of these
skills, as you train more, the skills get a
little bit better? So it's not having
a skill or not. Just, you have different-- SANJEEV ARORA: Continuous
improvement, yes, definitely. AUDIENCE: And that is it
requires 5 skills to do a task, and each skill is
better by a factor of 2, it's 32 times better
to do the task, and then it suddenly emerges
that you can do the task? SANJEEV ARORA: There's
a nonlinear like that-- not exactly that math, but-- AUDIENCE: But that's
a qualitative way. SANJEEV ARORA: Yeah. AUDIENCE: OK, thank you. SANJEEV ARORA: Yeah. MODERATOR: Could we
do a few questions? OK. Apparently, I can do more
questions, according to [INAUDIBLE]. SANJEEV ARORA: Ah, we have
the lead doubter here. AUDIENCE: Well, OK. So I was trying to reconcile
my worldview with yours. SANJEEV ARORA: Yes. AUDIENCE: And here
is one attempt. SANJEEV ARORA: OK. AUDIENCE: What if you have
a huge amount of skills? You have skills on the order
of the amount of training data. Is that going to be OK? SANJEEV ARORA: Then this
theory doesn't apply. AUDIENCE: I see. SANJEEV ARORA: This is
asymptotically assuming-- AUDIENCE: So you
assume that your skills is a very sparse set? SANJEEV ARORA: Yeah. It could be 10,000 or
8,000, but it's not tera. AUDIENCE: All right. We are still in
disagreement then. [LAUGHS] SANJEEV ARORA: OK. AUDIENCE: Just a
naive clarification. So in this example, based
off of that other question of this continuous improvement,
like epsilon to the power-- at the end of the day,
did you have a nonlinear-- were you looking at accuracy or
error as some nonlinear metric that you put at the end of-- that you're measuring? SANJEEV ARORA: Oh, I see. So I was assuming-- AUDIENCE: --continuous
probabilities and-- SANJEEV ARORA: So yeah. The calculation
I was showing was assuming that if you
are in that set of data, where by averaging,
there's error at least 1/2 or something on
the Cloze question, then you've completely failed. That was the assumption. I was not taking that
thing in account. Yeah. It's assuming it's
completely failed. Yeah. AUDIENCE: So just
wondering if you feel comfortable with the error
level that we are achieving. The initial empirical
graphs you showed-- almost at a phase
transition, almost 0. SANJEEV ARORA: Yeah,
but it's on a log scale. AUDIENCE: Yeah, it is on a log-- OK. SANJEEV ARORA: So
remember that we think that performance goes with
excess cross-entropy, right? Not cross-entropy. They are doing
cross-entropy, right? So when excess goes
down by a factor 2, it'll almost not
show on the graph, but it's massive
for classification. AUDIENCE: So I do NLP, so I
don't really understand math, but my question would be-- [LAUGHTER] --what is the most surprising
empirical phenomenon that you think your theory can predict
that we cannot predict if we haven't listened to the talk? SANJEEV ARORA: I'm asking
you to focus on this magic that you can give GPT 4 at
least, and maybe other models, too. Take any set of skills--
reasonable set-- and take 5 tuples, and
it can compose them. Now, that's not
AGI, but I'm just pointing out that that's not
enough data in the training set. Maybe that's one key takeaway. Even if the theory doesn't
impress you, that's fine, but that is very
counterintuitive. AUDIENCE: Yes. AUDIENCE: So rephrasing-- SANJEEV ARORA: Yes. AUDIENCE: You
started rephrasing-- you started with the sentence. You're saying, OK, a few
years ago, got it wrong. Now, you can get it right. So there were some-- SANJEEV ARORA: Get
it wrong, meaning-- AUDIENCE: The "they." SANJEEV ARORA: Who? AUDIENCE: Policemen, the
demonstrators, whatever. SANJEEV ARORA: Oh, the
language model-- yeah. AUDIENCE: And your theory was
saying, OK, all of a sudden, we have this
emergence of skills. Graph theory can maybe explain
to us why these emerge, but another way to ask it
about being actionable-- can you say how much data you need? Can you pull out how much data
you need out of your formula, out of your analysis to say
when some other set of skills would emerge? SANJEEV ARORA: Right. So the problem here is that-- what Alyosha is not happy
with-- what is a set of skills? So this is a qualitative
theory in that sense, right? Do you have a complete
cataloging of skills? Nobody does. AUDIENCE: No, you can't
have it both ways. SANJEEV ARORA: Yeah. AUDIENCE: [LAUGHS] SANJEEV ARORA: Right. So it's a qualitative theory. Completely. I completely agree. Yeah. AUDIENCE: So Sanjeev, I
obviously had, at most, k minus 1 of the skills needed
to understand this talk-- [LAUGHTER] --but maybe I can rephrase
my question this way. Why is it not obvious? As I improve each of
my individual skills, I will also improve
on any subset of k of the skills at some
rate that I can calculate? Why do I need graph
theory to tell me that? SANJEEV ARORA: So
the driver is not that you're
improving the skills. The driver is you are
reducing cross-entropy. AUDIENCE: OK. SANJEEV ARORA: So it's not
even trivial to argue that-- so you have now
1% of the training set, where there's errors. What is the fraction of
edges from individual nodes on the other side that
are going to that set? It's an arbitrary set. MODERATOR: OK, I think we
should break for lunch. SANJEEV ARORA: OK. All right. OK. [APPLAUSE] MODERATOR: So we'll come
back here at 2:00 PM.