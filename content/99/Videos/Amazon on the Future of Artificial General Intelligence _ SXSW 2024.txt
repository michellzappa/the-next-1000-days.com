(crowd clapping) - Hi, everyone. Glad to see a full room here. Good morning. I'm Ryan Heath. I write the AI newsletter, and I'm the Global Tech
Correspondent at Axios. And Vishal Sharma here is the head of Artificial General
Intelligence at Amazon. So a very small job that
you've got there, Vishal. Welcome.
- Thank you. - Now, over the course of the next hour, we're gonna dig into what
are the paths to this famous or infamous Artificial
General Intelligence, both within Amazon and more broadly. But I think we need to try
and get on the same page because America is not on the same page when it comes to AI at the moment. So we're gonna walk through
some of the tension points in the debate, what we can do
to deal with the trust gaps and the fear that exists
around some elements of AI. And even get into sort of advice for you if you have a small
company or you are a founder and you want to figure
out how to integrate AI into your work going forward. But Vishal, let's start with the easy one. It's the first AI track here at South by, so how is it for you? What's been the most exciting person or idea that you've come across? - Oh, I think there have been too many for me to pick one. I'm really happy to see those guys- - [Ryan] They're all winners. - What's that again?
- [Ryan] They're all winners. - Oh, yeah. Absolutely. It been like super satisfying to me. I've been involved in
AI for a long time now, going on about 20 years since
before deep learning existed. You know, I've worked on assistance and like other stuff, two of
the world's biggest assistants, and it's just so gratifying
to see this come to fruition and just the profusion of ideas that we are seeing in AI
now is pretty phenomenal. So hard to pick one. - Yeah. Well, we've all been drowning in the hype around large language
models, which are the kind of like leading edge of
generative AI over the past year. But how does that feel for you? LLMs. They're just one part of AI. There are many types of AI,
it might not be the way to get to artificial general intelligence. So when you look across
everything you're doing at Amazon, where do these LLMs fit into that puzzle? - Oh, so the one thing I'll
say about AI in general is, is there've been many, many
ideas that have been tried. It's been a little bit
evolutionary is what I would say. And there's little doubt in my mind that what's happened with
LLMs, with generative AI has been fairly monumental, right? So we have to like recognize that; other AI has produced
many interesting results, but this one is special. And, you know, there's some
like obvious markers of this. So, for example, Turing
test is an obvious one. You know, it was proposed
way back in the 1950s. Stood for what, like 72 years? - [Ryan] Yep. - Now experts will argue, but it's more or less fallen at this point in time. So that's pretty monumental. From an Amazon perspective, I'd say we think about AI at two levels and what's going on with innovation and Amazon at kind of two levels. On the highest level, what I'd say is that
there's a deep recognition that the chain of things
that we started on in terms of technological
development holds the keys to some pretty interesting
gains for humanity. Like some of the deepest
challenges, deepest opportunities that have existed for
humanity are in the process of being unlocked by them. If you do this right, it could unlock basically an
age of abundance for humanity. It's almost like the second
coming of Industrial Revolution. Now, what does that mean in terms of actual driven innovation in Amazon? There's sort of three components of it. So we often say we think
about it in three layers, and there's innovation
happening in all three of them. The base layer is hardware and the chips. Basically the chips. And here Amazon has,
you know, these training and internship systems,
we talk about those. The second layer is models, and Amazon's philosophy
on models is choice. So multiplicity, and the
highest layer is application. So that's things like, you
know, we did this Rufus assistant on shopping to help
people's shopping journeys. I'm pretty excited about that. So that's how we're driving it. - Yeah, so I know you work
across all those layers. You build chips, you have models, you help people fine tune it all. But let's dig into the
models for a second. I think a lot of people
come across sort of, they have the wrong impression that there are just like
a few models out there because we've all heard
so much about Chat GPT. But give us a sense of how
many, you know, like dozens or hundreds of models go
into what Amazon is building? - Oh, there's a lot of models. Dozens, absolutely. So if you took something
like Alexa, for example, it's a little bit of a misnomer to think there's this one
model that's driving all of it. It isn't actually. Under the covers, something
Alexa would have like about 30 machine learning models
that are entered in to it. Some of them are fairly small, right? So when you summon Alexa and
you say, you know, Alexa. And you start to like, talk to it, it's a very small model that's
dedicated to detecting simply that wake word and it's on the device. So nothing is getting streamed
and kind of gets woken up. Then there's models that
have to do with presence that are models to do with
turning voice into text and there's models around comprehension. So it's a multiplicity and pretty much any big system that you look at will often contain a multiplicity of models. But that said, you know,
there is obviously in terms of generative AI, the
models have become bigger. And so far, I'd say the
well on making them bigger is far from running dry. There's still a lot of
development happening there. Now, there's more than
one of those as well. And so if you were to
basically look at what we offer through our Bedrock program,
which is a AWS offering where you can come get the models. We have, you know, Mishra there and Claude 3 just got announced. So Claude 3's in there; there's
models we are working on. So it's a big, I'd say it's a big family of things that's emerging here. - Yeah. And how big is this trend of the models becoming specialized? Because, you know, it seems
to me at one hand, okay, you've got some models that
have trillions of data points that they have trained on now. But there are some basic
things that they still don't do well, and they
obviously don't have critical reasoning and they don't
think for themselves yet, but even with trillions of data points, they still can't get some things done. So I see this trend towards smaller models that can run on our devices or on edge computing near our devices, and ones that are specialized
in particular data sets so that they actually get that right. Like, if you are a medical AI model, you should be trained on medical data, not on random crap from
around the internet. How much of that is that gonna be a trend across everyone's use of AI? - I think it already is a trend. So pretty much in any device that you're using, your smartphones or what have you, there's models that are
running on the edge. That simply is the most
efficient way to compute in many applications. So you're gonna see that. It is also the case that you don't necessarily need to always apply the biggest
model to basic problems, right? So let's say you're like a business owner, what you're trying to do is basic Q and A. You want to use your, you
know, customer interactions or some sort of like a data
set that you train off of. You don't necessarily
need the biggest model. On the other hand, you're writer, you want creativity, you
want that turn of phrase, you want uniqueness, and then, yes. Like a bigger model will have more. I think inherent in your question
though, is this notion of, is like generative AI it? To some degree. And there to some extent the jury's out. It kind of, you know, just
to like be philosophical for a second, the big question there is as these models become bigger, do they contain in them
basically the world model, what's referred to as the world model? Like a complete
comprehension of the world. Now, it's certainly the case
that with generative AI, these models have surprised
us to some degree. Like they're doing things in
terms of image generation, video generation, text
comprehension, all those sorts of things that we would not expect. So that's certainly happening, but there's also a very
compelling argument that says, no, they don't. Language has a certain
level of abstraction in it, and you'll find some experts
talking about this, you know, it has a certain level
of abstraction in it, which is not completely
adequate to express sort of everything of the world. And there'll be situations, for example, if you have a robot and
it's trying to maneuver around a space, for instance. There's many aspects to
that space, potentially. Let's say it's a gardening robot, right? And it's like out in the garden and it's doing stuff. Well, there's leaves on
the trees that are moving, the grass is moving. Does it actually need a detailed comprehension of like all of those? And if it does have that, does that make it
computationally prohibitive? So there's cost considerations
that go into this. There is a specific data
training that goes into this. And this may go all the
way down to the base layer, where today you have transformers and there may be other alternatives and there's certainly things like MAMBA and some of you might
have heard, like JABA and other approaches in the industry that people are working on. - We could probably get a
bit deeper into AGI later on in the conversation, but what sort of fundamental
research do you do at Amazon? Do you actively look for
these alternative ways of developing AI models? Or is it all in now on transformer
models and generative AI because that's what your
customers are asking for, or you're really customer
focused organization at Amazon. So everyone's saying they need an LLM and you've gotta focus on the LLMs. - Yeah, it's a combination. I'd say we are extremely customer focused. So in the sense that, but that doesn't actually prevent us from doing a lot of research activities. And essentially when we
talk about customer focus, what that means is at all
times we're thinking about how this could make things
better for a customer, what benefits could a customer
derive from something. But when you back that out, it
turns out that there's a lot of fundamental things you
can do, which hold the keys to potentially creating
revolutionary results or very good results for customers. So it's a combination of
things is what I'd say. Not all our bets are on generative
AI already described like many models, which we use today, which are not generative in nature. If you went to our warehouses,
for example, we have robots that obviously have models too, that are working side by
side with human associates. They're doing things like,
you know, finding products, picking up products, visually
inspecting them for defects, you know, plotting like optimal pick paths and things of that nature, which aren't necessarily
generative in nature either. So it's a combination of things. - [Ryan] Yeah, yeah. - It's not all in on
transformers, in other words. - Good to hear. (both laughing) Because it really is
a big world out there. - Yeah. - And I think we overthink
the transformers a little bit. But as we're talking about
these practical examples, let's talk about that third
layer of what Amazon does, the application layer. In other words, the stuff
you and I are gonna interact with most of the time when
we're dealing with AI. What are some of those practical examples where you over years
really like two decades you've been building this
AI into what Amazon does, like how are we experiencing
that as Amazon customers that we might not be realizing? - Oh, you're experiencing
it all over the place and frankly, generative AI is everywhere at this point, is what I would say. There's scarcity a a significant part of the company that's untouched by it. So I can just give you so many examples. So if you are like an advertiser that wants to advertise on Amazon. Well, last year at the unBoxed Conference, that's our advertising
conference, we announced that you can do image
generation with it, you know, lifestyle images that you can do. And the folks that have been involved in the advertising industry know that creative is often
the most expensive part of advertising. Well, now you've got some automation to help with that. That's generative. If you are a seller that's
trying to sell on Amazon, then your product listings
can enabled with generative AI as well to just to make them
more compelling, more accurate, so that works with it. If you are a coder, right? So we have Code Whisperer
pretty extraordinary results with it, right? It's gonna recommend code
to you, code blocks to you. You know, on average I
think we clock something like 57% improvement in
efficiency in output, which is like a really big deal. There was an example recently where we had five Amazon developers do a Java upgrade for us. I forget which versions it was. And you know, they did like a
thousand applications in like two days, which is just staggering if you think about what that is. And then we have Rufus,
which is a shopping journeys. In Alexa, we had this
preview called Let's Chat, which just makes Alexa much
more conversational and fluid and so yeah. I could just recount so
many of them for you. They're all ending up and
showing up for customers. - I mean, within your
teams or your experience across the company, what
sort of impact does that have on the jobs of people at Amazon? So like, they can get more done, but does that mean people
are just producing more? Do they get to not work 12 hours a day? It doesn't seem like
anyone's getting fired because you literally
employ millions of people. But how do all those
pieces reconfigure as more and more AI becomes
part of the daily work? - So this is like a common, it's kind of like a meme
I'd say in the world. And, you know, it's a new
technology, it's very powerful. It has all these productivity benefits. So does like everybody
lose their jobs, you know, how does this turn out? And the answer's no, that's
not been our experience. And you know, it's almost
like a sense that happens with every new technology that emerges, often because you
haven't quite figured out what it's gonna unlock
for the economy yet. But in our experience, each one of those things just
results in more growth. Right? So an example of that would be
we have something just north of three quarters of a million
robots in Amazon warehouses. What does that mean? It means that the warehouses
become more efficient. They become safer. Things happen a lot faster, but at the same time, you
know, we take those, you know, gains and kind of plow them back to offer more things to customers. And the consequence has been that the workforce has doubled in size, I think over the last few years to like 1.5 million at this point in time. So that's our expectation. There's always more things to do. There's always more compelling, there's not like a ceiling on
the amount of economic value or value that can be created in the world. And so if you take that
as a guiding principle, it's just more stuff getting unlocked. - Yeah, what I was sort of
touching on there was the sort of trust and the fear aspects of AI. And they're not specific
to Amazon, so I don't mean to make it about you as a company, but I spent a lot of time
reporting on those trust gaps. And what really comes out
across all of the data, and it can be true of
America, it's certainly true of the western democratic
countries as a whole, is that they fear AI and to
some extent use it less or use it in more limited ways than people in some other
countries, particularly in Asia. And people here are more skeptical, both of the big tech companies
and of the regulators in the role that they might play in AI. And the most shocking figure I saw of all was about five times
as many Americans are scared of AI as they are excited by it. What does that feel like
turning up to work every day? Like your whole job is
to give people more AI. Does that feel jarring? Do you see those numbers and think, oh okay, how
can we change the products to close those gaps? Like how does it work in practice, your relationship with that information? - Look, I mean, I love science fiction. So there's scarcely like
a dystopian AI future that I haven't like, you know, read about and enjoyed, to be completely honest. And there's so many
like popular media kind of memes around this thing, you know, super computers just kind
of raging outta control to a terminator, to Skynet. There I've said it, Skynet. And you know, so these
things are out there. There's not a lot of
foundational in reality, what I will say is that there needs to be a responsible approach to AI because AI is unlocking
a lot of capabilities, much like the internet did before, and much like other
technologies have before. And so we have to approach
it with that sense. And so there's principles
which are emerging around this. And, you know, for those who
are interested, you know, a good place to go see this would be in our Titan Text, LLM Service Card. It's a mouth full, but Titan Text is basically our family of models that we give to
enterprises, LLM models. And so we, there's a service card and it highlights some
principles around responsibility. It can be articulated
in many different ways. But yeah, I'd just like to pull three out because they make sense to me, and I think they kind of resonate. The first one is- - This by the way, is
practically answering one of the audience questions
from Louise Kraver who wants to know three
ethical challenges about AI. - [Vishal] Here you go, Louis. - You're getting your answer. - Thanks for that question. So the first one is veracity. You want AI to be
fundamentally not hallucinate, be grounded in truth. And be accurate. And that's a pretty broad definition that also includes things like
hallucination, for example. Which is not intentional,
it's just a side effect of things that happen with models. The second is safety. - [Ryan] Yep. - And so here, you know, we do things like extensive red teaming. You will have to test these
models to see what happens and what they produce in
different circumstances. And the third point, which is related to that is controllability. - [Ryan] Yep. - And the notion there
is when you do broadly, when you interact with an AI model with broadly similar things,
with a broadly similar set of inputs under broadly
similar circumstances, you would like output or outcomes to happen, which are broadly similar to one another. It's predictable in some
way, shape, or form. So those three concepts
kind of drive, you know, then like a ton of stuff
comes out of their, you know, transparency and, and
explainability, et cetera, et cetera. - Now thinking about
grounding the AI in truth, one way to do that is something called retrieval augmented generation, RAG. Is that the best way to do it? What have you found is most useful for the AI to be able to
prove, to cite its sources and for you to be able to
check its work, basically? - Yeah, so one quick point
before we get into RAG. So RAG is very interesting
and very valuable. One quick point to note there is just to acknowledge a debate that's been raging in the
industry, I'd say for a while. So there's a school of thought that says, and I'm deliberately creating
like a polarized explanation of it, just to bring the point out. But there's a school of thought that says that you don't necessarily
need to go ground... So the whole point around
RAG is it's a structured set of truths, if you will, right? So RAG will say things like, Tallinn is the capital of Estonia, right? Did I get that right? Yes. I think I did. Tallinn lean is the capital of Estonia. So that's just the fact. And you don't want the
model to hallucinate that under any circumstances. So you can have like a list of truths that you could refer to. It can be organized as a knowledge graph. It could be organized as a vector database and many different ways to
like create that organization. And you're kind of guaranteed, quote unquote guaranteed
truth in that situation. A different school of thought
says, it's just a matter of time until the models
become capable enough where this truth will kind of get woven into the models themselves. So in other words,
groundedness will emerge as a property of a model
that's capable enough. - Okay. An open source model, like how's that gonna
happen in a closed model? - Yeah, so all those
strains are, you know, of discussion occurring there. It hasn't happened yet. There's some papers around this that would argue that no, you
will always need some kind of structured groundedness to this. And, you know, some of those things kind of take you back down
almost to the deepest, most fundamental levels of this, which is can transformers
actually produce this? So can other technologies produce this? Juries out. Lots of work is getting done. Long story short on this though, is you do need groundedness. So there are some
techniques that enable it. Right now, all the various
techniques I've seen, I would say RAG is probably the one that works the best in my experience. - [Ryan] Yep. - And then, and if there's others that people have run into, I wanna hear about them. - Yes, please send them
through in the questions and we'll ask Vishal. And then thinking about
the safety culture, like there's technical aspects
to building safe models, but a lot of this is cultural. You need AI literacy, you
need people to be taught sort of ethics at the
beginning of the process. And then there's a big debate
about open versus closed models and hearing you talk
about red teaming, you know, one way I think of open source is yeah, it can be really dangerous if
you're just handing over your model to Kim Jong Un in North Korea. At another level, you can just be permanently red teaming and realigning the model if
you're going open source. How are those debates sort
of handled inside Amazon? What's your take on it? - That's a great question. That's a great question actually. So just to take a step back and do like a cultural
note about the company. One thing you do have to
realize about Amazon is that the company has this
notion of leadership principles. You can go read about them, they're widely available,
you can go see them. And one leadership principle that we have, which kind of unifies
the entire employee base, like everyone's heard them and internalized them, is that success and scale bring broad responsibility. And I sort of put this
in the umbrella of that. Which is, if you're gonna
create powerful technologies, then there's a responsibility
that accompanies them. So in that, like the three
things I talked about, you know, veracity, safety, controllability are like an inherent component of it. Absolutely. And that applies to both closed and open models is what I would say. Now, other folks might
articulate it differently, but I think those
principles kind of stand. That's one aspect. The second is we also have to recognize that just as with the internet, you know, when the internet evolved
and it came into being, it threw up new challenges, new opportunities that we
then had to adapt to, right? So for example, I can give like so many examples and they'd be familiar. - [Ryan] Yeah, please. People love concrete
examples, so go for it. - You know, one example I toss out is like cyber bullying on social media. That's like an example, right? So we don't know how, you
know, the opportunities that will get unlocked or the challenges that AI will throw up. So we have to be adaptive to that. As more things are discovered
about it, you know, our framework on making
sure that it's safe needs to evolve and keep stepping out. That's gonna be very important. A different aspect here is, so, you know, obviously Amazon has a
lot of these practices. How do we take it to our partners? Because we work with a lot
of companies out there, a lot of people out there, and
they're educating folks through AWS is a very important component. So we have educational
materials that we take out and there's also a
component of just baking it into the tools themselves, right? So I spoke about image generation earlier, so you obviously wanna make sure that that doesn't do bad things. - Yep. And how does Amazon get
everyone on the same page internally on those principles? Is this something that now
gets baked into onboarding? Did you all have to do big
staff retreat, going like, "All right, we're all in on generative AI and here are the rules." Like how do you make
sure everyone abides by that rather than just trusting people are reading the fine print somewhere? - Yeah, I'd say leadership principles is the core component of it. And you know, that's the thing that basically distributes this concept and makes sure that everybody shares it. They are repeated often. And you know, it's not
dissimilar from like other organizations which are large,
where you establish a system of ethics and behavior and
principles and you follow it. And, you know, there is a
saying that, what is it? Repetition does not spoil the prayer. - [Ryan] Yep. - In many of these situations. So there's an aspect of that. But like I said, you also
have to be not formulaic and we have to adapt to
what happens with AI. Look, I mean, it's certainly the case that we are seeing some
properties emerging in AI. So you know, the notion of,
for example, induction heads to get like a little bit technical. You know, there are discoveries
that are made along the way. They are scaling laws, which are being, which are being observed as well. There's gonna be other principles that come up, emerge over time. You know, Moore's law did not exist before semiconductors became big. You know, Metcalfe's law did not exist before people began to really think about what a social network or
networks could look like. It's probably the case
that there'll be other laws that remain to be discovered, but that's gonna unfold
itself to with time. - Let's bring in Richard Gucci's question on the screen here. Could you share examples when
AI caught you by surprise? - Oh, Richard. There are like many
examples of this, you know, over my career. Way back in the day, I remember this is, I'm going back to like 1999 now, when one thing we were experimenting with and you know, was taking
some techniques and- - Were you a PhD student at this point? Or you were out in the workforce? - I've actually been on a leave of absence for 30 years for a PhD. - Okay. - One day. - Let's talk about that. What was this PhD on? - One day.
Distributed systems. - Okay.
- Yeah. Distributed systems, which also I love. We can talk about that. - Okay; okay. We'll get to that in the IDI section. - Yeah, so back in the day, you know, we had like a catalog
that we were working with, and I was trying to
classify items in there, and we were taking some
clustering techniques, which today would be
considered very, very basic and clustering with them. And it was just magical to be
able to give some examples, not have to define hard
rules and to have it go in and cluster these things. It was electrifying. It truly was electrifying to watch. It was almost like- - [Ryan] It was a positive surprise. - It was a positive surprise. It was like something was
biding, you know, these things. So that was pretty electrifying. There was another time when,
which was many years later and after deep learning had emerged or was beginning to
emerge, there was this time when I got exposed to this project where we were taking like images, and we were trying to classify whether there's a cat in them or not. You know, people in the tech
industry love cats as you know, all of you love cats. Is there anyone here
who does not love cats? And so it's classifying, and
I was looking at, you know, what's going on in the different layers. And at a certain layer,
you saw the outline of the cat being extracted
and kind of emerging. And when you inspected it, and the fact that this had not been
programmed into the system to extract those kinds of things, that was kind of happening by itself. Self-organizing to some
degree was pretty magical, I have to say. - Yeah, well that- - Very early examples. - That's actually a really
good way to get into AGI and the self-organizing question because obviously one of the
principles of being human or a capable human is your autonomy and your ability to
self-organize your life or your family and so on. So I dunno that that is
like a definition of AGI, but when we think about, like,
I think Amazon defines AGI as a safe controllable AI
that instantly understands and anticipates and accomplishes a customer's needs and wants. And there are a few other
definitions out there, but that to me describes
a different type of AI to what we are talking about today. It's a leap ahead. And Sam Altman in November was saying, we're gonna need something more than LLMs. Yann LeCun at Meta basically
thinks we just need to go back to the drawing board and come up with a different approach for AGI. And this morning I published a story about a startup called Verses AI that thinks you actually
need a nature based approach that we need to reconstruct biology, biological organisms and create webs of small AGIs basically rather
than ever aim at a single or fuse super intelligent systems. So it gets a bit back to
this distributed intelligence idea of your PhD. So I wanted to get your
take on kind of what type of AGI Amazon is aiming at and what do you think
is the sort of framework of AGI when we get there? Are we going to have a web or a huge network of all
these little specialist AGIs? Or are we headed towards
something more like the tech markets we see today where
there's a few big dominant systems or maybe even sovereign AGI where you have like these
national super intelligences? - Oh, just some deep questions there. - Just a Little question. - Not difficult at all. And I do want to correct one quick thing. I never actually finished that PhD. - Okay. - You know, so I wouldn't
want to claim credit for that or not yet anyways. - But tell us about that. What was your thinking in that PhD? Because it kind of was
directed, pointed at AGI. - It's relevant. So it's basically without
talking about that, you know, that exact thing, what I'd say is there's this notion of like when you have
multiple agents or nodes or machines, how do you
get them to co coordinate and collaborate with one another, right? And that could be required in many different situations, obviously. And there's this notion
in distributed systems of loosely coupled versus tightly coupled. So super tightly coupled
would be they're all sharing the same memory, and they're
able to operate from that. Loosely coupled would be just
message passing, et cetera. And there's pros and cons to both of them. So what I'd say is the world as it exists today in terms
of technology as it's likely to evolve, is going to be a
combination of those things. And you see some to and fro that happens in the tech industry on this obviously. The world started back in
the day with mainframes and client server, which,
you know, you had sort of these gigantic systems and then, I'm sorry, with mainframes, you had gigantic systems
in this terminals, not that smart hanging off of that. Then you got disaggregated into
client server a little bit. - [Ryan] Yep. - The web and the Internet's emergence, disaggregated them even more. Then you had quite cloud computing emerge because there's inherent
economic efficiencies and just technical capabilities
are enabled by that. And so there's a to and fro. What I'd say is I think I
see a heterogeneous world personally where these things
coexist with one another. So I definitely do see the notion of smaller scale focused models, smaller scale focused
AI that are operating, they operate today and to great
effect and to great benefit and then larger scale things. The G in AGI stands for generalized. - [Ryan] Yep. - So the inherent notion there is that these models are gonna become more and more generalized
able to do more and more and more at human expertise level. - [Ryan] Yeah. - And so that's what the AGI thing is. Now the thing is, getting
a precise definition of AGI is a hard thing as you know. If you ask 10 experts on AGI, you'll get 10 different
explanations from them. But I'd say the notion
of generalization is something that stems from it. - [Ryan] Yeah. - Now, just creating a big model
could also potentially mean that it makes the smaller models better because you can do
things like distillation. - [Ryan] Yeah. - And you can do things
like taking something larger and then then making
more focusing from it. And yes, you could have a situation where you have more distribution. Now the startup that you were mentioning, I'm not super aware of them, but what I'd say is the
notion that, you know, a world in which everything
could be distributed is an interesting one. I'd say the current access
of development has been around larger models. Now we know that there is some efficiencies to begin there. You know, perhaps they're like too large, and they don't always
need to be that large. So it remains to be
seen how that'll evolve. - Yeah, it seems to me- - But you need a crystal ball
to be able to completely- - Well it seems to me like that the big models have advantages
in particular tasks, but then there are other tasks, and this is the one that
Yann LeCun mentions. He was a bit like, well we have trillions of parameters in these models and they can't instruct someone a robot to unload a dishwasher, but I
can teach my 10-year-old kid in 30 seconds how to fill
up or unload a dishwasher. So what's the point of these trillions of parameters if they can't do these basic sort of human tasks? - Yeah. - And so his thinking there is
you have to go back to basics and think how do babies learn basically and how do we as biological organisms, all the cells in our body, how do they interact with each other? How do they talk to each other? And until you figure out how that happens and then have them all
talking to each other, you won't in the end get
to something that counts as like a super or general intelligence. - Yeah, super intelligence
is not a term I would use. With generalized intelligence- - Because super intelligence
implies like a single overlord or is there some other negative? - It's just not a term of art
today is what I would say. So if AGI has 10 definitions, I think if you pose the
term super intelligence, it's gonna have like
a million definitions. So it's just hard to, not
everyone will have the same thing. So this is just more
semantics than anything else. What I would say is that
yes, it is probably the case that there's more fundamental work that needs to be done in terms of AI that the current
technologies we have are not the end all and be all. So in that sense, I
think that is accurate. Yes, it is true. It kind of hearkens back to
what I was saying earlier a little bit though, which is, you know, is it a fair assumption that the world model is contained
within a large enough LLM? And can it get inferred? So is it the case for
example, that you could using enough language
describe the act of, you know, a cat walking down to the enth degree where a large enough system would be able to comprehend it simply by
learning enough about language? - [Ryan] Yep. - And the jury is out on that. There is not complete agreement
on that is what I would say. There are some pretty
valid points made that, you know, the models that
exist, the biggest models that exist today aren't capable
of doing certain things. That are certainly, or sometimes just not capable
of doing them efficiently. - [Ryan] Yep. - And so that's a valid point. I think we will see
groundbreaking research and developments coming up
that that's probably true. - Now time to build in a couple more of these audience questions. Julius Leevish is asking, "What are the greatest technical
hurdles Amazon will have to overcome in the near future?" So I will take that to mean
related to the path to AGI, but throwing some others if you want to. - So you know, we've been working on AGI for a long time, and Alexa was an early example of that. You know, it's in millions of homes. It's entered a part of the
popular culture, if you will. There's songs about it and you know, it's- - [Ryan] Songs are made with it. It's very popular here. - So we've been working
on this a long time. One thing I'll say is
for the pursuit of AGI and to get to AGI, it's very
unlikely that there's going to be a moment in time when
you suddenly decide, oh, AGI wasn't here yet and AGI is here today. That's probably not gonna happen. It's gonna be a series of developments. And I think we are along that journey. We've probably been on that journey I'd say for 20 years now. It may not feel like it, but we have been, so we are using AI across the company in pretty
much all aspects of it. Like I said, you know,
it's part of shopping, it's part of the robotics, it's part of you just take any example, and that's just going
to continue to apply. The interesting thing will
be to unlock possibilities that haven't existed before. - [Ryan] Yep. - So for example, you
know, we have self-driving, there's the Zoox unit that
engages in self-driving, and there's other units that are working on things of this nature. So I'd say the biggest
technical hurdles in my mind are unlocking use cases, which
will be very valuable to people and to humanity, which
have not been unlocked yet. - Yep, and what's on your dream list? What are the things where you sit there and you think, wow, if I could
do this by the time I retire, you know, I've really made it. - I mean the, like I said,
the inherent in the notion of, you know, the next
industrial revolution in inherent in the notion of like
unlocking an age of abundance is the idea that the
profusion of use cases that will become possible and
benefits will be limitless, almost will feel limitless. So, you know, one notion,
for example, that's kind of near and dear to my heart
is the idea of applying AGI, you know, just to everybody
and to every situation, especially to situations
which are challenging today. So I'll pick like an example. Imagine that you are a doctor in an underdeveloped
area of the world, right? And medicines are in short supply, information is in short supply. You know, a patient walks in, like how do you interact with them? How do you diagnose them? How do you do treatment for them? And now imagine that you
actually had a helper that was able to take your notes. It was able to take, you know, the patient's history maybe
because this has been used for a little bit of time. It was able to take wellness and disease trends in the
region, maybe even able to take into account medicine
availability in the region and then come up with diagnoses and maybe a couple of like
alternative treatment thoughts, right? Phenomenal difference in
this person's life, right? And so that's such a
magical kind application. Imagine a student who's in
school, let's say middle school, and they're struggling
with some coursework and you know, they happen to be in like, in a certain position with a certain level of comprehension about principles and topics and fundamentals. And now they need to get to
this like, other position. Well, you know, the way I
can sometimes think about it is what we have today are these because of economic reasons,
these cookie cutter bridges that exist and somehow that
bridge needs to fit you. And that's how you get to walk across. Whereas, you know, what AGI enables you to do is just inherently
be way more flexible, take you into account, take you through your own self-paced way, what works for you using
principles that you understand and sort of walk you over that bridge to where you need to be. Pretty magical. Go ahead. - I'm gonna ask you about that because I agree on the personalization point. That's where, you know, there's been a lot of
talk recently and sort of the debate is getting
narrowed down a little bit to people talking about
sort of AI being like an assistant with superpowers for you. And that sounds great, but then kind of the challenge
I'd throw out there is that a lot of the time when I've tried to use different AI
systems, you know, I worry that it's based on lowest
common denominators and averages and cliches basically when all you're doing
is fancy auto complete. And so my question then is, you know, what is this gonna mean
for neurodivergent people? And I would classify myself as one of those people. And I don't want something that is trying to shove me into a box or doesn't work the way my brain works. So can we imagine a world where there is like an AGI for each of us? Like is that reaching too far and dreaming too much where instead of there being a few huge AGIs, we can actually have these
extremely personalized AGIs that do act as like a genuine
assistant with superpowers where we don't have to be fit into other people's boxes and labels? - So first of all, thank you for sharing that personal note about yourself. I definitely appreciate that. And yes, I do think it's the case that it will be a capability with AGI to increasingly personalize
interactions with people. And to be honest, you know,
just to take a more trivial, I'd say example. I'm not really the same
person in the morning as I'm in the evening, right? In the morning, I'm trying to get to work. I may be like more focused. I don't want an AI to be recommending ideas
to me about, you know, maybe what to watch or
what to do and so on. I want, you know, clip responses. I want specific interactions. I want it to help me do
what I'm trying to do. In the evening, I might be
in a different frame of mind, might be more relaxed. Tell me what to cook to, you know, gimme recipe ideas or
maybe like entertainment options or like something else. And you can imagine this applying
in many different places, even with stuff that's
much more significant, like neurodivergence. It definitely makes sense that AGI will be capable
of far more personalization with you and maintain
personal context about you, which could be anything, right? It could be the music you prefer. It could be the things you like to eat. It could be the mood you are in. It could be a social interactions just like all these dimensions. - [Ryan] Yep. - That's a different way of
saying that to some degree, it's gonna essentially
have a model of you, if you will, and then
it'll be able to adapt to communicating with you and
the style that works for you. So yeah, I think your hope
is actually well founded. - Yeah, and I guess some of,
I mean, to be fair as well, I think sometimes we tell ourselves lies. We think we're more unique than we are, or we think we're more
divergent than we are because that's a nice
story to tell ourselves. - Yeah. - And what I'm noticing from a lot of the algorithms on social
media are now AI is like, well actually we're a bit more similar to each other than we realized, or, you know, we are a bit
more repetitive than we think we are when we actually think
we're these clever people who have all these different modes of living and ways of thinking. And actually the AI knows us
better than we do sometimes. - Well, I mean, we should
celebrate our similarities, and we should celebrate our differences. I think they're both add like
a ton of value to our lives. - Yeah.
- Absolutely. And yeah, it's not necessarily a bad thing to have reflected back at you, you know, as long as it's done for
your own personal benefit in a way that works for you
to have reflected back at you. You know, it's like having
a trustworthy friend, if you will, is the
direction it should move in. And it's quite instructive
at other levels too, right? So for example, when you consume
media, sometimes it's good to know how much media you're consuming and be made aware of that. You know, I've done things like
just tracking all the places I've been in the last month or so, and just getting that reflected
back at me is valuable. So getting- - But do you actually
do anything different with those numbers? I just feel like I'm being lectured- oh, another nine hours on the phone today. I don't do anything to change. - (laughs) Well, so you know
that this is slightly a, you know, a different topic. But what I'd say is the other aspect that we've been working
on for a very long time with AI has been ambient computing. That's been a very big
aspect of what we do. And so the idea has been
you don't really want people to live their entire lives
with their heads bent over. You want them to hold their heads up and be able to talk out into the open. And you know, what you see with
Alexa really came from that, which is yeah, we want computing to be just being woven into your life, not become the center of it. So, you know, you use it when you use it and then it fades away into the background when you don't need it. - Yeah, well that actually,
like, in a strange way that now brings me to embodied AI, like how like uses of AI with
robots, for example. And so Alexa is kind of
like almost the opposite of that Jetson's idea,
sort of in the sixties, that we were all gonna
have this sort of assistant that wheeled around in front
of us and made us sandwiches and babysat the kids and whatever. And then the AI you are talking about sort of fades into the background. How do those sort of
choices get made in Amazon? Because, so you do have this embodied AI or AI powered robots in your warehouses, and then you've got this stuff that fades into the background. Is it just different for each use case? Like how do you sort of
like handle those questions? - It's really focused on customer benefit. And so the idea there is,
you know, what will be good for customers to use and to
have as a part of their lives? That's what we end up focusing on. And in fact we do robotics
even outside the warehouse. So for example, we have the Astro robot which is, you know, a robot
that can like, follow you around and does things and increase security. - [Ryan] Oh, I did not know that. - Especially for like- - I can buy that as a robot from Amazon. - You could, you could buy
that as a robot from Amazon and, you know, it's small and
kind of follows you around. So yeah, we are investigating these things and investing in them again
to being customer obsessed. That's actually one of the
leadership principles is customer obsession literally is what it's called. And so that's a component of that. I'm excited about embodied AI as well. And that's because it's
also pushing the frontiers of AI in some interesting ways. You talked about some of the limitations of existing things that we are doing and there's some interesting thinking that comes from when examining
embodied applications where you want to bring in visual input, you wanna bring in audio input, make that all a component
in, you know, in sort of some sort of a multimodal
approach to these things. - And are these independent AI tools or are we talking things
like prosthetic limbs that are powered with AI? What sort of things are we talking about? - Yeah, there's no reason
why there's not, again, the G in in a GI would be generalized. And so there's no reason why
all these applications will not exist and benefit from it. - [Ryan] Yeah. - I'd say all of the above. - Hmm. We're getting down to our sort of last sort of quarter
of the discussion now. And I asked you sort of some of the things that surprised you on AI and things that you dream of building, but what is the sort of killer app that would improve your own
sort of work life or home life? Like for AI? What's the thing that's most
missing out of your life now? - First of all, I'd say that
my home life is tremendously improved by AI even today in the sense that, and it's
just a spectrum of things. So Alexa definitely improves my life. There are like specific things
I could pull out from there, which are quite interesting. And actually this goes
back to the question that was asked earlier,
when did AI surprise you? So Alexa for example, has
this notion called Hunches, which I don't know if
you're familiar with, but it's this notion that
if you're using Alexa to control your home, you
know, you're turning lights on and off with it and perhaps
and you know, let's say you, you're going to bed every night and you have a routine, right? You turn the TV off,
you turn the lights off, you lock the back door,
maybe you hit your bed and then you turn on the white noise because it helps you drift off into sleep. And then one night you
forget to lock the back door. - [Ryan] Yeah. - And what happens frequently is that Alexa will notice that
and the AI will notice it and say, "Would you like
me to lock the back door?" And it's one of those things
that increases security and safety in your life just inherently- - Unless you're putting
the trash out while it does it and then you're locked out. - Those things happen, absolutely. - I locked my husband outta the apartment once, like, so don't worry. Like it happens. - I bet you heard about that. - Definitely heard about that. Yeah. - So yeah, I mean, so the notion here is that it's gonna basically
benefit all these aspects of our life, and
you could think about things to do with safety and security. We get these mails and communication
constantly about people that have been in difficult situations that, you know, they've called... For example, there was this one case where this gentleman sort
of fell through a staircase and was alone at home and was able to use Alexa to call for help and someone came in and
sort of rescued him. So ensuring safety, ensuring security, sort of like a baseline
case is very important. And then all the way up to
helping you be more creative, I guess I'm talking about
the pyramid of diesel almost, right?
- [Ryan] Yep, yep. - Helping you be more creative, more fulfilled in life is another aspect. So I think AGI is gonna run that gamut and to different degrees
it's already begun to enable all of it, but
this is my point around, it's a process, it's not a specific point in time. - Yeah, and is there, 'cause
there is a big debate about, and one of the things that,
sorry, I'll take that back. One of the things that has surprised me as companies have attempted to integrate AI into their
processes since Chat GPT launched is I expected that some would race ahead or some would be a bit afraid or bound up by the regulatory implications what they were doing. So there would be different
tracks of how people adopted AI, and I expected that people
would not really gravitate to the productivity benefits
the way that they did. I thought more of them would try and use AI for more creative
ways of doing something or doing something they
weren't already doing. And a lot of organizations
are just gravitating to the productivity
efficiency aspects of it. Do you have a sense that
there's a certain point in time when the creative aspects are going to get unlocked? You know, is it we just
need to get more familiar and more comfortable and then we turn to the creativity? Or do we need a different type of AI that just makes it obvious
that you're supposed to use this for creative purposes? - That's a very interesting question. I'm not sure I completely agree
with the thesis behind it. - [Ryan] You can disagree. I wanna be disagreed with.
- I'll tell you why. Because, you know, the sense I have is that a lot of those productivity aspects are actually related to creativity. So it could be around writing an essay, it could be around, you know, maybe creating like a blog
post that can help you out. And it is true that, you know,
people have engaged in that because that's the aspect of creativity that connects with economic
value in a more direct visceral way for a lot of people. So it compels 'em in that direction. But I think it certainly is the case that there's probably a lot more poetry being written in the world. Now I can't vouch for the quality of it, but there's probably more
limericks in the world and more poetry in the world and, you know, pick your
favorite genre, haikus and karans that exist
than that existed before. And I think, you know, I
think it's safe to predict and that we'll see some
sort of like an explanation. - [Ryan] Yep. - Now this kind of stuff has been happening
for a while, right? So if you look at, you know,
filters that people apply to their pictures and so on and so forth. So it's gonna continue to unfold. Again, I buy the idea
absolutely that there's going to be more things that happen
here than vanilla elements. There already are, right? So we are thinking about multimodality, bringing an image and
things of that nature. Certainly we ourselves do
things like voice synthesis, speech synthesis from the written text, which is not LLM basis, a different set of techniques
that applies to it. But what I will say is that what LLMs have done is they've
accelerated this journey. Because it's been some
time since we saw such a profound shift in
capability than we had before. So we can't take that away from it. So I think generative will always have that place in the sense
that it was the first thing that kind of shifted us in
a stepwise way towards AGI. - Yep. And then Neils has a question here, which I think relates to that. So the question is, "Do you
think once we have AGI there will be an intelligence explosion?" And I suppose that connects
back to the abundance point. Where it's like, if we
went like this with LLMs, and we can keep going like
that on the way to AGI? - Yeah. I think, Neils, there already
is an intelligence explosion that is already underway. You know, the fact that
if you have a thought, you have a question, you could be sitting around the dinner table arguing with someone about some something, and you could literally turn to like Alexa or, you know, search or
like many other techniques, but basically say, "Hey, you know, what is the capital of Estonia?" Someone says it's Helsinki,
it's not, you know, and you can have an argument
and get that resolved. I would argue that already
brings intelligence into our world. Will that accelerate and
just get bigger and bigger? Absolutely. And it's going to be a
great thing for the world, in my opinion. As long as we do it with responsibility, it's going to unlock amazing things. - Now thinking about how
people here in the room sort of might wanna take your advice and apply it inside their
own company, for example. You know, there's a lot of startups here and at the very least, a lot of founders, if they're not trying to build AI, they know they need to make use of AI. So what advice do you
have for those people who are looking at their AI options and what are the underserved
areas of opportunity where you're like, if someone does that, they can come into Amazon World? - That's a good question. I've been an entrepreneur myself, so I've been in those shoes before. So look, I mean, it's hard to be formulaic about these things. And what I will say is
that if there's some people in here who are, you know, PhDs in machine learning and
a joint PhD in mathematics, and you've decided that
transformers are not it, you are going to invent
the next, you know, fundamental thing which is
gonna unseat transformers have at it, bring it on. Like we would love to see that. In terms of general thoughts
around incorporating AI especially into like a startup effort, what I'd say is, I'd start with this. I'd say AI is not perfect. Recognize that. Don't build your company on the assumption that it's going to be perfect. So what are some imperfections
that exist today? Imperfections like latency
in generating answers, right? That happens. Images that you generate
might not be perfect. They might be off. That kangaroo with the extra arm, for example, depending
on how you want to do it. - Or Kate Middleton, that
wasn't even an extra arm. It's like a quarter of
an inch of a zipper. She's being eaten alive. - Yeah. I'm sure with the right
prompt you could get in an extra arm. So assume that's gonna happen. And so what does that mean? That means, you know, whatever
product you're working on, whatever offering you're working on needs to be resilient to that. It needs to provide room for that. That's the first thing I say. The second thing I'd say
is, you should bet on AI, you should not bet against it. So in other words, realize that
AI is continuing to evolve, become more capable. It's happening very, very quickly. You should probably have a roadmap and a notion of your company that adapts to that and evolves with it. So bakes in, you know, it's
kind of the counterpoint to the previous point I was
making around imperfection. Assume that it's gonna
become more and more capable. And so what would get
unlocked in your company when that happens? Having a notion of that
could be very helpful. Or even like if you have a set of things, if we had this, this is
what we would do with, it kind of puts you in a
position where you're ready to take advantage when that time comes. The third thing I'd say is maybe much more of like an eternal
point for all businesses and for all people is
to know your customer. You need to know your
customer extremely well. And what I'd say there is
that, you know, you can kind of get to know your
customer at two levels. There's a notion of what
the customer does today, the workflows they engage in, the kind of tasks that they have to do and how they accomplish
them today using technology that exists today. I would call that the conventional wisdom understanding of that customer. If you know something deeper
around what would benefit them, what can advantage them, what can help them lead better lives, and you can unlock that with
AI, that's super powerful. Super powerful. So at Amazon we have
this technique we use, which is called working backwards. And it's quite instructive
actually, which is before you get a project started or you begin to build a product, what you do is you
write the press release. Like the first thing you do is
you write the press release. And that press release
obviously is not gonna contain details about the technology
and how amazing it is. It is probably, unless you are someone that's serving engineers, obviously. It's not going to contain
details of like every last, you know, feature function
that your product has. It's going to be like a
customer quote that you try and put in there, and it's going to be how you
want the world to perceive it, you know, what you have built. And then having written
that, now build a product and now implement the
technology that's worthy of it. And this sounds like a super simple thing. It's actually really hard, and it's really, really hard for often for people like myself that come from a technical
identifying to sort of do it and very instructive and
educational when you do it that way. So that's the thing. The fourth thing I would
say is AI is expensive. We should acknowledge that. So you have to be mindful of that because if you're a startup,
you have a finite, you know, pool of resources, you
want to preserve them and you want to, you know, until you sort of realize some value here. And so being cognizant of that. You don't necessarily need
to run the biggest model. You don't need to do all those things. And when you're ready to
build, build it on Amazon. - And then I guess one follow
up to that is, you know, I have personally experienced
frustration trying to get chatbots to give
more useful answers. And I'm not even trying to run a business or integrate it into huge workflows. What advice do you have for people who hit those frustrations? You know, obviously
you're not gonna tell them to give up, but how do you sort of get through the frustration
layer and keep persisting and what are the benefits of continuing to persist in your mind? - Yeah, what I'd say is that this is such a fundamental transformation and we are probably, you know, different ways to talk about this. You could argue that, you know, we are on a new s-curve in technology. So for those of you that are familiar with the whole innovators
dilemma, you know, framework of understanding things, it's
the case with when you move on to a different s-curve in technology. And the idea there is initially
the technology, you know, you've got like an s-curve
where a technology came into existence and for a while
it was a certain state and then it rapidly improves and becomes very, very
good at addressing things. And then it ends a plateau and diminishing returns begin to set in. Often when you move to a new technology, there's a different s-curve that's higher and you've got benefit and
value on this dimension. It's higher than the previous s-curve, but the lower level of the second s-curve is below. And so what happens is you
actually have less capability in some ways, but different
types of capabilities. - [Ryan] Yeah. - And you know, mastering
that is important. - It's like when I swapped
to be an AI journalist, that was definitely what happened to me. But go on. - I've experienced that myself many times in life, right? And so for instance, when
the internet first began to become big, there's some
things you could do, you know, off the web, which you
were not capable of doing. And obviously that accelerated, so persist is what I would say. This curve is gonna go up a lot. It's almost inevitable. And learn from it, adapt
to new tools would be my secular advice. So like everybody, including including engineers like myself, right? Which is you wanna bring in things like Code Whisperer into your life to make you more efficient. And you could be legendary
as a software engineer, I guarantee you'll benefit from it. - There we go. Keep persisting everyone. You've been a great audience. Thank you, Vishal.
- Thank you. - And enjoy the rest of South by. Thank you. (audience clapping) (uplifting music)