all right thank you everyone um we're gonna get started it's really my great pleasure to introduce timnit um who's going to give our second keynote for this first set ml conference so timnid has had such a profound impact on our field that it's hard to select which papers to mention when introducing her so I chose three um I think earlier on her work on deep learning to analyze uh street view imagery and estimate demographics of U.S neighborhoods was quite interesting for instance I remember that we could predict um based on tinder's findings who would vote for certain political parties based on the types of cars that are parked in neighborhoods which is quite interesting then timid had a number of discoveries on the limitations of facial recognition technology in a project called gender shades and then more recently was a co-author of the paper titled stochastic parrots in which her and her co-authors raised a number of limitations of large language models including bias potential to deceive and environmental costs just to cite a few and I think we can all agree given the last couple of months that uh this paper was just a little bit ahead of its time but uh was right to point out all of these limitations so now Timmy is the founder of dare which is the distributed AI Research Institute uh where the mission of this institute is to prevent the harm of AI and bring diverse perspectives to promote a more beneficial application of AI so I'm really thankful that timnit agreed to spend the next hour with us today she's very very busy has received so many awards last year was named one of times 100 most influential so thank you so much Tim the floor is yours I'll mute our mic while you're speaking and then when you're done we can take questions from the audience welcome thank you that was such a beautiful introduction I feel super honored um to be here and um yeah I wish I could be there in person but I really can't um I couldn't really travel uh but I'm I'm glad thank you for inviting me um so I wanted to have this conversation about AGI um given uh the kinds of things that are going on and um so I um what I was gonna talk about today is based on a paper I worked on recently and um you know I don't know I learned stuff while working on this paper you might not it might not be used to you um but I kind of sort of wanted to that's the conversation I want to have today and um the reason is because you know I you know about like 10 years ago or so even before that when I was working in the space I didn't think that the whole HEI thing was very Central it wasn't like mainstream I mean I don't know maybe some of you might have a different experience but that was not my feeling um and now I think that even people who didn't you know want to work on it or something like that have to like write AGI and all of their documents and things like that I'm hearing from some of my friends and some organizations and so I was just trying to figure out like what's going on why why is that the case um so you know so that's what uh you know kind of want to talk about um I I don't know you know I think you all know like everybody's talking about HR right now right I don't want to I don't need to give examples of that and stuff so the first question I had was like what is it uh what is Agi I uh some like trying to find different people who Define it differently so the open AI um one they say a highly autonomous systems that outperform humans at most economically valuable work all right then there is this book I was um looking at that was the first book on AJ I guess contemporary approaches to Adrian I think it says a software program that can solve a variety of complex problems in a variety of different domains and that controls itself autonomously with its own thoughts worries feelings strengths weaknesses and predispositions all right then I saw Peter Goss who says he also was one of the people who coined the term AGI that a computer system that matches or exceeds the real-time cognitive not physical ability of a smart well-educated human what does smart mean and what does well educated mean I mean those are some we can get into some issues there and then here's Russell norbeg it's like a universal algorithm for Learning and acting under any environment so like what I hear when I see this is like this is sort of what I understand it to be it's like a thing that can do anything for everyone in the environment and so I was like you know this to me is an engineer before I was um quite got into the space I was an electrical engineer I did electronics and you know um we were very much into you know scoping at our systems understanding stability like that was all all the class I took right um and then you know you know try to ship products to your most of your work is like um scoping things out testing things out documenting whatever and this to me felt like an unscoped system with a goal of trying to do everything um it sounds like why you know and and this book you know it says they say it's a fully you know it's not a fully well-defined system but then they still want to build it so I'm I'm kind of trying to understand why why why why do people want to build this thing and what is it so that's kind of the rest of my talk right it's gonna be you know why is it that people want to build this thing uh what What's the motivation and then what what has the consequences what have the consequences been um and the short answer is that it's rooted in the 20th century anglo-american you know Eugenics tradition and then the long answer is the rest of my talk so let's go to the long answer um so this is how I'm gonna try to structure the rest of my talk and this is the first time I'm giving this talk so I'm going to try to make sure that I'm you know timing it properly for everything but um yeah so a brief introduction to Eugenics then we'll go to you know what we call second wave Eugenics and then we'll talk about the AGI to Utopia and the AGI apocalypse so when you know we're like now we're going to talk a little bit about a brief introduction to eugenics so what most people think about Eugenics they don't think of it as a progressive moment right um that was popular among scientists but that's what Eugenics was it was a very it was considered a very Progressive Movement and it was it was thriving in a lot of universities Harvard Stanford um and this um quote from Philippa Levine and Ellis Bashford says the optimism of eugenics and its aspiration to apply scientific ideals actively was among the reasons that so frequently attracted progressives and liberals um and this is um the Stanford Eugenics history project um the first president uh of Stanford University where it was a one of the most um powerful eugenicists of the 20th century right um and he definitely believed in the um you know that the human race could be improved by restricting the reproduction of inferior populations such as disabled people and people of color and he believed in the hierarchy of racist uh with white people at the top and black people at the bottom he promoted the forced realization of disabled people Etc and by the way I don't know if any if you go to this if you've been to Stanford you know that Jordan Hall is like there's all sorts of places dedicated to this guy um another big one at Stanford was also Shockley you know who got the Nobel Prize right for physics for the transistor but he was also I mean he wrote more things about each and it's probably than and that so you know most people think Eugenics equals the Nazis right um and then they also think Eugenics is over after World War II but both are wrong uh the Nazis actually studied California sterilization program in detail when they were working on their blueprint of the racial hygiene policies that obviously eventually led to the Holocaust um and if adronix also did not end after World War II so California's sterilization program continued until 1979 the British Eugenics Society lasted until 1989 then changed their name to the Gallatin Institute then to Adelphi genetics forum so you know there is a term that uh what says I I wrote it here but um somebody says is the eternal return of eugenics right um it always comes back in one some form and um you know and Galton very you know famous for uh statistics too was the person who actually coined the term Eugenics right he is Charles Darwin's cousin and he's you know the founder of the modern Eugenics movement I'd say the first wave water and Eugenics movement although there were instances of what we'd call proto-eugenics before that like if you know uh Plato actually in the Republic talked about uh making sure that you know people were the inferior people were not able to um reproduce and that the Guardians and the elites and whatever would you know be encouraged to reproduce so um so these were the kind of the tenets of that what I call first wave Eugenics right um actually a male coin that's some first wavegenics starting 1883 plus so you want to improve the human stock through positive and negative eugenics so both methods both positive and negative methods it's important to note we're popular and long many in U.S and the Europe not just Nazi Germany um so I think that's important for us to understand right because a lot of times people just Associated genetics with with that so the positive Eugenics means you know you make those with quote-unquote desirable traits breed and remove the people who have undesirable traits so that way through generations you quote unquote improve the human stock negative um Eugenics means you stop the opposite right you've stopped those with quote-unquote undesirable traits um from Brady um right so um those they can they call defectives and imbeciles idiots congenital invalids morons and quote-unquote the feeble-minded and they identified them using IQ tests and I mean I'm not going to talk about the history of IQ tests here but those themselves are quite racist um first way to Genesis believe right Dalton for example that poverty was a result of one's inferior nature um more recently in 1994 there was this book the bell curve by Charles Mary that basically says a similar thing that social policies like welfare won't work because um you know people's inheritance genetically determined differences in IQ so that's like the hereditarian view of first wavegenics and obviously you know first wave Eugenics also believed in the hierarchy of the racist so that's why they didn't want miscegenation laws I mean they didn't want misaturation which means like you know intermediary intermarriage around along the races all right so that was um um uh the the brief introduction to Eugenics right so now let's talk about what we call um second wave Eugenics and like my co-author thankfully um Emil coined the term um the test squirrel bundle of ideologies so let's go through the um test scroll bind uh bundle of ideologies and let's go through second wave Eugenics so you know that's According to second wave eugenesis they're like what was wrong with the first wave eugenics it was that um well you know the really the thing that was wrong was these population level um improvements of the quote-unquote human stock that you want to have resulted in horrible laws right like genocide like trying to sterilize people Etc so first wave second wave of Genesis are like um well we can we can um we can do this without those kinds of laws without those kinds of population things right so we don't need these population level policies we can just have you know and we don't need to wait across generations to have these you know improvements of the quote-unquote human stock we can just you know improve the current too in a stock right I can improve myself parents can design their children based on you know a genetic engineering so um basically you know claims to be liberal because there's no coercion um so that's sort of the the more um kind of uh the second wave Eugenics so so I would say the second wave Eugenics so we'll talk about the test real bundle but so let's talk about you know the test wheel blender are transhumanism extropeanism Singularity singularityism uh cosmism rationalism effect of altruism and long-termism so we'll go through them one by one so transhumanism right the word transhumanism was coined in 1940 by Julian Huxley and Julian Huxley was the president of the British Eugenics Society uh from 1959 to 1962. so Julian Huxley writes by controlling the mechanisms of heredity the human species can if it wishes transcend itself not just sporadically but it's in its entirety as as Humanity if enough people can truly say I believe in transhumanism then the human species will be on the threshold of a new kind of existence as different from ours as is uh that of pecking man it will at last be consciously fulfilling its real Destiny I'm making you suffer with me because I had to read the stuff that last whole month so I'm not doing this alone so anyways so according to Julian Huxley right again uh president of the British Eugenics Society um you know it we can Humanity can just transcend Being Human right that's the goal um and that really is the the destiny the collective Destiny of humanity so it's not just to improve the human stock like the first wave Eugenics but it also is to transcend Humanity altogether so that was I would say early um transhumanism right but modern transhumanism started in the late 80s 1990s so they combined his vision huxley's vision of transcendence with a new methodology of second wave Eugenics which can deter which can depend on new technological advances like artificial intelligence or um nanotechnology or you know genetic engineering so again the point is that people could choose to quote unquote radically enhance themselves and become post-human and one be what is being post-human mean it becomes it means to be a new Superior species that you can create so it's the same you know it's kind of like the roots are obviously first wave Eugenics right but now you can create this Superior human race without supposedly having population level coercive issues so Annette Boston is one of the most famous um transhumanists around um and so he defines a post-human as any being that has one or more post-human capabilities like indefinitely long Health Plan uh Health span augmented cognitive capacities enhanced rationality Etc so that's transhumanism and so the first organized transhuman movement started in 1993 they called themselves at the extrupians um there's five fundamental principles of extropism boundless expansion self-transformation Dynamic optimism intelligent technology spontaneous order um so then a few years later Nick Bostrom and David Pierce founded the word the world transhumanist Association WTA so we're at the E we're at the extrupians now uh so so now let's go to a singularity I can't say this word Singularity all right and so these are the people who talk about the singularity so this is another variant of transhumanism um the leading Advocates included Ray Kurzweil and udovski Eleazar udovski um uh I haven't I don't know where very great Carswell is around I think he sort of fizzled off but I used to hear about him about five years ago a lot more five six years ago so they emphasize the coming quote-unquote technological singularity and what's the technological singularity so there's two ways you can look at it first is the point at which the rate of technological progress becomes so fast that it causes a fundamental rupture in human history so human they believe like humans and machines will merge right and life beyond that is hidden from view behind an event horizon so us as units can't even understand really what's happened what's going to happen we can't really imagine so uh according to Kurzweil the universe as a whole will quote unquote wake up as intelligence spreads through it he predicts this will happen in 2045. you'd ask you however predicted that this was going to happen in two years so if you haven't made your plans um for the singularity in 2025 I highly recommend that we all do it um because you know who knows what's gonna happen in two years so it's going to be an event horizon so I'm waking up uh for the universe to do you know some intelligence to be spread but anyhow that was um this is in this extropean um listserv uh where you'd also kind of um you know according I don't know what calculations he did but his calculations show 2025 there will be some sort of singularity all right so that's that's the first aspect the second one you know is an intelligence explosion whereby algorithms undergo recursive self-improvement until they become quote unquote super intelligent so that's and that that will be that will you know that would have another history rupturing event and so the resulting superintelligence could further enable the merging of humans and machines and also the colonization of space which is the collective human destiny um so that's the singularism the next one is cosmism what's cosmism so uh Ben gortzel's 2020 Cosmos Manifesto says you know humans will merge with technology which will inaccurate a new phase of the evolution of our species so they're all you know trans human right um it's the Transcendence of humans merging with technology Etc um we will develop sentient Ai and mind uploading technology that permits an infant indefinite lifespan to those who choose to leave biology behind and upload we will create we will spread to the stars and roam the universe where we're going to create synthetic realities we're going to develop scientific Future Magic that's a quote um much beyond our understanding and Imagination so um basically these are transhumanists who are concerned about not just about how humans become post-human but how post-humans transform the quote-unquote universe right that's cause muscle and then we have rationalism so this kind of um started around you know 2000s late 2000s I think 2009 is when the less wrong Community was founded by yudovski who's a singular and it you know transhumanist right and this isn't it says they say it they're an online forum and Community dedicated to improving human reasoning and decision making quote unquote rational rationality training uh they say many members are heavily motivated by trying to improve the world as much as possible and the less wrong team are predominantly not motivated by trying to cause quote-unquote powerful AI outcomes to be good because they're so motivated by improving the world as much as possible um and the the singularity in the second sense uh that I discussed the intelligence explosion has been one of the most Central topics that um they discussed so that's um rationalism so now you know again extropeanism singularitarianism cosmism they're all like transhumanists right they transform they're all different kinds of Transformers rationalists are not necessarily transhumanist but they were founded by transhumanists right and also a singletarian um and but they're very much into it like that's a lot of what they talk about is you know that a lot of what they talk about is being transhuman and the singularity that brings us to effective altruism um effective altruism emerge around the same time as rationalism and it's basically it's sibling right um You can see it as you know when the principle of rationality is applied to the ethical domain so how do you do the most good possible with finite resources so there is a lot of overlap with the two communities um effective altruism and rationalism and the initial Focus was on global poverty but you know after discovering Boston's book uh and his work in general many members of the effective altruism Community started to focus on the very long-term future of humanity some right when we say long-term millions billions or trillions of years from now um Boston imagined a techno utopian future right because of the human the trans humanist kind of idea of the radical human enhancement you can have radical human enhancement in many ways and one of them is again um AI um oh Pop pill I meant to say paper but hey uh in a different language that means paper too uh so um in 2003 paper um title astronomical waste um Boston said that if Humanity colonizes the universe and creates planet-sized computers to run virtual reality worlds um and amino populated by the digital people the posthumus the population could be 10 to the 38 in only the Virgo supercluster and at least 10 to the 58 within the accessible Universe why are these numbers important because in total utilitarianism our sole moral obligation is to maximize total quantity of net value right and if you have these 10 to the 58 digital people think about 10 to the 58 digital people and and if you say they they have net positive lives that is astronomical literally astronomical amounts of value so that could be like the most good we can possibly do and that should be our goal our goal should be to have these 10 to the 58 digital people have net positive lives that you know to generate um astronomical amounts of value so long-termism is when you know effective offers reason wait a minute if our goal is to do the most good possible in the future can contain these astronomical numbers of people that's what we should do instead right we should focus on that instead of current problems except in so far as the current problems allow us to you know help those 10 to the 58 people have net net positive good positive lives so long term is Hillary Graves and William mccaskill write about simply ignoring the effects containing the first hundred or even a thousand years um and on the most canonical accounts of long-termism being posed human again that transhumanist kind of idea of transcending humanity and emerging with machines Etc talked about is is a central component of quote-unquote fulfilling our long-term potential and that reminds me of again uh Huxley who was the you know remember the president of the British omugenics Society who wrote this um when he coined the term transhuman he was talking about what our Collective Destiny is right um okay so uh right and so we're uh here so now we have learned about right and so according to Boston right even small probability increase right affecting this um future that we're talking about and mean literally saving billions of human life today so that's that's kind of basically what we should um focus on so we've learned about all of these different um uh to summarize we've learned about all of these different ideologies for uh many of them have roots and Terence humanism which was created by persuasive Genesis right now what are some um some properties of these uh what we call the test real model first is the historical roots of in contemporary communities they have a common lineage first back to first wave agenda it's both in goals and the actual people who founded it they have um they're intimately connected to transhumanism there's a huge overlap in the various communities right like many of the founders actually overlap like for instance you know udovski and Bostrom uh Etc um like uh gorsola and stuff they're you know they're they're all overlapping in various communities um and um they want to radically modify the human organism right in in different ways there's also um what's called eschatology like things about last things convictions and these take two forms Utopia and apocalypse uh what's Utopia Utopia you know it's kind of like the uh it's the same as the Christianity uh David pierce the co-founder of WTA called the transhumanist project Paradise engineering the complete abolition of suffering in Homo sapiens the rest of their living world Cosmic rescue mission to promote Paradise engineering throughout the Universe Boston's letter from Ethiopia Utopia talks about how the post-human has so much pleasure that they sprinkle it in our tea Kurzweil who was hired personally by Larry Page I see some people laughing in the audience um well I had to take this seriously so you gotta listen to it with me okay so I'm just joking but yeah well you know this is this is what's going on that's why I want to talk about it so hers will who was hired by Larry Page says that the merger of man and machine coupled with a sudden explosion of machine intelligence will allow us to transcend our frail bodies with all their limitations illness as we know it will be eradicated illness as we know will be eradicated we get to transcend our frail bodies as human beings so that was the Utopia right and first wave eugenesis also understood their project in utopian terms so Gallatin wrote a utopian novel for example called the eugenic College of Kent say where where parents have to uh test to have tests before being pronounced to fit to reproduce right so the unfit parents were banished from this utopian place so he wrote this novel of course there's also the apocalypse so an eschatology you have the Utopia and you have the apocalypse so the apocalypse comes like this it's like well we can have Utopia if we have our official super intelligence and we become post-human and we have like you know all of this biotechnology or Etc but that can also bring us an apocalypse right the same technology that can bring this to Utopia that can also bring produce quote-unquote clear and future dangers that may be unprecedented that was um Ray kurzweil's term in human history artificial super intelligence consult literally all of our problems all of our problems but if it's not quote-unquote value aligned it could spell quote unquote Doom that was Boston's um word um so but but the thing is critically even though they have these apocalyptic Visions they believe it is our duty our moral obligation to bring Humanity to Utopia even in spite of these risks so both building Utopia and avoiding the apocalypse are a priority they should both be a priority for us finally but yeah I mean not finally this is the third um property they're all have highly discriminatory views and that is it's not a it's not a surprise it's not a surprise because you know comes from first wave eugenics so first of all intelligence quote unquote is really important for transhumanism because and and to quote unquote fulfill our Collective human Destiny why because it's really important to colonize space and become quote-unquote post-human um I mean Bostrom argued that blacks are more stupid than whites than he used the n-word in his 2002 paper he wrote that dysgenic pressures were an existential risk to humanity as as big as you know nuclear war for example what are the static pressures dysgenic is the opposite term from eugenic right eugenic discogenic dysgenic pressures means like less desirable people um reproducing so here is a quote from his uh paper it says currently it seems like that there are there is a negative correlation in some places between intellectual achievement and fertility if such selection were to operate over a long period of time we might evolve into a less brainy but more fertile species let's call it homophilogram I don't even know homophilaprogenitis lover of many offspring so this is an existential risk according to him because if Dumber people we produce we get to you know evolve into a donor species basically that's what he's saying and that's an existential risk same um right like so he just recently tweeted about you know IQs dropping in Norway and he was like oh but it doesn't seem to be because of you know dumb people immigrating so I guess we'll have to figure it out right so this is the same kind of attitude about intelligence um they worried that they knew though in their conversations they did worry that people were gonna label them as um neo-eugenics racist or new Nazis Etc um but they you know like they say since we as transhumans are seeking to attain the next level of human evolution which is basically kind of what the agent the first baby Genesis we're trying to do uh but they do um cite the support um you know Charles Murray uh again who wrote The Bell curls uh in 1994 and it is like very well known for his um anxieties but also dyst genetic pressures right as um as a as a risk um more recently there was this um article that uh Carla Kramer wrote about how effective altruists had um like were experimenting with a new test score called the peltic score called um potential expected long-term instrumental value to identify members of the community who are likely to develop High dedication a low um a candidate with a normal IQ of 100 would be subtracted Pulpit scores because points could only be aired above and IQ of 120 uh low pulses value is now assigned to applicants who work to reduce Global poverty or mitigate climate change while highest value was assigned to those who directly work for EA organizations or an artificial intelligence so and then there was this article you know I'm not really going to talk about the horrible um harassment Etc so so again discriminatory views and finally influence and why did I go through this whole entire thing talking about the history and all the different experience and all that that's because of the influence that's really what I want to get to the influence there are lots of billionaires in the movement many who are either in it are adjacent to it and many directly funding it right and this bundle of ideologies was a crucial motivating Source behind the goal of creating AGI and moving resources and focus to it so that's what I want to get to right the AGI now like now let's talk about AGI AGI Tokyo AJ apocalypse so very briefly you know 1956 there was um four guys who wrote this um proposal um to Rockefeller Foundation wanted to have a summer where they you know uh and that was the first time where like in written you know we saw the term AI um officially coined but a lot of time you know but back in the day if you read their readings from like 1956 1958 or whatever the way they talk about what they've built was precisely kind of like how it is right now they're like oh my God we have built a thinking machine and you will be amazed and in five years humans are not gonna have to work anymore what's gonna happen that's how they were talking back then too So eventually that kind of you know High Plains led to what some people call the AI Winters right there were two waves of those Winters and so after those AI Winters a lot of researchers did not a lot of researchers it feels that you would currently associate with a quote-unquote AI didn't really say they were AI researchers it was like you know oh I'm a machine learning I'm in computer vision and natural language processing I mean you know robotics they didn't really associate themselves with this whole you know larger general intelligence kind of goal um so then in 2007 there was this um book again um where uh Ben gordson Edwardson attention Kristen the term AGI um then like you know before that there was like Mary by so in 2007 there weren't so many people talking about AGI working on it funding it Etc right and so they weren't happy about this like they were talking about how you know this is really the field needs to move in this direction we have a collection of quote-unquote dumb Specialists you know um uh doing various narrow AI they call it domains we want to work on HEI um then you have you know um so Ben gortzel if you remember is a cosmos so he believes that you know our our goal is to be post-human and you know colonize the cosmos and the cosmos and make the cosmos happy not just humans then Mary was founded by udovski um and their goal is to basically make AGI safe then 2010 decline was founded by the same goal right make AGI safe so funded by all these tests you know the billionaires that's realistically it's like Peter teal Illinois Etc um but you know and uh uh lost room superintelligence comes out um and then you know deep mind is bought by Google in 2014. that year I don't know if any of you remember how Elon Musk that's when he started talking about like um you know Doom based on AI as a single biggest threat um oh why didn't I forget the whole the my entire thing was to get to uh open air I don't know why I didn't put it in there then after 2014 after this happened opening a was founded by the state for the same goal was to make AGI safe so basically people like Elon Musk and um Sam Outland were not happy that deep find was getting bought by Google they're like oh my God we can have this Doom um and you know deepmind is working on HEI and we can't have Google controlling that so they start and so they you know start open AI they say that it's going to be a non-profit but in 2019 they uh you know become a num a for-profit they get 1 billion from Microsoft you know this year they get 10 billion for Microsoft right so um Sam Altman talks about how yodowsky was actually very important and um in the decision to start opening eye and he was very important in like getting deep blind funding Etc so the whole point I want to make here is that they are like driving the whole you know AGI um craze um so why are they doing that well you know again we talked about AGI Utopia and AGI uh apocalypse there's two ways in which we can get AGI Utopia the first one is like you have a HR that's going to be so intelligent that it will figure out what to do in any scenario right so Ilias escover when um you know Chief scientists at opinea he was asked at a conference at Stanford like uh and he said you know in the future the AGI will tell us whether we should use gpt3 or something like that scenario two is again the transhumanist view right where we will have morally Superior beings um right because you'll have AGI enhanced transhuman Minds benefiting the cosmos um and that's what Ben wartzell talks about he calls it like transhuman AGI like that is the goal of building Asia and so um they talk about then we will have these plus humans will have growth and joy beyond what humans are capable of abundance is a wealth growth to all lines of so desire um Sam Altman says it's clear we will have unlimited intelligence and energy before the decade is out the future can be an almost unimaginably great once AI to arrive growth will be extremely rapid changes coming are unstoppable we can create a much fairer happier and prosperous world um but like for who for whom has this Utopia arrived right so Nando um is talking about how you know uh what we need to do is just scale these models you know it's um it's game over um if you scale these models like large language models or something like that um you'll get to AGI um so now you know this wreath because um basically because they want to create AGI and that's the goal has created a whole race right opening it has you know has to release like chat GPT now Google has to do this and that you know Microsoft now deepmind has to so they've been like basically creating larger and larger models as advertised as being able to do everything for everyone um so in terms of large language models we have you know the paper on the stochastic parents um uh oh no that's not my title I I that's that's not the title of my paper it's on the dangers of sarcastic parents don't tell anybody but um yes so we we talk about you know the the issues perpetuated by largest models like as one is bias perpetuating biases associating you know black people with violence Muslims with violence women with stereotypical careers Etc I mean I'm not gonna go through through that but the fact that you are um um using lots and lots of Internet um texts to train these models and what that represents leads to all of these issues text to image models right like Dalian um I guess um stable diffusion um from stability AI um all the issues there um and now you know on top of this they're racing to have a search engine based on these things right and that is in line with the goal of having quote-unquote and AGI that knows everything tells you everything right you can like answer any question but um if you read for example I recommend the paper um situating um search by Emily bender and shiraksha who talk about how it's really important to have private provenance when I'm seeking information I need to put it in context I need to have I need to know who what's going coming from I need to know how I can kind of figure out whether it's true or false or you know whether it is like discriminatory or not I can't just have an an answer from some quote-unquote all-knowing thing right that's already dangerous um you know and finally you know again they talk about like you know unlimited intelligence and energy um Peter Voss even writes about how nobody's gonna have to work because of AGI they talk about how we're going to have a fairer fairer happier most more prosperous uh world but like look at the worker exploitation actually we're going to have an event where one of these people is going to join us anonymously because they're worried about their identity being um disclosed but there was recently an article in time where these Kenyan workers who were paid like one dollar an hour right remember the Utopia it was supposed to be like unlimited wealth for everyone right they have they're paying them one dollar an hour to filter out toxic tax from chat GPT and they're mentally scarred right there's like all sorts of horrible texts that they're they have to filter out to help open AI create some sort of automated filter so that the rest of us don't have to see this stuff um they had to do a similar thing in some of these text to image models um and so you know obviously that's not the kind of you know we don't have Utopia for we're not at the Utopia at all it's a complete opposite and then finally there's a lot of centralization of power that's going on so I'm getting almost out of time um but you know on one exam is resources that could have gone to a whole bunch of different groups of people um you know serving their own communities are now going to like one company that or two countries let's say that claim to do to have one model for everything right um for example I mean um we were working just on another paper where we were analyzing actually the claims of let's say you know Facebook came up with this I mean meta came up with this paper called No language Left Behind where they literally claim to solve machine translation for all languages right and they even have um you know they have certain languages like um that are spoken in Ethiopia and we looked at their data set we looked at their models for those languages and it is laughable right for example part of their data has outputs of Google translated text so it's like you know testing on your training set right so your their training data has synthetic tests that is output from Google tram the output of Google trans translate right you can't have that as an evaluation for a machine translation task um they have you know instead of having many of their URLs are actually from Russia not Ethiopia stuff like that but then they claim to have one model that does everything which means that people like lesson I'm machine translation company Focus specialize on those languages can't get funding because people go they hear this kind of claim and they're like oh you know what sorry meta has um you know solved it with uh no language left behind and open AI has solved it was whisper so we don't need your kinds of companies anymore at the same time we know that those kinds of claims are not true um and finally you know these uh leaders like Sam Altman talks about how in the next five years you know uh their models are gonna you know be able to give medical advice and uh you know lawyers replace lawyers and all of that and then obviously what's going to happen you're you're just gonna have to pay them for anything uh any task anywhere so you know what it's not power for everyone it's not like wealth for everyone what's happening is actually they're attempting to centralize power and the final thing I want to say is um really quickly go through again because you have the eschatology the last things there's the Utopia and there's apocalypse so the apocalypse is obviously that we're gonna have um again um we're gonna remove our ability to have Utopia because of an existential risk because of artificial super intelligence Etc right and like I said we're morally obligated to both have create the heis program and prevent the AGI apocalypse so they frame both as a safety issue creating AGI and preventing the AGI apocalypse but then according to them preventing the AGI apocalypse is much more about that the people a thousand years from now not existing right rather than the current event the current um harms to current marginalized groups so they create those harms by pumping resources into AGI but then they at the same time um prevent us from investigating those harms by by saying oh no what we really should focus on is not the current harms and issues right it's like the long you know the the humanities potential to achieve Utopia like um and colonize space or something like that um the other issue is um when we frame things as AGI we are allowing the people building these systems to evade accountability right you're talking about an artifact as if it is a thinking machine you're talking about an artifact is if it's not a company you know taking data for from millions of people without their consent like artists for example um and um you know it's having exploited workers Etc you're you're ascribing all of these um uh properties to an actual artifact I think so that allows them to evade responsibility so there's this thread by artist Carla ortz who talks about how you know you can't say like these kinds of images are inspired by humans just like artists are inspired by humans as she goes through why that's not the case and finally I want to close by saying you know trying to build AJ is inherently unsafe like it's an unscoped unscoped undefined uh thing for which it's very difficult what is your hypothesis and how are you going to have experiments that have what we call construct validity um what is from an engineering perspective what is your what's your spec what's what's the standard operating conditions what is your expected Behavior what is you know like how are we gonna perform stress tests Etc um like what's the standard operating condition for a system this was Galactica right they were like it can summarize academic literature solve math problems generate weekly articles write scientificode annotate molecules and purchase and more like how do I test a system what what where do I even start so I mean I want to say you know uh stop let's stop with this whole HEI thing right that's just not I think that it's not about making quote-unquote AGI safe it's just an inherently unsafe practice trying to build it um and so that's basically what I want to say and I'm sorry I don't you know yeah let's let's have time for questions I didn't plan to go that long like