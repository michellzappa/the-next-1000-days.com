all right hi everyone Hugo B Anderson here I'm so excited I could barely sleep last night I'm here today with uh currently with uh uh Eugene Yan uh Shrea shanka and Hamil Hussein who I'll introduce in uh in in due course uh their co-authors will join us join us very soon um but what I'd love to do is we're going to start in a minute or two if you could introduce yourself in the chat uh let us know where you're calling in from and what your interest in this type of stuff is as well and also what you'd like to get out of the session do you are you Building llm Systems currently or work in AI data science um all of these things so let us know in the chat and we'll get started um my name is Hugo B Anderson and I've worked in data science and machine learning and education for a while now and this is a live stream for a podcast um that that I do called Vanishing gradients and I'm just going to put a link in the chat here um um and this is whoa whoa yeah I'll put a link to the podcast and then we have we have a lot more live streams coming up and I put them on Luma so I'm going to put the Luma there as well um and excitingly I've got we've got a bunch coming up but two of my next guests uh coming up soon are going to be shre shanka solo who I haven't spoken with in public solo for some time so we're going to be talking about evaluations um and how to evaluate the evaluators and what that means in the cool product products and and research she's she's doing another guest will be Hamill Hussein who we have have here um again wow dude yeah exact and so Hamill is everyone here has been on a real journey as of of late but um Hamill and our dear friend and colleague Dan Becka recently taught a course um which blew up into a conference everyone here uh spoke at it and Dan um and Hamill correct me if I'm wrong Hamill but you thought maybe going to teach a couple hundred people turns out 2,000 people enrolled became a multi-sided marketplace with products and vendors and open source Frameworks and speakers and Learners um so we're going to have a podcast episode on what they learned um teaching llms to thousands of of people around the world um so I've just put the uh Luma calendar Link in the chat got lots of people uh tuning in from around the world doing all types of different different stuff with llms which is which is super cool um I honestly think um without further Ado we should get started I do want to say if you enjoy this type of thing please do share with friends and hit the like And subscribe uh because we do a lot of stuff like this that that'll be super cool um but without further Ado let's jump in so to set the scene um over the past year Eugene Sher haml and their co-author authors have been building real world applications on top of llms and have identified crucial and often neglected lessons that are essential for building and developing AI products and they've written a a very meaty report on the lessons they've learned which I'm going to share in in in in the chat now um but I would love um if we could start off maybe just go around and introduce yourself maybe starting with Eugene um and let us know why you're even interested in llms all right um hi run I'm Eugene I sell books at a bookstore literally I work for Amazon books um but that's it opinions here are my own um so my work uh is really around recommendation systems and search but recently I think large language models can help readers customers understand their recommendations and search a little bit more and just add a little bit more life to it so that's how I'm thinking of using lar language models to try to help customers uh understand their recommendations and search Amazing um and to be clear you build rexis among other things from millions of people that millions of people use worldwide wide right yeah I used to do a lot of rexis I think my first three or four years was really focused on rexis by the last 18 months I've been trying to catch up with what's happening in language modeling and I'm still struggling to catch up um so that I can figure out how to serve it reli absolutely how about you shya when what's your your background what led you to this wonderful world of L and thinking about Avil so much in particular yeah so I am a researcher and also ml engineer I'm doing my PhD in kind of data management and ux or HCI for machine learning why am I interested in llms I think most people myself included want intelligent software and many people want to build intelligent software in llms make it super easy to do this um not just prototype it but really to get it into the hands of other people um and create very simple flywheel to learn from you know what are the errors and how can we really quickly iterate on it um yeah very simple great and in particular though you've always kind of had a deep pathological interest in databases no I'm joking about the the pathology of it all um but hopefully something we'll we'll get to is once again the importance of of the data so maybe you could just speak to your interest in in databases more generally yeah I well I think all ml problems are data management problems to some extent um and you can look at it in a lot of ways right what are the right samples of data that we use to solve problems how do we quickly incorporate data how do we quickly iterate on these systems to improve them um and a lot of it just comes down to how do you help people manage their data better so they can build these intelligent products um databases are one way of thinking about doing it but databases do a lot of things so can't really compete yeah with this mod of software absolutely and just before we move on to haml which I'm really excited to get to can you please introduce your friend oh this is my dog papaya he's very happy that I came home from the lab what a cutie what's his interest in llms is it just through os does know what it is it's probably for the best the good old days um when we're all naive ignorance is bliss so haml what are you up to what's your interest in llms I mean you have a you know a decades long history in in ml and these types of things so yeah um so yeah I've been doing ml for 25 years I've been I've worked a lot on infrastruct like d uh tools for data scientists and machine learning infrastructure and developer tools um started working with large language models early on at GitHub uh I led some research that led up to get up co-pilot um around large language models and code understanding um yeah I just been doing it for a while um at this point doesn't make sense to do something else so it's not so much like why is it interesting it's like well it' be kind of feels dumb not to keep doing it because they're so powerful of Technology um so I haven't really stopped to think about I don't know I don't never I never really thought about like why is it interesting it's like well yeah this seems like like like you know it's it's obviously like a powerful technology and yeah very helpful in many respects so yeah I love that you're like it's it's just what I'm doing now but then you speak to to the power and I think the power of these these systems that we're still you know working hard to understand um we talked about this recently as part of your course right but Simon Willison has a video where he he actually referred to llms as fractally interesting that you can look at them at a lot of different scales and all types of interesting things emerge and I do think there's so much to discover in there um yeah one thing I'm interested in the most is um or probably the most is like how to really speed up the software development life cycle with llms like how you can code faster uh launch applications faster with a smaller team I haven't worked specifically on that area but is like probably the thing that's the most like I haven't worked on that in a while um but I think it's pretty interesting yeah absolutely and I do think you know some of the things you all think about and what you work on Sher in terms of you know human computer interaction stuff um you know speaks speaks to this and um making sure that developer experiences are as as frictionless and um smooth as as possible um but hey we've got a lot of people saying they've read the report recently or they've read part of it and they're someone says lovely dog so papaya I mean clearly everyone who could not think papaya is a lovely dog um but this is a beast of a report um and I I love how you know you go through um tactical operational and strategic things like position towards different parts of an org who may um you know really need to wrestle with all all of these issues but I'm interested with you six authors I know you've all known each other and been friends for some time but what how did this even get started what's the origin story of this report you want to go oh okay I guess it's oh yeah probably should should uh talk yeah I guess we were just chatting in our chat uh we have a group chat small group chat we were just chatting about how we were thinking of I don't know Brian was talking about something and he was talking about creating slid you know let's maybe discuss during our office hours and then he was saying that he was thinking about writing about a year of llms and I like dude I'm like drafting it right now and I took a screenshot of my obsidian note and it's like it's like haha okay there we just left it at there then Charles gave us the Nick not he like guys it it'd be cool if we just collab and of course Hammer was like hammer and Jason just yeah let's do it and of course the first thing I think about is Tria right Tria I really want to work with you again I mean I've worked with shria on several of my own writing shria has been an amazing editor and I also uh chatter with her about her own her own uh her own her work so it's really exciting and that's how it came about and then okay everyone write your ideas we just figure out how to mishmash and merge them and that's how it happened so I think I think you spoke about tactical operational strategic right that was how Brian thought about organizing right and I think I think I'm so glad that he actually did that because it there's just there's a lot in there I think there's 40 something different lessons and by organizing it at different levels I think it just makes it easier to consume so I think that's how it came about and of course the moment I asked like yeah man coming in she was just so game I was just so so happy that that happened awesome um so I'm interested in we've already mentioned evals so I've got a question for you sh the report really highlights a lot of work and stuff you're doing on evaluating llm output quality um can you share more about your approach to this Challenge and how it differs from what we've seen in traditional data quality yeah I mean this very kind first of all other people other authors wrote about my work and there's no greater pleasure than having other people write about things that I'm researching um I think there's two main things that I was thinking about one was when you consider real world data quality right so not Paul grams essays which every single llm has been trained on or collections of data that you know just are so per have permeated the internet a lot how do a bunch of evals perform right for example canonical needle a Hast stack queries um I I just had this question tried to conduct a bunch of experiments and learned some really interesting things for example when there are typos in the input data or the casing is a little bit different um when you ask mistol to retrieve a name from a document and the name is lowercase mistl fails to retrieve it like small very um small idiosyncrasies like this are really hard to uncover in traditional evals and I just I don't know how to study this further I think the report that we wrote makes one step towards in pointing something out um but there's still quite a long ways to go the other thing around evals that we talked about in this report was um just generally how do you think about validating the valendar how do you how do you construct a flow to give you the right assertions and evals to deploy this is really hard because one people don't know what Concepts they should check for in their outputs right this itself is a function of the llm outputs you have to look at hundreds of outputs to even know what are the weird failure modes in llms and what at the end of the day do you care about as a developer folding um so go kind of doing this process in flow it's dog is stretching um we wrote about this in the paper in our EV gen paper and we that kind of flow made its way towards this um applied llms report which I thought was very exciting and I'm happy that it resonates with HL um Eugene and the others well yeah I actually wrote about it I actually put it in I'm like one of maybe I'm the biggest cheerleader or one of the biggest appr I appreciate that it's because like okay like people every time I talk I uh I'm helping people in the wild with large language models they really struggle struggle with evals like how do you get started how do you think about it what do you mean write a test what does a test look like should I just write like unit test is it like high test like they like mentally like a writer's block and just like don't know where to begin um and they get really they like people shut down they're like this is so hard I don't even know I need like I need some expert to help me I'm stuck there's no way I can do this this is like beyond my capabilities whatever and can be really easy to get overwhelmed um but actually like okay so like my daughter who is 5 years old she's learning programming and like it's really interesting um like if you you know there's these amazing applications like or like languages like scratch that are like really fun and approachable and like teach you that programming is is doesn't have to be hard like you know it can be fun and so one of the things that I noticed with Sha's tools like that were part of her research it was kind of like scratch for evaluations like visually showed you how to think about it you could like build these building blocks and then like sort of it would help you generate evals and different kinds of evals and like validate them and um what I saw in that I'm like wow okay this is kind of like scratch for evals and I would show it to my clients they would immediately say like oh I get it I like finally understand what to do and uh yeah super powerful like super powerful teaching tool honestly and yeah I think I told sh she was like maybe a little bit surprised at first that that's the angle of of the word like I'm like yeah it's teaching I use it for teaching no I've been thinking about it a lot like we collectively are teaching people the processes we're not teaching people tools we're not teaching people how to use llms we're teaching people how to do the strategy around okay I'm committed to building this AI product how do I go about doing it and making sure it works and one year from now it's like still there and people using it like that's there's so much work I think to do in that process oriented mindset so when treya said Hamill might be one of the big one of the biggest celebrators of of my work I was going to joke that Hamill just doesn't stop talking about it but then he launched into it immediately which I love I um Eugene you actually uh prepared a question for for sh which is related to this conversation so maybe maybe you could ask that now yeah I guess the question I have for you sha is why are you so bullish on LM evals LM LM based Eves LM as a judge or LM as a evaluator it's like we've never talked about this before ever kid oh man lot of reasons um so I think there are a lot lot of people who want to build llm powered software that don't have the resources to do you know traditional monitoring and evaluation that traditional ml products would have like it's one person team two people team very small startup um very early stage product and they just they want to ship quickly um I think there's also people who just cannot collect the data that they need to be able to write their own evaluate find to their own evaluation models um and to have all of these barriers seems very challenging and at odds with the Simplicity of deploying llms in the first place like if llms are so simple to deploy we need to have somewhat of a simple evaluation method otherwise like what's the point um and then the other argument here is you these llms are getting much cheaper and much faster for the same I hate to use the words intelligence but I don't have a good substitute um because I don't want to go down this like AGI route but gbt 40 is great this new Claude Sonet model came out this morning it's just getting faster and cheaper and you don't have to run on all of your data but if you sample your data every day run gp4 on it GPT 40 it's affordable could be good awesome so we've really really jumped in in in a nice way but getting in the weeds with evals but I I'm interested haml maybe you could tell us why it's so important to talk about eval as soon as possible yeah I mean so I actually sometimes I think that maybe I should just not even say the word evals because it's like when I you say the word evals then people start looking at you like you're some kind of rocket scientist or something like oh like he's using this word like what does it mean and really it's not really about evals it's like hey let's take an like a systematic approach to making your AI better like just stop stop [ __ ] around and start having like a process like you can only like just eyeball it um you know eyeballing it can only take you so far you know and so it's just like hey it's not really eval it's like it's let's measure have some way to know whether we're making progress or not and that that measurement of making progress like let's make it to where it's as frictionless as possible so that you you can like know every time you make a change whether you're not making progress or not otherwise like there is no other way to make progress and like it's not really about evals it is just how you work on AI like it is not a component of AI it's not optional it's not like oh like you should do evals or maybe you can do it or it's good if you do it if you're not doing it you're not doing like you're not doing any AI stuff like you kind of maybe fiding with it and just saying H I'm in the prompt and I'm using you can use AI like whatever if you're like building a product around it if you're not measuring anything then if you're yeah you don't have a way to make it better if you don't a way to make it better you're not like how are you going to build and so it's really just how you build and then like you can drill into that and say what are the steps yeah I think it's just machine learning 101 I mean the first thing you learn machine learning training set validation set this is the same thing it's just validation set yeah and it does I mean in some ways it it's aoxy for a loss function because in the movement from classical machine learning to generative AI it it isn't obvious um what the loss functions are anymore um and what we need to fit to I um as we mentioned the report covers a lot of ground across the Tactical operational strategic aspects of building with llms um I'd like to hear from each of you as to which area do you think is currently most underappreciated or overlooked in the industry from tactical operational and strategic I don't want to go first because my answer would change every week Eugene go first I mean I'm going to go first but I'm probably gonna say what Hammer's gonna say I think what is overlooked is how to bring every want how do you train your existing software Engineers equip them with the skills to be able do this right I mean um so I think about this a lot I work with a lot of EX software Engineers how can I teach them to do very basic evals you know create some kind of synthetic data I use something data data from kle etc etc um how can I teach them to understand that generation is auto regressive a longer output will lead to longer latency it's just the some of the basic think of how it works how can I teach them that context is just conditioning um so they can be more effective in using that um I think that currently right now I just don't see a lot of that I don't know why um and I think about about that both on my job and outside of my job and that's what I think we haven't done a lot of so yeah sh I'll tell you the one thought that I had last week so we conclude saying something like oh there's like so many demos and we've got to like put things I don't remember exactly what we say but enough with the demos like let's start putting things into production and I heard a talk at a conference that was also saying the same thing and then I really thought like there there's a new demo Plus+ or prototype Plus+ going out there where you will see things in production and you don't really somebody like hacked it together over 3 days and then it's like a catalog product and like a large Cloud retailer I don't even know what to say and and now I'm thinking like how do we move from prototype Plus+ to like the real product and the bar for production has lowered a lot with generative AI um anyways this is this such a rant it's a tangent it's not answering your question but in what way do you think tell me more tell us more about how the bar for production has changed well I think it's you can hack together something in three days that you know passes a sniff test for VPS and it could go to production depending on the culture at the company and this can also be large companies I'm not saying like like chat gbts like this or some or like Bard or whatever Gemini I don't remember the Google equivalent is but there are small small prods you said both of them actually both yeah it was really confusing actually um yeah so I I think it's I think it's like the these products have not really been launched with eval or some sort of rigorous way to quantify improvements um and I I wouldn't call them production like I I now want change my definition of production to like you also have a way of systematically improving the product and a way to convince yourself and other stakeholders of this um and and and I think we're pretty far from that I hope that the report helps people get towards that but and but is there also is this related to the issue that with generative AI like shiny demos are a lot easier to build but then we have this tale of all the things that arise in from hallucinations to difficulty getting prod to not having robust evals this this type of stuff so you get a you get a Flash and then kind of a relaxation of some sort maybe but I think I think still people are putting their first things in production without having any form of evals and I think well go ahead oh I to me that's not a commitment to the process right that's like a commitment to showcasing your demo um yeah there's I like that um we've touched on like learning the process which I think is key I think that we have to like it's worth it to linger on that for a second so a lot of people in the space are really obsessed with tools like you say hey uh we should do evals we should do you know we should improve your rag the first question you get asked more often than not is oh like what Vector of database should I use what embedding should I use are um what uh vendor do you recommend for evals and we get like really hyper obsessed with tools and no and like we're not learning the process and that is where it's going very wrong I think that's part of a some like that's that is part of a narrative which we can drill into um but I think that's that's where a lot of people shooting themselves in the foot is like not learning the process and focusing on tools so haml um you've been working in and out of Consulting for decades now in classic data science ml now all of this generative AI LM stuff as someone who's advised a range a wide range of clients on AI strategies what are some of the most common misconceptions or knowledge gaps you encounter when ORS are first exploring the use of of these Technologies yeah so there's one that is probably I would say the big elephant in the room and really what it is is it's a skills issue and that's related to a certain narrative and this narrative is so um you may be familiar with the role of AI engineer and the way the the role of AI engineer is coined um we should I should probably share a screen or something I don't know if I should do that yeah do it man you're giving away the keynote like heo bring out his big slide his R man no no no I'm not going to take talk slide let me just uh uh hold on let me just so I guess for the audience I think this is a sneak preview of the keynote that we'll be giving at a conference next week yeah I'm not sharing my slence yeah AI engineer Worlds Fair this one I didn't even know you believed in the AI engineer haml there you go trying to uh yeah so okay there's this very popular sort of characterization of the skills that you need and that you should focus on in this new era of generative Ai and it's really taken off it's really like been um you know it it's it's kind of been adopted by many people uh across the industry and it's this articulation of skills where um you know you have the spectrum of different roles and different skills and you have this API level boundary and on the kind of right hand side of the API level boundary you have something called an AI engineer and the AI engineer is like differentiated in this new era because unlike you know unlike years before to deploy an ml product you don't need to know about these things that are listed here training evals and inference data and that you only should you should like be mostly concerned with tooling and infrastructure chains agents so on and so forth like that's the thing that you should pay attention to and so you know like this seemed very reasonable to a lot of people there's some like this is very like a it was very persuasive in some degrees in in some circles very persuasive and it really took off and lots of people hired their talent according to this narrative now I think like one thing that sort of went a little bit sideways with this narrative is that you know evals is not really this doesn't you have to like have a model that you train to need evals like even if it's someone else's model evals is just like how do you measure stuff and evals can like the the notion of evals in unit tests are actually pretty close the notion of having evals and writing software are like very close in nature and then like data actually is another thing like so data literacy so data is not about necessarily training models per se like that's not the only use of data is actually it's actually useful to look at lots of data to see to debug a system and it takes like some data literacy to to be able to navigate data data is kind of messy as you know Hugo like and everybody else like data is like really messy and like having good tools to like parse your data navigate through it filter it do so on so forth um it's actually like surprising like it takes some skill over time and so by you know sort of ignoring evals and data and like you don't need training and inference for sure like those things I would say okay I would agree with that training don't need it if you're using API inference you don't need it using API the thing that is really this really gotten people stuck is like this evals thing and this data thing and by kind of like making it not your concern um this AI engineer role sort of gets stuck after the MVP and they get stuck immediately like immediately stagnate and that that really hurts and then also the title really hurts so the AI engineer title just like if you name someone AI engineer it's the same thing as naming them AI King like the expectations are really high if something is going wrong with the AI like you know your your company is building AI stuff and you're like hit stagnation your CEO is going to look at the AI Engineers like you're the AI engineer I thought your title is AI engineer everyone else is going to look at you and say say hey I thought you're the AI engineer AI Engineers going be like I don't know like what to do now I mastered the chains the agents the tooling I have all the tools I'm like expert I like all the tools but like what do I do and that is kind of this is where this like Gap is where most of the cons is like not most I would say 100% of all the Consulting business comes from and I'm talking about between Jason and I it's like you know Jason's not here but it's like millions of dollars in Consulting just because of this one thing is the talent Gap and I would say this is like very impactful because um you know like if you're building AI or really anything not not not even AI the talent that you have is the biggest impact is the biggest like the lever that you can pull like what talent you hire what skills how you hire and this is the thing people are getting wrong so I'll just stop for a second I think I ranted on first no that that was that was really really fantastic and I think helped um elucidate you know one of the the major issues I do I am concerned now that like I can feel job listings going up as we speak for AI Kings now and that makes me slightly slightly uncomfortable I also would have loved I mean you and I talk about eval and data a lot and I know how you feel about them everyone here now knows as well but I would have loved to be a fly the wall when you first saw this this figure and and and seen what what happened I also I do I'm interested in your all of your thoughts it looks like there are just so many false dichotomies in this I mean it's it's it's useful in some ways but you know if we're talking about product we're clearly talking about eval as well um in some sense or hopefully like some way of measuring the impact or Effectiveness or however you're measuring a product right so I do feel like there are some false um dichotomies there I I am also interested um actually so Eugene you got to ask your question to Shreya Shreya you've prepared um a question for Eugene as well which I think will help us step back and consider the report as a as a whole yeah okay Eugene my question was as as you kind of primarily LED this effort and it exploded into I don't even know how many lessons if you had to pick if you if this report could only be about three lessons which three would they be all right so I think the first one I would think of um so I'm going to try to pick three lessons to what I think has brought the most value to people every time I spoke about them I think the first one is going to be eval I mean not going to lie I mean I was just talking to a Founder yesterday I think Tuesday and he was telling me about here here's how my here's how my process looks here's here's how my team's process looks and I was like asking him why do you only do evals at the end he's like wait that's not what we're supposed to do like we build a product EV no but why are you not evaluating throughout your development cycle it's like huh why didn't we think about that because it's like almost like a training a machine learning model and machine learning model is the artifact is the product you do the eval at the end I guess but now you're bring a product you do the eval throughout your iteration cycle as you update your rag as you update your fine tuning as you update your product you do the eval I think the second thing is lexical search so I'm on this soap box I think Joe is also on this soap box not everyone agrees but I really think that if you everyone should have bm25 as a baseline so now that may not work uh but most most of the time it does I think here's a story whereby uh there was a team that was like asking me hey no Eugene how can we improve our retrieval and asked them what are you using said there uning embeddings like some embeddings on the huging phase I was like why don't you try cons bm25 so I was pretty insistent about it like okay try it and then let me know how how it goes so the next time they got back to me I think like one or two weeks later they said Hey Eugene did you know that bm25 accounts for 80% of our retriev documents relevant documents flip it the other way if they were not using bm25 they left 80% of the juice on the table it's like holy crap that's a lot bm25 is such an easy and Mindless Baseline very easy to tune I mean you don't even need to tune it honestly just use it I think the last one is I think really much Salient because of what happened today today PL 3.5 son dropped okay we're going to update the model just change the model ID I think the point here I'm trying to make is that the models will keep coming and go like when Lama fall drops H's going to start his fineing pipeline bam it's going to be done or like gbd 3.7 kick off his F pipeline the model will come and go but what's Constant your evals your G reals your fineing pipeline your retrieval system right these are your mode the the the durable parts of your system the model is not so a lot of times so the thing is some teams are like okay this model is not good enough let's go start to fine tune our model and they they place all the bats on the model like okay it's all about the model but no you you I I I think it's really think about your pipeline that generates the model which is the artifact your eval that evaluates the model your retrieval system that augments the model with context um I think that makes a more durable system in the long run and like it's just lower effort in the long run so that's my three things that's are you are you satisfied Tri yeah well the Evil's thing is very unsatisfying to me not for anything Eugene says but just the fact that we all talk so much about it and still it's such a problem it makes me wonder like what are what are we what are we doing it's it's it's sometimes like for example for me I am crippled if I don't have EVS when I start a new use case I mean how would I try to prop how try to R so sometimes I'm like advising someone I asking them how do you know if you're making you're getting better it's like sometimes they don't they just look at us like oh my god dude that's so tiring like you look at it over time you just get numb you need a way like assertion based way a a programmatic way to evaluate it uh I I mean gu I'm just lazy um but for the audience I mean do try it um having the EVS up front as your insurance H your test Hest just simplifies your production uh so simplifies your development cycle yeah absolutely you say like about evals is like okay again I predict that roughly a third of people when we have this podcast be published be like okay evals they're going to look up evals and be like okay what are the like let me get the tools like what are the generic evils oh yeah there's this like conciseness metric and this toxicity metric and they gonna like pull all the generic evals off the shelf be like yeah I got evals that is actually worse probably than doing evals so it's like again it's like domain specific evals for your problem and also looking at data so what I tell people all the time is like don't be lazy you have to look at your data people this like a dichotomy is like oh okay there's AI like automates everything in life but it doesn't automate looking you have to look at your data like otherwise you don't know if something is going wrong or you have to look at it for debugging and that that's the part that I think yeah that that's like a very interesting Gap that people succumb to they just want a tool that just does it like let me just do it with some pip install something and I checked that list that haml said like okay I'm doing it no I'm just question no to to some extent I understand this mentality though because it feels like AI should be able to help you look at your data I think it's very funny that the AI engineering job description or whatever that that diagram is does not have data literacy on it as you said haml like data literacy means you can look at your outputs and your inputs and make conclusions about them that quantify your performance that can help you drive up the performance stuff like that it feels like AI should be able to help you do this but I don't I I I don't think people should rely on like chat gbt or AI to look at your data it's very interesting you said that like I was talking with Brian Bishoff he's gonna be on this panel after us so Brian Bishoff actually posted a job titled AI engineer very early on in this like cycle and I had just he was like Hey man like you hate this job title so much like but I you know we kind of went back and forth and I drilled into like okay how in the hell is it working for you because usually when people post the job title AI engineer is basically saying you want a unicorn and it doesn't really work out but for him it worked out because Brian is a machine learning engineer and his process like okay concretely one of his tests are about cleaning data like take-home exam is cleaning data so already if you give me someone so Brian is probably like very rare in in terms of a hiring manager that's giving someone a take-home doing cleaning data like that takes a lot that's like very thoughtful that like you you know he's correcting for I think a lot of things and and is actually that's why it like works really well he's like has a superb process and it also suggests Brian's definition of data AI engineer includes data literacy which is different from the graphic that hammer showed right so I think there's different definitions of a engineer Brian happen to cover uh Brian thoughtfully covered the literacy which I completely agree is absolutely essential and I found it not so easy to train I also I just love that we can say Brian says anything right now I've got so many things I want to say Brian's Brian's into but to put more words in Brian's mouth as well we can when he arrives we can we we can explore this further but I feel like something Brian would say um is look at your data in notebooks as well right like data Literacy for him and for him notebooks are one of the best well no the best eval tool but you know when you're generating data or you've got data you're playing with get it in a notebook explore it experiment with it and then start to automate things and validate things more programmatically um for example I did include I think he might even say that notebooks are magic okay bad joke I I don't know if anyone got the pun but yeah I mean he has this think he's building a to Magic so um books great great joke Eugene and I will cast a hex on all of you with my with with my notebooks um I also put two links in the show notes um and sorry in the YouTube chat one is haml post your AI product needs evals uh seeing this is a panel about evals now um but no more seriously this provides a very nice way of thinking through how to how to softly and slowly build up evals into your system and it's actually fun we Hamill and I talked about all of this on a podcast last year um and kind of came up with the story just beforehand then had EML from reat join as well and I remember shre was in in the chat and really like forcing haml to to break down his ideas more which led to this this blog post I also shared a link to Eugene's prompting fundamentals and how to apply them effectively also like read these blogs all the time I think the guys have newsletters as well follow them on Twitter I mean I can't can't get enough but I do something Eugene writes here at in the second paragraph is his usual workflow um when building so manually label around 100 eval examples then write initial prompt then run evils and iterate on prompt and evals and then evil on heldout test set before deployment um so I've put Eugene's words in his own mouth Hamil and Sher I'd love you to reflect on kind of this rapid iteration as a as a process particularly when we're thinking about how to improve um the velocity of the life cycle and all the things you think about with humans in the loop trail that's a lot of content and I'm not sure I have anything new to add one thing I will say is in schools and in classes and in research they don't teach this process they have standard canonical benchmarks data sets for everything that are already labeled and cleaned for you so you cannot expect you can't do the same thing that you did in school or for a project here you have to actually look at your data first um one thing that I feel pretty strongly about are binary and H having an assessment of you know whether an output is good which is a combination of binary indicators of quality so let's say I care about conciseness tone um whether or not a specific phrase is present whether or not a specific phrase like as an AI language model I cannot that also should not be present right if you're able to characterize what makes for a good output with some combination of binary indicators evaluation becomes a lot easier because now you just need a way to evaluate each of these binary things I think a lot of you know ml literature and even people in practice are trying to train these like multi like ly Kurt scale like evaluators and that gets really hard to calibrate um ai elev ai evaluators like how does it know that a seven out of seven what is the seven out of seven versus five out of seven versus three out of seven these kind of raining based things are just way too hard to train but yes and no is much easier so I don't know if that answers your question no that's super helpful and could you just actually tell us a bit more about like it scales and the Alternatives such as binary but also choosing between options and kind of multiple choice Vibes and and this type of stuff yeah there there's a lot of ways to evaluate things I think um I like to think about it as maybe you're comparing two methods to produce you know the same output so maybe two different models or two different pipelines in this case pairwise comparisons is great like you should look at the outputs and directly compare is one better than the other instead of try to rate each individually and see if one rating is better than the other and reasons for this like goes back to you know crowdsourcing days like when you do crowdsource data management there's a ton of strategies to improve like inter sorry inter at reliability consistency stuff like that it's the same stuff hold here I think there's another form of evaluation that is simply how good is the output with respect to what my implicit constraints are um and that's that's not a pawise comparison problem per se that's like how do I get a rating or some definition of good that aligns with what I think would be good or what my user thinks would be good in that case I think like we should break things down into you five different criteria good or bad I think we can we might be able to you know expand to multiple choice um but in the beginning assertions are just the easiest to start out with and then begin trying to calibrate for very cool haml I know you've thought about this a lot is there anything you want to add yeah when Eugene said okay so this is kind of like very similar to a meme in a way that let me like share my screen share the meme I also know how much you making memes no no so I can't actually find the real me I I can just find a meme that's like somewhat related so like someone can maybe find the one for data scientists but you've seen this meme before right like what my friends think I do what my mom thinks I do my Society think I do whatever right exactly and this like lower right hand corner is what I actually do so when so like everybody thinks you know like AI like you know the real work in AI is like look looks like this Matrix thing it's like some magical thing that's what you're really doing that's how everyone's articulates it that is kind of like you can find evidence of how even this post like thinks that what machine learning people do is that if you look at it if you read it carefully but really what Eugene is saying is like okay in this lower right hand corner is looking at lots of data that that's what it is very unglamorous it's like very tedious that's what you're doing what what you actually do so when anyone tells me their process is like swamped out by looking at lots of data like 50% of the time looking at data I'm like okay you are they actually doing the the work like that person is actually doing the real work of you know AI um that's that's the signal to me and it's like really funny that this meme exists it's like it's it's like hilarious because like when you see the meme you're like haha of course and like whatever but actually it like lives out in real life like this meme these memes actually like are real like people actually have like really gross misconceptions of the role and what it entails so Eugene is absolutely right like looking at data that is where you should be spending most of your time and it it might even feel like this so yeah that's my thoughts awesome and so we actually have a related and and great question in the chat once again about evils um how can you have evals up front um and they've asked Eugene but I'm opening this to everyone if you haven't had your app used by people in the wild yet I think H and sha can easily answer this question yeah I could take a stab at it so if no one has okay so yeah um so I always work on these tasks actually like all the time where no one has used the app yet so there's a lot of things you can do one is synthetic data you can generate synthetic inputs in in your into your system you know you can sort of like outline what are all the scenarios that might happen what are the different features that you want your AI application to handle and the scenarios within those that might occur and then you generate lots of synthetic sort of user inputs of what user inputs might look like and then from there you can generate you know that is like good test data and then you can also write start writing tests you can write some evals that measure that and kind of go back and forth so that's like a good way to bootstrap yourself um but then also like honestly like you can definitely you don't even need to do synthetic I mean so the process that shya shows is actually quite wonderful like the tool that she the scratch thing that I talked about it doesn't it doesn't necessarily suppose that you have any users at all and does the exact same thing it like shows you how you can brainstorm tests based on just your own prompt like if your own prompt says Hey the output should be marked down or it should have three bullets then yeah write a test for that so it's like I give to sh sh I don't want to take the wind out of all that Co stu okay one one one nice thing to think about is for every instruction in your prompt do you think an llm is going to follow this more than like nine out of 10 times probably not write some test around this like having at least three bullet point like these kind of things llms just cannot follow over you know batches of outputs so so kind of think about this um scale case um another thing that is that I would say is talk to the end users to come up with potential sample queries or tests I always very surprised when people try to build things that other people use without talking to those you know end users it seems very wild that this would not happen um we're doing a kind of HCI research project on evals for rag systems and you can already classify you know successful rag systems into people who got a sample workload of queries of like 20 different queries at least versus people who did not at all and tried to build something for them um you don't need that many you need some and you need some confidence that this is what people want right and I think llms are a great tool to scale up what you think is useful so like if you want to come up with additional test cases or data points I would hesitate to say Outsource everything to the L but yeah I think the one last thing I would add to that is that you are your user you are your own user you should be dog fooding the heck of or your app you should be trying to break it and okay collect all those edge cases where you actually broke it to make sure that you fix it so that it doesn't break happy cases bad cases and I think yeah just just based on that I think in the day you could probably like just write a 100 evals which would be good enough I think it's good enough but not the best but you want more but 100 should be fine so I'm very excited to soon be bringing on um your co-authors but before we do that um what else can we put in Brian's mouth um no I'm kidding I I've got one F final question for the three of you just so forward-looking um what do you think some of the most promising opportunities for llms to have impact are and what are some of the challenges you think we'll be facing I can go first I guess a lot of people are now thinking about a lot of companies and startups I think about how do is LMS to do really fancy things right okay I'm going SF next week can I extend my stay oh I want to have breakfast haml figure out a place around dollar H Etc I actually think about I actually don't think about that at all I actually trying to think about what are the things that are unsexy expensive slow that can now delegate to an LM it's like simple things like classification information extraction or maybe for example given some kind given the DMV handbook how can we extract quizzes from it to help people learn the DMV handbook better LS are extremely good at that but for a person to actually do that it can be quite slow so I'm thinking about all this unsexy tasks that is just very feasible at low cost and high scale now great haml shreer anything to add to that before we introduce our new arrivals yeah I mean I think like having good ux is is underrated and a lot of people think of AI as like a one shot wonder that you should just you know the ux needs to be you ask it and it gives you exactly what you want and if and then when you don't get it you're kind of stuck and really like um you know I think more like the co-pilot mentality of like and people abuse the word co-pilot so I hesitate to use it but essentially you know having a graceful failure mode where you ask an AI to do something it shows you what it's doing it maybe gets it wrong but you can edit sort of what it's doing and have it assist you on the task rather than just having you just completely fail and you're like you're like okay that output is garbage or it's not working I think more thoughtful ux is is going to be really key and that's yeah people are I think ignoring that Tri yeah mine is very similar to that I think um AI is never going to be able to read your mind what I think is very exciting is everybody end users can be programmers to some extent um and that's actually where the successful AI Technologies are right chat gbt as an end user you are kind of a programmer keep repr prompting when it's not understanding your intent properly you go back and edit your previous messages I love and and Brian's now in the chat I love notebook like interfaces for using llm or AI products for this reason right it's inherently this work space um and programming environment for even non-technical people so I'm very excited about that for the future well thank you all and what a great introduction to our new arrivals what's up Brian what's up Charles yo we're here that is what's up this is actually really exciting now it's um a shame Jason can't be with us today but you know he's he's living a a a 40-hour day with everything he's he's up to um very briefly Brian is the head of AI at hex where he leads uh the team of Engineers building magic um he has a long and illustrious background I'm I'm I'm really excited that he started the data team at Blue Bottle Coffee as well as a coffee fiend myself worked at stitch stitch fix before that built the data teams in weights and biases before that as well um but even more importantly Brian is the only person I know in the ml space who has a background in non-commutative algebraic geometry like myself I know another a few other algebraic geometers but not the non-commutative side it's you know when we realized that it you know um and so great to have Charles here who Charles does many things um working at modal currently but in broadly speaking Charles teachers people to build AI um applications he's worked in Far psychopharmacology neurobiology um got his PhD from um sh's background is that correct someone correct someone in the chat said Shreya looks like she's at Hogwarts um I don't I don't I don't even know enough about Harry Potter don't know if that's funny or not but it got no this cus does not look like Hogwarts maybe maybe other people will disagree the campan is the most Hogwarts like component of the campus which otherwise yeah kind of resembles diet Stanford I love it so um as we did with all the other guests Brian and Charles maybe Brian you can go first i' i' just love to know why you're interested in LL like why you even working on llms and spending a lot of time thinking about them oh interested no um working on them maybe just kid um yeah um I think I about a year and a half ago I went and did a short research program at the Redwood Institute um related to like interpretability and trying to understand like using mechanistic interpretability like what was going on with llms um I was a part of a demo of gpd2 when I worked at citx using it in production and I remember think thinking like these completion models are kind of fun but like man they're flaky as hell um but everyone had been so excited about chaty PT that I went and did some mechant research and I found it to be incredibly exciting the things that it had suddenly gotten really good at one specific example is asking it questions to sort of like reference earlier in the conversation and looking at the attention patterns of what it's looking at when it's making these implicit associations and I started realizing like okay this feels a lot more like intelligent conversation than I ever had before and I asked myself ultimately if this is really such a big step change where should I be like interested in applying this and as a longtime data scientist and someone who cares a ton about like answering questions with data this seemed like the biggest opportunity and the most exciting thing to apply it to so why I'm interested is because I think the technology has gone through a step change and why I'm working on these problems is because I think they're the most important problems that I can possibly have an impact on awesome and how about you Charles yeah I think um my answer most closely resembles Sha's answer which is like I want intelligent software I um kind of pivoted into machine learning about a decade ago from Neuroscience I did some psychology research shot some lasers into Mouse brains and I was like I want to understand like minds and brains but it's like trying to study stars without a telescope we haven't invented the measurement apparatus to understand animal brains so it's like okay what do we need to do well I guess we need to invent like artificial brains first and get that working and so then I did my PhD on you know studying neural networks and I guess I thought maybe like by the time that I died that I would be able to have like a reasonably intelligent conversation with a computer and then it happened like shortly after I left grad school um and so that kind of pivoted that was like a wakeup call like cold water like oh wow this isn't just about this like broad intellectual goal of like understanding thought and cognition it's also about commoditizing cognition and putting it into computers and like filling the world with intelligent systems um so yeah uh the same similar thing to Brian it's like this feels like the most important thing to possibly be working on this like everything from like the the model Foundry people to the uh to folks Like Us in the application layer um and spanning that and all the way up to like turning these things into products so if you thought there was a chance of you're speaking with something computational that was reasonably intelligent and creative or whatever else before you died and it happened after you left College what do you now think could happen before you die yeah I uh I've become much less confident about predicting the future you know I've decided to just flatten my posterior over future events you know so much heavier discount rate for example on future value because who knows what might be around in 10 years or not wow I mean that I don't know if that's half glass full or or empty but there's you know the glass is in an indeterminant state in the future so who could possibly plan for the state of the glass you're not telling me about Schrodinger glass now are you because I I won't I won't have it what I am interested in this is so you all have created something very valuable and and wonderful here as as far as I'm concerned it your report represents a collaborative effort um to distill a a huge amount of insights and lessons learned about something that's really Cutting Edge currently I'm just wondering like as a collaborative effort what was the most valuable or surprising thing you gained from the process working together um and perhaps um Brian you you could go first yeah a lot of Twitter notifications um I'm still getting them right now actually sh yeah shout out to the infinite group chat Twitter notifications um no I mean like God this is so cheesy but like I think the connections and the community of the group like uh we talk about a lot of like very real things um not just about like generating these particular artifacts but just like ideas and discussing them at a pretty like high level um high level of uh sophistication not high level like abstract and useless um like yeah honestly just like the ability to if I have a deep hard question about AI I have five people on call that like not only will like tell me very transparently if they don't know but are ready to like battle if we disagree about something and that is very valuable how about you Charles yeah [Music] um I think maybe yeah I was surprised by how much alignment there was like the I was expecting like people wrote separate documents wrote stuff out and then kind of pushed it all together and then we had a job of like kind of turning into a single article and I was expecting there to be like oh gez like page one says like um prompt engineering is the best like light your gpus on fire and page 10 says like you're just a GPT rapper you're ngmi like but it turned out that we all like while there was disagreement and distinction and daylight between people the like there was not really any merge conflict to be done uh like uh like the the like core insights that we all gained even working at kind of different parts of the stack I was doing a lot of infrastructure work and some and like advising venture capital and like and other people were working Brian was working on one product really intensely for a long time um like haml and Jason were looking at many projects for uh like over that time Eugene also single product sha like in research taking a very different lens from everybody else in industry and yet we like basically all like we had grabbed different parts of the elephant but nobody had grabbed onto the elephant and declared that it was actually like a crocodile you know we were all like oh yeah that's a very fine Tusk you got over there I had hypothesized the existence of tusks while I was fondling this ear um and that was really great to see um awesome i' I'd love to hear from the rest of you I just do want to say haml seems to have disappeared from the call but he's making comments in the chat trolling Charles so he says Char well reverse trolling Charles is a GPT rapper um or some kind of higher intelligence still trying to figure it out um and Brian you're right funling an ear is definitely going to be a pull quote for this um yeah you know H's just H's just trolling me because I'm an AI engineer and he feels like threatened by my success as the as a unicorn AI engineer who proves his claims incorrect so he's just coming in the chat real hot because he's mad I mean h just has more fun heckling for by God that's H Hussein's music welcome back haml no I won't say anything it's uh I mean you can't argue with unicorns I'm I'm want to step back and watch this play out no I am interested in what like haml Shreya Eugene what what what's some of the most valuable stuff you've got out of this experience I think I can go I think the experience is really fun um I don't know how much how many of you play You Know M RPGs world Warcraft like this thing was like a raid boss and it's like wmo a really solid team I mean it was so satisfying I mean just seeing it come together and then everyone was just like so high agency right I mean Charles was talking about you know how there was no merge conflicts well there were no merge conflicts because Charles fixed all the merge conflicts right he like we had like 60 things and then he shrink it down to 30 things no literally he shrink it down from 40 pages to 30 pages so that's how much Charles Str it now and then it's like okay uh we posted it on three different sites uh no three different pieces on Riley and then ham was like guys we need a site like two hours the site was up but the domain set up I'm like that's really satisfying I mean to work on a team like that it's I I I I just felt so excited so inspired every single day while we were working on this and just seeing it come together it just got bigger and bigger and just when nuts and I I'm really thankful the community enjoyed it and they found it useful um so yeah I I really enjoyed the journey awesome I just really enjoyed just becoming better friends with everybody that was like the best that's cool how about you shre yeah similarly I mean I Echo what everyone says I feel like I learned so much from this group who was building on a day-to-day basis and I probably build much less but talk a lot more or write a lot more as a researcher um so it was it was very eye openening to learn from the group I I will say I think I was there is so much impact that this piece made in especially in verticals I did not even think like academics for example just don't read industry blogs but this is one piece that professors that I met talked to me about they were like oh my God this is like really good I think it was the right time that we released such a piece because people are curious about this people really believe that this technology is changing Computing as a field um and I'm I'm very proud and grateful to be on this project that is able to communicate this to a bunch of people yeah I think that's critical and this is kind of I wanted to avoid this this earlier but you you mentioned Shreya that this type of stuff isn't taught in college and I like I for one am entirely overeducated I think a lot of people on this call are probably entirely overeducated as as well um spent way too long on on campuses love it though but I I actually I think there are strong arguments that the way education evolves should be conservative in in in a variety of ways like you want to make robust you want to have something robust and dependable before starting to teach it to that many people um and of course one this is a slightly cynical and slightly joking take um but one take is that as soon as we rebranded all math and stats departments um as data science data science went the way of the dodo and it's all AI now right so should we have rebranded everything as data science I don't know but that's just one I actually I don't know I feel like that's exactly the skill set that people need to be effective like turns out you can actually probably like prompt your way through a lot of software engineering problems now but then like the like understanding the problem as a good like data science program teaches is much harder so I like to think that actually the rise of like the like data Centric AI application developer will successfully consume all of the data science grads that these programs are pumping out and they will be in a notebook like environment sort of like going back and forth with a uh you know a a SQL database and a language model and a python kernel to uh like solve real business problems man I'm the name of the title can can we say it again data I think proba call application developer because I feel like AI engineer sort of implies that you can actually engineer the system you know like a database engineer is somebody who can like uh debug lock contention in postgress not somebody who can like query postgress uh you know for information so I would I would say it's like a more AI application developer I like that that's really that's I'm going to write that down I love that and I do think if that's what I mean a lot of these programs are teaching I do think and we've all discussed this in a variety of ways but you know it was I don't know several years ago that we all became like the term data Centric AI kind of started to gain more critical mass because we've been so model focused and then all the generative AI stuff took off and we kind of have become like model focused again and it's about bringing it back to the conversations around around data and I think you all are doing doing the good work but this type of stuff this is one of the reasons I do this podcast because this type of stuff doesn't happen in in college you write what you do the type of things you write including this report including your blogs for for similar reasons so I suppose I'm trying to figure out what just what are the best re resources besides your report of course and this podcast for for people where should people be looking to figure out how how to work I I have a couple thoughts here also one if this podcast is really supposed to be about like the data then why is it called Vanishing gradients um but like we can let that Vanishing men why are you always trying to optimize uh okay let's let's let's let's leave that to the side couple comments one um so I teach uh I've an adjunct at Ruckers and I teach data science and I teach data science to master students who want to get jobs um they literally like that is their focus when they come to me they are not thinking like I want to really deeply understand the atom Optimizer they're also not telling me like diffusion's pretty cool because I like pdes they're like I would like to get a better job than my current one that is like what drives them and there is one amazing thing about having that kind of student they know what they want that was never something that I had and I have so much respect for them for knowing what they want and when I when I try to like teach them the core thing that I am trying to focus on is problem framing data Framing and objective framing those are like the three lessons in my class and I like repeat this over and over ad nauseum um and like it's yeah so like all of those are the exact same like essential tools of an AI application develop V er um which I have recently been hared into calling this now um so like I think that feels identical to me as what I always try to teach in data science that that feels like a zero zero shift operator and so I I sometimes feel a little bit surprised at like how much people think that this is fundamentally different than data science um a lot of us in fact all of us gave talks in hamel's course with Dan um this really wonderful course that turned into a conference which turned into a meme I mean just a conference um but I I think I prepared my talk for that felt self-conscious because one of the first things on my slides was look at your data and I was like I feel really strongly about this I'm a little bit self-conscious I'm gonna get laughed out of the room people are GNA be like shut up like yeah okay and then the day that I presented three people had that explicitly on their slides and that wasn't the only day that that like phenomenon occurred like this was a recurring theme in this lecture or in these lectures and so I think there is a lot more um similarity to what makes a great data scientists uh then I think people give it credit for so I feel unscaled by the notion that AI application development is simply data science can I ask a question or is it gonna derail too much go for it so we had some we had some debate about AI engineer and you've been hered into application developer can you like but you have a you have posted the AI engineer job and you have successfully hired Engineers can you talk a little bit about the process that you use that makes it successful like just I want to actually hear about it like yeah more so so um very unironically um I uh about a year and a half ago I started hunting for AI application developers which I was calling AI Engineers um this was before the famous blog post was written um I genuinely just came up with the term AI engineer on my own um and I thought of it because I wanted to encapsulate a couple ideas the first idea was that they were going to be working on like applied llms at that time people had already started calling that a I what I didn't want to do is put like applied llm engineer because I was worried it might give people the impression of like sort of like model development like model like uh tuning or deploying and I knew that we wouldn't be deploying the models or like even like fine-tuning our end models initially so I wanted to kind of steer away from that the second reason I went with AI engineer is because I was thinking like what's a good analogy for the skill set that they need to have it needs to be related to data engineering because they need to be like manufacturing pipelines of data that like process and clean and it also needs to be a little bit related to like ml engineering what's ml engineering about well it's mostly about like processing data and like building evaluations and in March of 2023 what made sense in my brain was that if it was sort of like about ml engineering and about data engineering but it was in this like applied l space that AI engineering felt like a reasonable title I'll just go with that what's the worst that could happen no one could ever like make a a conference or anything around that um so like I just kind of rolled with it um one thing that I'll I'll share is like in my job posting uh there's a little disclaimer and uh treya please cover your ears um I said no researchers please and like I I I really did that's really in my job posting and it's not and I say like I really respect the work that researchers do I really like care about that work it's really important but that's not our job and so like I I wanted that to be really clear because one of the things that I've felt really guilty about is when I've had people apply to other jobs I've posted and they've been wonderful people and they're applying to a job that I literally don't have that always makes me feel really sad and so I want to be really explicit hey we're not going to be like trying to improve like flash attention that's not this job like trust me um and so like wanted to make that clear the other thing that I wanted to make clear in this is that they should be like aware that they're going to have to make contributions to a production stack so I put typescript in the job description I did not put you must know typescript and I also did not put you must know python what I said was you must be good at one of these two things and you must be willing to engage with the other because in my opinion I'm only hiring people to like do work that eventually will lead to product and so they need to be able to look at the production stack and the production code and be able to make minor changes infer what's going on there have opinions about how that is built they don't have to necessarily be like doing a bunch of back end or front end coding but they have to be able to interact with it and on the flip side if they were really strong on the backend side that's great but they need to be willing to engage with what I thought was going to happen somewhat in the python environment because after all we're a notebooks company um so that was my mental model and I stand by that job description and I reposted it three months ago so when you read that blog post the AI engineer blog post how did you feel would you if you were to write a blog post would you tweak anything about it yourself or like change like how you frame yeah the one thing that has always stood out to me about that framing that has not resonated is I don't think a lot of the good work is simply like interacting with the model I think a lot of the good work is really digesting the data and that's the emphasis that I felt like was never there for me it was never a deep enough focus on like the types of things that like data scientists think about that's always been my criticism of it is it was a little bit too focused on anybody could do it that's not a gatekeeping thing I don't want a gatekeep I don't want to say like product managers you can't do AI my wife is a product manager who does AI we do it at home together it's fun um now it sounds like I'm making a euphemism but truly um like I I'm not interested um but like I I'm really excited for a lot more people to be involved in the like application development with artificial intelligence or language models but like when I'm trying to hire top Professionals for my team which I'm very privileged that I get the opportunity to do I need those people to have really strong priors about looking at data making inferences based on what they see and what to do next what I want to know is does anyone disagree with Brian I will I'll take a slightly counter group position here which is that the user experience around models like this the user experience of the application is very critical and the you can't really do it in Python like work at modal it's like it's for serving python stuff I have built full stack applications on it using gradio and streamlet as front ends and even as somebody who is like python like you know python is my primary language I'm like I feel frustrated using that stack when I know how much nicer it is like to do things in in reactor felt and so I maybe the piece that feels a little bit missing here is with other types of infrastructure like database like messaging services Etc there is a clean API divide where there's like you should like a full stack JavaScript developers should know SQL like most people seem to agree that with that but they but they don't have to know about operator planning like they don't have to know about like they certainly don't need to know about like Cindy Vector instructions for olap like and they could they could ignore all that and learn like a Post-It note version like use duct DB if it's a big query like and sqlite if it's small and so I feel like what maybe the AI engineer blog post is pointing towards or what people are trying to imagine is like a world in which there is some kind of a divide that splits up responsibilities because otherwise you will need these like full stack unicorns who can follow a problem all the way from a like a a you know a 400 in the front end or a angry user all the way back to like you know floating Point numbers in a GPU great so I am we've talked about all the wonderful collaborative efforts that you you've done together I am interested it seems like you all are so aligned on so many things what I do want to know is were there during the writing process were there any key areas where you found yourself disagreeing or debating different perspectives or have you all come to a lot of the similar conclusions in in your work I mean sh and I just had one the must judge I think the other one is I strongly think that even right now I strongly think that fine tuning is smaller models is probably more cost effective more performant in the longer run not everyone agrees with that some people say you know I don't know all that other people trying yeah I'm not I'm not super bullish on fine tuning still I'm still like hesitant it's just like I mean you know uh I get a new model every two weeks like how much faster could I possibly need one like I'm I'm sure I'm sure son it 35 is good enough right well and to that point as well I mean haml who may be the king of fine tuning um in this group at least um the AI heard um oh no you know Hamill is AI king of fine tuning it like he may be one of the people who's as bearish as possible on fine tuning as well in the largest game of things that doesn't mean he won't I mean you can tell us more about this haml but if fun is necessary you'll do it but by virtue of that and as the king you understand the limitations of your of your Empire and your moat right yeah I mean like look I'm not I'm not zealous Zealot of f no you're not a zealot you're a king we've we've agreed on that um yeah I mean I only do it when it makes sense and it makes sense in a it really it tends to make sense and like when the use case is fairly narrow you're like very specialized task then it makes like ton of sense and like the in the you know the thing that is trying you're trying to do is not moving there's not like immense drift um then like yeah I find that it makes a lot of sense but it's not just a technology thing it's also there's a people thing there's a business thing you know uh and all these other factors to consider that can derail a fine-tuning project so it's not like oh is it just like faster and cheaper it's like it's all the people involved like can can can you actually use a fine tune model in this environment and deploy it in this company that is you know that's the that that is like something that actually uh like is a key cons consideration for all technology but like that's the reason why you know like open Ai and and anthropic like apis off the shelf that's why uh they're so popular is because you don't have to deal with your own internal company's red tape absolutely and so I would say like even more so than like whatever the use case is and whatever the biggest barrier is like people most like a lot of times I I'm interested in I would like yes I would like to bring up the possibility for Brian and Hamel of a skill issue which is sure every two weeks one of the foundation model providers will put out a new model um but you can probably set up a fine-tuning run that finishes in an hour you might be able to run like a hyperparameter sweep at large scale that finishes in an hour like what fundamentally prevents us from having single minute exchange of models to steal from my new favorite Senpai shingo shingo and uh like homl um you also mentioned I think um that there are many things that can derail a fine tune fine-tuning project sure but if your like fine-tuning apparatus is like fast and automated supported by a highquality evaluation framework then like it should be like as straightforward as releas like I have a client like actually EML from reat and the yesterday he got access to GPT 4. uh GPT 40 fine-tuning like Alpha access and he's able to deploy a fine-tuned version of it within an hour um and that is a case where that particular client of mine is insanely aligned they're like yeah this great we'll do it like let's do it whatever but I mean actually it's like a little bit I mean they're not we're not that's the easiest kind of like kind of fine tun you can think of we're not like just fine tuni open AI but it's a powerful Pipeline and also yeah we are starting to fine tune open models too because like once you have the pipeline you can go anywhere with it um but you know it doesn't work for everybody I would say like what's the difference between EML and some other person and people like if I look at the gradient of everything and it's not necessarily like the use case of Technology a lot of times the difference is like hey like okay you want to deploy a fine tune model in this such and such startup you're going to have to have like 100 meetings and get some BP approval and some like you know socks compliance and it's like okay just let's let's just like just stop so that's what I meant about people I also just want to give a shout out to I've put a link to modal.com in the in in in the chat um if you want to do fine tuning and get spun up with it really quickly with some really nice tutorials and documentation model is a really fun fun place to do it I don't know whether you created much of the you know Dev facing content Charles but it's it's it's really beautiful thank you I wrote I my fingers bled to get trt llm running the I've seen I've seen seg faults no human should ever lay eyes on and I did it for you I feel the love man trlm my ears are bleeding um like a torture yeah I also put yeah this is like what the AI King torture Vibe is on I think um I um I also put a link to I I was really grateful to be able to have a a live stream with EML and haml about everything Hamill was just discussing so I'll put a link to that as as well um I am interested um Eugene answered this briefly but I I would love to just go around and hear from everyone um what your favorite lesson in uh the report is and I just want to say schedule wise um so we've had the first third of the panel this is the second third with everyone and um the final third will be with Brian and and Charles um so Eugene haml and tryer feel free to leave at any point but if you want to stick around feel free to as well so there's no need to to leave but I know you all have you know duties and three hours is a long time so yeah favorite lessons let's let's go who's who's got one ready to go my favorite is no pmf before GPU other way around well it's the other way around I don't like that one actually no no I don't like it anymore I I prefer I prefer the other way what yeah sorry man I mean if you can raise $6 billion dollar from uh various Sovereign wealth funds then sure you don't need pmf for your chatbot U but I but for everybody else who needs to like tie um outcomes you know like tie input Capital to outcomes you know you probably don't want to take on that CeX or operational expense before you have like a some sort of business model that it supports I yeah I I rarely I rarely get embarrassed I rarely say I say a lot of dumb things and I rarely say something so dumb that I blush under my beard and that's what's happening now so clearly the lesson is no GPU gpus before pmf um but perhaps we can hear from you all as to your favorite lessons now I put my my foot in my mouth it's hard because they're all such bangers go ahead treya yeah I can go and then I'm gonna start cooking dinner um mine is the discourse between Rag and fine-tuning I don't like it I don't like other people's discourse on should I do rag or should I do fine tuning or like oh we have a million context length like you Long Live whatever I think it's all stupid we have a whole section on it um what I particularly like is the emphasis to really put what's useful and relevant in your prompt and not anything and everything just because you can I know Brian has spoken extensively about this and in general like people are not optimizing for what's relevant in the prompt um and it does boost model performance a lot more yeah somehow we made Ram too cheap and now it takes 8 gigabytes to have a Firefox Tab and you know let's not do that let's not do that with rag you know just because you can cash a million context tokens doesn't mean you should yeah and one thing people don't talk about is when you're debugging your system are you going to read your 1 million tokens and your prompts to to figure out what's going on I don't know that's a great question then that then Le leads into no you're not debugging your system like which is very often the case except for the legends on this call I see Brian's like looking at me very skeptical but I'm like you guys I always like remind Brian and Charles everyone I'm like you're like the one of the few people that are doing things like correctly like in this world and not only correctly but at a very high level of Excellence that you just like don't you don't know you like maybe have lost touch with that you're at such a high level of Excellence H's always like people are oh sorry God I saying Brian's gonna pull out his he's pulling his hex notebook look guys yeah exact half a million contexts no problem no I'm totally aligned with SH I want to use my hex notebooks to like deep dive into this stuff not shake my head at a million tokens you don't want your you don't want to put your whole sqlite database in your prompt I've I you know I benchmarked it I really have I promise you like I benchmarked it um also fun fact here's the thing that people underestimate about just Jan the squel schema into the prompt people are like oh that's that's that's a lot of tokens but like context windows are big and I'm like how many columns you think people have in their schema and people are like I like a 100 I'm like order magnitude two orders of magnitude higher people have 10,000 columns in their schema I can't get into specifics but that's not the biggest one in terms of orders of magnitude that we've seen people have no idea like the kind of like actual like scope of these things that you want to do anyway that's a tangent but well it's a good T it also reminded me of one of my other favorite lessons besides the one I messed up actually is look at computes getting more expensive no exactly no look at look at your prompts particularly if you're doing if you're putting a lot of stuff in prompts from rag or whatever it is actually have a have a look at what your prompts actually look like and you'll see all types of really wacky stuff right oh my God like so one day I was like just a thought occurred to me like there's all these tools that we're using right um and I was like you know what I can't I like looking through the documentation I'm like how does these things work because I know that there's an API called open AI somewhere that's happening somewhere in this and I like started looking at the code base I'm like where does this API call happen like what is I just want to know what the prompt is and then finally like I um I was like you know what I'm going to do a Mana in the middle of attack so I'm gonna put a do have like a a proxy on my laptop and I'm gonna use like a bunch of these tools I'm going to intercept like what the API call is and like I was like I wonder I'm like I'm just curious like I'm just curious like what is going on and I was like shocked by some of the things that I saw I saw some dirty [ __ ] going on in some of these libraries that I was like Wow and then that's why I wrote the blog post and that's how the AI King MKBHD of AI wrote duu show me the propt exactly um and I love that that's how he wrote his review the way the way the king just framed it was like oh yeah I was like I would like to see this prompt whereas the blog post isn't oh yes please uh show me the prompt it's duck you show me the prompt and I just put it in the in in the chat as as well um awesome I um I suppose the other thing we've used the term evil so much oh there are two thing two places I want to go um we haven't used I I think I was the first person to use the term rag in nearly two hours like a minute ago which is incredible but seeing we have some you know we don't have Jason here but we've got some real ragheads on the call um and rexis legends so I would I wish Jason was here for this but Brian perhaps Eugene anyone else wishing to chime in I know a lot of you kind of like to think about rag as rexus in Disguise so I'm wondering if you could tell me a bit more about about that yeah um so yeah uh this is a phrase that I coined like last June um just being straight up um and that is because uh ultimately what you're trying to do in a rag system is provide the most relevant content for the agent you're making a recommendation to the agent of what might be helpful so um I don't think it's like actually that big brained of an idea I know that it's gotten like varying levels of reaction but I ultimately think it's like straightforward to the point where like a lot of people feel this way I just happen to say it on a podcast um but the reality is like where I think the value comes from is not the like memetics of it I think the value comes from the like type of evaluation that you can do on a rag system if you remember that you're making recommendations to the agent you can ask questions like does the order of my recommendations matter you can ask questions like how like how often do I even get the right information for the agent to have a chance if you're trying to write SQL queries based on particular like schema but you haven't shown that schema to the agent I can give you a hint as to how often it's going to get it right that's it and so you can answer questions like well what's my eval rate when I don't give the right table uh close to zero what's my eval rate when I give it the right table hopefully a lot higher and so then you have two problems and actually breaking them into those little problems is easier so I think where I'll start is just like yeah the the point of that like uh head Cannon is to provide you more opportunities to dive deeper into your problem um I think where people have gotten maybe a little too excited on this is they are jumping straight to a lot of things like rankers and they're jumping straight to things like okay now we're going to build an entire like ecosystem around like like just trying to tune Rag and I find myself now hearing about people training rankers uh very early on and I find myself thinking like have you actually tested how much ranking matters for your I would say like agent BAS pipeline um one fundamentally fundamental difference between a rag system and a rexus system is that if I am talking to Charles and Charles was saying I'm going to the beach I need a book recommendation I can give him three recommendations if I give him more than three it's static there's no chance in hell he's gonna remember more than three and that's not a statement about Charles that's just the way we are with recommendations agents they have longer context window this three is not that much now let's be like reasonable and let's not give them a 100,000 but like I can give an agent 15 recommendations so I do think that like topk accuracy is a very different situation in agent recommendation and so I I I find myself in this like weird combative stance where the people that don't think of it like recommendation systems at all I'm like half you're wrong and the people that are like it's just a recommendation system I'm like I'm sorry you're also wrong um and I find that it does require that nuance and that familiarity with the field um I am very fortunate that this particular group has to like OG rexus people um like and like both Eugene and uh Jason are also like very experienced and rexus and so this group we didn't have a lot of like clashing but um yeah that's kind of my like feelings on the whole rag rexist dichotomy e does that resonate or do you feel like yeah it does I think I mean I'll just end this is going to be my last comment before I drop out I think like bri complely uh he just said everything that you need to know about ra and I think Hugo actually before he mentioned this he actually said this is the first time we actually mentioning rag in his podcast so actually a lot of people ask me this question like they asked me like I chat with them they like Eugene what's your thought about rag like you have why why are you not talking about us oh rag it's like I don't have a prompt I I have never run a single query that doesn't involve rag it's just a way of life I mean like you want to be feeding your query with the best possible data that you have be it your company policy your previous chat your database schema your product reviews Etc I mean it's just a way of life I mean like yes we just got to do it now um don't need talk about it you just everything should just be retr the generation now then I talk about spoke about how you actually want to evaluate it well I think by doing rment generation it's just easy to evaluate the retrieval I think it's just easier than evaluating generation so that's my two cents all right peace out that's definitely more than two cents and appreciate your your time and wisdom and and spirit Eugene so see you on the other side I've got one more question for Hamill um so do you want to do you want to come and say hi haml um so because a lot of the things we've been talking around are the importance of data and evals data on one side evils on the other but actually being part and parle of the ways we need need to think and being very um closely aligned I want you to tell us a story it's story it's it's Story Time with the king um and the story I want you to tell it figures slightly in the report um but it's the story of you working with honeycomb and working with Phillip and how you got him to be a judge got an llm to be a judge and then had some like meta Inception of llm and human judges um going up what your process was there and maybe start by just telling us a bit about you know the text to sequel was text to Honey con query language sorry yeah okay let me see if I can tell the story so um honeycomb is a observability platform where you can log you can log traces to and traces are just sequences of events and it's used a lot in the devops community um especially think especially in distributed systems like kubernetes where you have lots of different events and you want to correlate them um so this database is has a domain specific query language called honeycom query language and the problem with the honeycom query language is no one knows it in so it's a big onboarding problem so honeycomb said okay um we have ai maybe we can just do natural language to honeycom query so they started with that um and you know one thing that they desired to build was like hey can we have a private version of this model because like not all customers can like send their data to open AI um and so you know is it possible to kind of have a private model um um and so like when I began the project sort of so okay it was like basically like can I find tune the model like that's kind of the modus operandi of like you know it had to be small it had to be really performant it had to kind of to make it like a to make it really appealing I wanted to see like how much I could push it like can I have model that's like really small can it be just as performant as open AI on this task so on and so forth and so um yeah the first thing I did is like got the data so like this is a very classic data science mistake um I was like Hey like do you have data that has good examples and my client's like yeah I have like curated this data very carefully and you know I went through it and these are some like really good examples here's a thousand examples I'm like great I have this like a it's like a kaggle competition just like you have the data and I'm going to find tun it's like model I'm like okay but he told like he told me the data is good so like let me just let me just go with it let me trust that this data is good and then I like a fine- tunea model and like you know deploy it and then you know uh have him T I was like can you just eyeball it test it out I was like oh you know these queries are actually like not that good kind of mediocre kind of like what from what I expect I'm like huh okay can you give me some examples gives me some examples and then I go and then I like look in the data that he gave me um like very similar queries and very like very similar similar like natural language questions and very similar queries I'm like do you see this like this is almost the same thing in your data that this thing that you're he's like oh yeah yeah the data yeah the data sucks okay so in a in a classical machine learning problem you would just get stuck there you would be like you know what that's it like move on like I give up like where I have to now like go get data let me like it's going to be a long process I have to like find the data from somewhere I have to under deeply understand this domain and I have to like go do this like long expensive process um and so the magic of large language model is like okay let's wait let's hold on a second so I need to know whether each of these data are good or not and um you know like I don't have all the knowledge my client has as a domain expert I wish like I had his name is Philip I wish I had philli in a box like some artificial version of Phillip that could just query right so you know where this going so so I hope I know where it's going you're not putting Philip in a box actually yeah if you're not if you're not in the gutter then you know where it's going so but like basically uh so you know Phillip I said hey Philip like can you label some examples like hey I think I need like you know like whatever few hundred examples to start with it's like you know I don't have time for that I have to like read each query I'm like I need to know I had to have a critique of like each query why it's bad so then I can like understand like what the hell is going on so he's like okay I don't have time for this like it's going to take up all my time to like write these detailed critiques I'm like okay that's pretty fair that's pretty fair so what I did is I said okay let me kind of I prepared a spreadsheet I said like every day I'm going to email you 20 examples and all you have to do you could just do 20 it's not going to like destroy your life and basically had him label 20 queries and write critiques then on each iteration I would make the llm as a judge align more with philli based upon what his critique was just through prompt engineering eventually I got the LM as a judge to write critiques that he that he agreed with at a very high rate enough to where I could just basically I had fill up in a box at that point and then and then basically I used that to make all the data better because I could have the LM as a judge write the critique respond to the critique make the data better and then also like generate massive amounts of synthetic data helped me generate massive amounts of synthetic data and do so uh in all that data was now like much higher quality and then we're able to find tuna model and then we're able to get really good results so it's like really interesting about getting unblocked on data it's like a that's like one thing that's like super exciting with llms is you can get unblocked with data I think Brian has some examples of how he's like generated entire worlds with dat I'd love to hear about that right now someone has started the hashtag um hash free philli which I think is important in the chat I like that yeah but but thank you for for sharing that and I think one thing that I I love about that story is that like it it really demonstrates nicely the importance of looking at data as a practice like Zen In The Art of looking at data or something something like that right that it's not a one-off thing you do it's it's like drinking water or or or breathing or doing ta Chi if you do before breakfast or whatever it is right so I having said all that I would love to hear from you Brian about building synthetic worlds um I can't get into all the details but I can say a couple things um one thing that I definitely like we do a fair bit of is like human labeled data here at hex um we we I call it homework um I joke that we have homework uh and everyone on the team gets a interactive hex application where they can see whatever it is that we need more signal on from humans whether it are it is Generations from the model that we're saying like good generation bad generation whether it's two generations and saying is one better or the other whether it is a classification problem that we're asking the agent to start doing for us and doing manual classification um we have a couple types of like hard classification tasks that have you know seven classes and we will like use humans to bootstrap that and just be clear like the humans are people on my team these are not like random people that we find like I do need Philip's phone number so I can let him know he has some homework but um but you know besides Philip they're all people on my team um so I do think that like this is very much part of our our process 100% And um I think one of the things that like H's alluding to here is once you've got these sort of like initial bootstrap data sets you can use that to calibrate or align some people call it your llm as a judge or your llm evaluation like he was talking about and that it feels like a souped up bootstrapping process from the olden days and statistics and it is it is quite powerful I will I will just say like it has been a very valuable process the other thing that you you can do is you can make synthetic data sets where the inspiration for the initial data set is real and human labeled and then you just expand on that Corpus more and more and more and more and more so um one thing that you might realize is our evaluations for SQL they need to query databases so one thing that would be really nice to be able to do is say what are some particular like um shapes of data that we're struggling with we want more and more examples of trying to solve problems in that ecosystem Well turns out like synthetic data generators is something that LMS are quite good at and so being able to sort of go from a vague statement to a data set that's custom built for that particular shape and that particular like um pathology is actually Something That We're capable of doing and in fact we do and so it allows us to really go from originally like 25 examples of something that's usually our lower bound all the way up to like human eval of 200 examples and then bootstrap that to uh recently 7,000 in One cases case so I think there's a lot of power in this process and it's it's fun because like El and I haven't talked about like this but we kind of co-evolved into this like process of use a human learn from the human Benchmark a little bit bootstrap a little bit and then and get that to a state and then use a human to kind of like pull it back into alignment and then bootstrap from there and yeah uh I've had to do like evaluation and other kind of models and over the years for ML and I've never felt so empowered to build phenomenal like evaluation Suites as I do today awesome um something I love about that response in a lot of the conversation we've had and the report as well actually is that we're really focusing on processes and method IES rather than than tools as well besides the Glorious notebook um but I'm wondering Charles from from your experience and Vantage Point what are the in terms of think about methodologies and processes what are some of the key kind of process level considerations that are most critical for ensuring success um when building llm um I think the ability to bring back information from production seems like the most critical piece like going all the way back to the before times of people training their own neural networks before the Fire Nation attacked in uh uh with the release of foundation models and like that there it's like it's non-trivial lift to set that up like you like production data is treated very like you frequently have like an iron wall in between your ml teams and your production teams and like you have like they have like a simple handoff of an artifact and then like there's not communication back so that like some like honeycomb is actually a great system for this like it's a generic system for this kind of like event tracking um but and that's kind of what you want to do there are more specialized tools like the the like llm Ops tools are oriented to solving this problem uh like getting information from production like from the production behavor of your LM systems um I think the what makes that useful is then also like an experimentation oriented workflow um so the ability to like try stuff out very quickly and easily um that you know evals evals should be like easy to run straightforward like it should not be something where it takes like it should be like your your test where you like run a single command line command with minimal configuration and it runs your evals and like you can drill down and pick a specific one or you can adjust like your flaky like your flakiness setting or whatever but you you it's like relatively straightforward um and the ability to like yeah quickly run those things and iterate on them track what you were doing as you do that like that feels like the other like critical piece both from like a cultural perspective of like caring about thinking about and supporting experimentation and then for the like uh like yeah the tooling that supports that so with respect to the tooling I actually I think I asked Brian to prepare a question for you and I think that leads really nicely into his question Brian do you do you have the question the hand yeah it's basically like in this llm ops world like what do you think is is like required like what do you think we really do have to buy what do we think we have to like not just build and bootstrap from gum and shoelaces yeah that's a good question I think there's yeah there's maybe two components like a back end and a front end and I think it's probably hard to build both of them yourself unless you have a lot of resources so by back end I mean like the ability to collect this information from uh like from production or from annotation teams uh like that it sounds like you have a really great system for that from everything that you've said in your talks about like what you do at hex it sounds like you like maybe unsurprising for like such a data Centric organization but it seems like you have a really good way to do that I don't know that everybody can build that themselves um uh and then on the other on the other side you have the like front end which is how do you like interact with this information how do you like discover like how do you discover the patterns in this data I think you're like that one that one you have to buy and that's hex and it's the only way oh oh hex how's that spelled h e cks is that or uh yeah that's what we say internally sometimes but not like that uh H ex I think is what you but you're shopping for he. Te if I was I was I was actually using that platform recently to analyze some data and I there was this nice AI co-pilot that wrote several queries for me which was very nice um but then goodness uh yeah but then my computer ground to a halt because it consumed eight gigabytes of RAM which was all the RAM on the machine but boy did I get some data science insights out of that cache I'll tell you that much um but um no trick is buy a buy a better computer is actually is you know Ram issue um but yeah so yeah so I I I think you actually really hon something with the idea that it's like a notebook environment like when I was at weights and biases I was when we were weights and biases you may recall that I was like like a very big proponent of the flexibility of the weave like query language and weave platform it was like this is exactly what I need to like I send a logging service like high cardinality like semi structured like everything I could possibly need to figure out the problems in this system and then this there's an inter interface for me to discover the information in there I think you can dissociate the front end and the back end like you can have some like grafana Prometheus Hotel kind of thing pulling information in and then like you could consume that from a a front end that you build yourself or that you like I I expect people to build more L Ops tooling um that like consumes those sorts of things or you could like I think the people who are trying to be like llm Ops vendors are going straight for like let's put all of that in a single package and depending on the team like buying build like building one part of that for yourself might make more sense um for I think for others if they already need the lift of having to learn data science having like previously convinced themselves they just need to know software um then they probably shouldn't then like also build this platform that they don't know how to build um just like the resource requirements are really high yeah yeah yeah for what it's worth um just because you kind of like implicitly asked um we write a lot of our like events as either segment events or like traditional events to a database and we transform them with like five Tran into Snowflake and then we read directly from Snowflake just to kind of oh how do you read from Snowflake uh that one's with hex uh oh what do you know um so that's the notebook environment you're referring to indeed indeed so yeah I I I do think that like our stack is incredibly lat um we we have five trans separately um for the data team and you know mixed feelings there and we have segment um which is also for traditional like product analytics so yeah that makes sense yeah you need to Define your traces and like I guess the thing that I like about for example Lang Smith or like actually going way back only the real '90s kids will remember this but elicit light OH Close elicit had um like a tracing tool from like they tried to get like GPT early gbd3 models to work well for stuff and they were all about evals they were all about like finding like uh they were also all about like shrinking problems down so that they're easier to solve which is another thing that came up uh in our report um and Eugene big advocate of that um you decomposing problems you mentioned that as well um but they had a they had their interactive uh composition Explorer ice and it would just grab every async python call the Assumption was like any call that you did async is like probably important enough to log it's probably a call to an external service for information retrieval or a call to a language model so let's just Trace all of those so something like like the nice thing about that tool and Lang Smith which basically just like hooks into the Lang chain ecosystem and automatically grabs things is like it defines all of your like Trace events for you it defines youro expans for you and like that and it like puts it a little bit closer to the python programmer so if you have like it's like a python programmer who's the Persona who's defining your chains and flows um you like there's less lift and there isn't the impedance mismatch of them having to like hand something over to somebody who knows how to use something like segment which is like I associate more with like your full stack JavaScript persona but let me let me maybe push back on one side of this okay so like sure I hear you on the segment thing um I have a lot of mixed feelings on segment but let's come back to this Trace thing so like I love traces I've been beating the drum for traces since I was at stitchfix where we had light step and it super convenient when I was at weights and biases I had suggested that we' have a trace Vier actually that weights and biases have a trace viewer as like a core feature for a long time um you and I were like very aligned here um I did like physically demand a pytorch trace viewer um I remember and I've seen you yes that was wonderful um but but there's a lot of great reasons that when you're building these kinds of applications that you need to write events to a database and those events can be used for control FL and what I claim is that there's not a lot of difference between the events that you write for control flow and the events that you need in your Trace log anyway where it can be a little bit like funny is if you're using microservice architecture because then you have like spans across different services but they all have Trace IDs anyway so like I guess what my only like push back here and why we're not currently like a uh customer Lang Smith which I think is a cool like product is because like we we have like Trace IDs on all these events we use these events for control flow and so what I end up finding out is that like okay all I've got is all my spans tied together by Trace IDs it's like five minutes and heck to make a DAT visualization like giv me my Trace Vier anyway so like I just don't see the extra like value in a real Trace fewer and you might say well cool do you are you not a customer of data dog then and the answer is like no no let's like we are a customer dat do so like I mean I know that there's a limit here and maybe it's a scaling thing and maybe it's a like you know um a fidelity thing but like I don't know I think most of the kind of things that you want to trace for llm Stacks is damn similar to the the control flow okay fair enough I guess are you saying there is not a trace viewer visualization in HEX notebooks that I can turn on where I there is there is oh good good good I mean it's python you just build yourself oh I mean yeah I guess if I that's what I did you just yeah yeah you got you got that Tony Stark in a cave with some rocks kind of energy build it yeah Fair um yeah I'm lacking some of the like um Charisma or Riz sorry the Riz as they say but other than that yeah um brutish and uh somewhat evil yeah yeah yeah okay wait maybe one last point on this is that one of the things that I found like surprising and interesting about the product Direction of langing Smith is the orientation to enrichment where it's like you can either trigger enrichment like semi-automatically but that involves humans like going to an annotation flow um or like humans can just go in there and just like you can type a little note and be like this one looks sus or like and that is in the same database that is storing all of your like Trace information you add these kind of like semi structur like Json keys and tags and metadata what I like about that is it like the thing that Lang Smith is trying to build is like pretty close to what we're describing as the place where value accumulates in like llm applications in our report which is like it's got information from production it can be turned into evaluation sets and it brings in like product information and is accessible to like like a product manager um and to the like technical team delivering stuff uh so like yeah maybe to bring up a point that while you and I were seemingly out of the room some of the other uh like people like HL were making uh it's like we would if we were to build this ourselves build that but not everybody would they would maybe build something that doesn't include that doesn't actually serve as like the full store of value um and and yeah so there's utility in like buying the right thing when you might build the wrong thing yeah and on that one like again now I feel like I'm like I'm you know I'm in my cave and every everything is peaceful and warm and my cave but like you're like oh like I want to pull down all these traces and I want to like manually inspect them I want to dive into them and at a certain moment I want to like add back some data and I'm like cute I pull that into my my hex notebook I do my deep dive and then I use my right back cell and I literally write it back to snowflake it's in my same ass data warehouse and like I I know I know like the modern data stack is dead like you know rip um but like I I do think that this is one of the things that we loved about the modern data stack is like it's all in the data warehouse like where is it it's in there you can find it it's in there and so I don't know I like I said I do feel like a little bit biased but yeah that's my rejoiner I guess got it well you know I've only been using hex for about two weeks so give me some time to get hex pilled fully okay okay yeah um we usually estimate like 19 days so I'll see you in a couple got it so you've seen the hashtag hashtag heex pill as well Charles I presume oh yeah I mean yeah we we all know what Brian's alt is on Twitter so in I know at least three of them I um so I I love that you referred to Brian as Tony Stark in the caves banging rocks together or something like that because Eugene has said in the chat prior to you saying that um you can tell Charles is getting angry when his Mohawk starts to glow and his face turns green like the Hulk so so we're really bringing some you know a more Avengers energy to this um wow that was so Avengers dude dude dude oh wow you have done it this you win you just won um ding we may have already kind of covered this but in the interest of symmetry you did prepare a question for Brian as well and I would like you to ask him that you've already been talking around Lang that's true I've effectively asked it already but I'll ask it literally um hey Brian I really like Langs Smith and I have used it to debug llm applications and I have to admit that I have recommended it to people are you mad at me yes no maybe absolutely not here's why one um I think that team is really great and I think that they've got good intuitions um when a team has good intuitions they're certainly going to be building things that like have potential for Value what I've seen of the product so far I think is incredibly aligned with kind of like what I want if I wasn't in the K with my like you know uh crappy iron that I have to like Forge into a ugly suit um like would I be entertaining something like Langs Smith maybe I think I'm certainly not mad and I'm certainly not even like skeptical that it's like wrong for everybody or anything like that I think I feel very passionate that like the time to buy tools is when you have like identified the Delta between what you can get to in a short amount of time with your resources and what like can really make an impact on your product and your like team velocity so like if I look over and Lang Smith like that workflow is going to accelerate my team's ability to like act on user interactions oh yes like it's time but not until I tend to be a little bit of a like build it and then buy it when it really becomes like that transition time um so I just think that like for me it hasn't been that time yet um I talked to Harrison about like would hex be a lsmith customer and I I find that like the production environment it doesn't feel like it hits for me but actually some of the like offline uh like workflow development and like the evaluation Loop that it can build actually some of that was really exciting to me um and it's not feel that weights and biases weave offline experimentation platform met your needs so so yeah so let me let me like officially do my disclaimers of my like um yeah uh my conflicts of interest um uh one I have a conflict of interest that um Harrison is a very nice guy and very smart um to uh I am an ex-employee of weights and biases and I am very aware of like the smart and like knowledge of that team um so like those are my like uh disclaimers uh but I think what's interesting about weave is it feels very like notebook focused and that really really appeals to me what's also interesting about weave is it interfaces directly with the rest of the weights and biases ecosystem which um like I mean let's be real that's the [ __ ] best for experiment shacking come on like I have no conflict of interest obviously and I'm not also a weights and bias's ex employee so I would I would agree I've never met this man before in my life right um but like genuinely like I I think for experiment tracking like let's not even have this conversation it's useless but that relationship between like the experiment tracking for llms and like how much are llm workflows experiments is a really deep and nuanced thing there I think that interface is really exciting and interesting and so what I love about like the weave approach is precisely like how it links back to the rest of your experimentation flow we're doing like we're doing experiment Flows In in HEX and we're tracking them very similarly um so like if you ask me like what's more important here like Langs Smith or or like weave for the IR and experimentation um yeah I think if I was more of an expert on both I could give you like a very like blunt answer but from from my outside take I think I kind of do lean towards weave sorry um but I yeah um but I think on the production Flow side like I don't really care how you like build your Trace viewer I don't really care like how you get that as long as you're integrating with it and interacting with it uh that's really that's really wonderful um uh whether you're like you know whether you're using Brain Trust or Zeno or petronis or Lang Smith or log 10 or or human Loop or uh uh prom layer or Auto blocks or these are all my beautiful children and I by them all I am a customer of all of them and they are important parts of my stack um please invite me to your AI dinners and just to be clear this podcast is not sponsored by anyone this is a labor of love of me and all of us and none of those vendors have sponsored this um I do it's not sponsored by any of the Venture capitalists you know in the same way that the city of San Francisco is not sponsored by The Venture capitalists you know it's like um or in the same way that the room is not sponsored by oxygen and I totally yeah we're totally on the same page so I do I do want to step back a bit because there are several other moving parts of the report that I really like and I'd like your your thoughts on um stepping back from the more technical side of things you all discuss the importance of building trust with stakeholders and users when deploying um AI software or LM llm powerered applications what strategies or best practices have you found effective for establishing and maintaining trust um yeah get people in the room I think that's one like one point that I liked was like yeah get designers and ux people in the room from the beginning I think we a lot of the implicit position of the report is for applications where the developers are sufficient domain experts to confidently grade llm outputs um but for you know medicine or law or other like very specialized domains um that might not be the case and that so like getting those people in allows you to find the like shaletts or the like very easy loow hang fruit that like you know if you violate that you will lose their trust um and like yeah to sort of cdesign with them I think yeah there's some good stuff from Carrie Kai who does a lot of great um like human computer interaction work at like Google and with Folks at Stanford um that sort of talks about a lot of techniques for sort of like bringing these people in involving them in um in application development um yeah and I guess maybe you know rolling stuff out slowly so that you can kind of like I think frequently like issues or problems are discovered sort of after several iterations of interaction and so if you like roll something out to a small group somebody in that small group will interact at least 10 times with the system and then discover this bad pattern so there's like more value to that like repeated measures interaction than to going to 10 times as many users at the beginning and so that allows you like that gives you like a a a benefit of operating at that smaller scale at first um and to like correct issues before like you tell people to eat pizza with uh glue on it um like you only tell like one person who's your friend to eat pizza with glue on it and then they like help you fix it instead of dunking on you on social media absolutely Brian is there anything you'd like to add to that I think the antidote for demoitis is user feedback like last summer um I went to as many of the like little AI meetups as I possibly could go to and my motivation was not like to eat like really crappy finger food um it was actually to like just like quickly put like turn my laptop around and like do one thing with magic and see how people reacted um and like this was like while we had an active beta going on but just like seeing people's reactions to different things was super super interesting I had people who like immediately were like I don't know what's happening and I was like okay great that's lovely to hear um okay and then I had people that were very like you know immediately like can I do this can I do this and this was while we were sort of like building a new product in secret and so I couldn't show the new product but I was just like gaining feedback about the thing that we already had in beta and seeing how people were like thinking about what it was doing doing um I learn a absolute ridiculous amount from every bit of feedback that I get and so I I I I don't know I'm I'm terrified by people that are spending two years in stealth before launching anything and I think like as much as we're all tired of the like AI hype demo what I am very excited for is the AI hype demo as a beta like sign up list that they start rolling it out to users like that's a whole different game um yeah absolutely and something we've been talking around also is Building Systems focusing on systems and and not models and so Brian I know you have a lot of experience building end to end systems with llm so I'm wondering if you could share an example or ideas of examples where a systems level View was particularly important for achieving the goals of the project yeah I mean you know I I started at hex the last day of February 2023 and um at the time there was one engineer on my team and he flew to California like I think it was two weeks later and we went to the Whiteboard and Drew out our like prompt templating architecture our evaluation architecture and our like context construction and rag architecture um that was like what we did with that week and the rag architecture is still in prod with a lot of changes to like certain aspects of it but like the fundamental architecture the prompted architecture hasn't changed almost at all um including the like appendix thing that we have which is like we inserted this thing into our system called a conformer layer and we were like this is going to be really important and like it's very much an appendix in the sense of like it has it has a purpose um but like it sure is easy to misunderstand why um and then like uh our eval architecture like that lasted us nine months and then we ultimately did have to rewrite that um but like seeing ml systems meant that like two weeks into this job I was like okay evals are important like like I know that evals are important um building something that's like composability first for our like context construction is important and then thinking about like our prompt construction like flow as like essentially like metaprogramming like that was those were the three things that I was just like cool stealing this from my previous experience and then of course the rag is rexist thing like yeah that's it and those were all things that I just pulled from like textbook ml if you go and you pull down the the the book um machine learning design patterns like everything that I said in my first like month at hex was like could have pulled it straight from that book um shout out to the authors of that wonderful book it's still my all-time favorite O'Reilly book um yeah hopefully not your all-time favorite O'Reilly document though uh I ple that um so Brian Charles Brian one of the things Brian mentioned was composability and last time we spoke this is something you and I chatted chatted quite a bit about and oh do you have opinions on this Charles oh do I have the monad laws on a chalkboard in my room in the back of this shot oh yeah oh I think I I think I actually What's um so Brian if you could indicate in which kley category your contexts compose that would help me understand your previous claim regarding compositionality we used to we used to call it cley is it kley uh I would I went with like a German in pronunciation but that's probably wrong no I'm not actually sure it feel like it feels like you're probably stonewalling because you don't know in which monad your contexts compose uh no it's it's a totally valid it's a totally valid criticism I'm not sure if I usually like just stick to the monads I mean let's see all I really care about is that the use arrows well okay let's see I care about the types of the interfaces and I care about composition from the purely like there is an arrow in the like Union category so that's a yeah that is a stronger monad than like the weakest one so for people not versed in monads and category Theory ask who isn't sorry wait what audience do you appeal to oh dude you know I eat spectral sequences for breakfast but it's I don't they're [ __ ] hard they're horrifying actually um all I guess maybe the thing I would dial in on what Brian said is like so you mentioned unions which is sort of like splits of things or pairs of things where you have like one or the other you know either um in in hascal or um like left right so um so you're saying your notion of context composing is like there's lots of forking paths as opposed to say like concatenation is a form of composition right that's like that's a more producty or intersection approach right right yeah I I am not just concatenating I am sort of saying like um like I have to do some currying at sometimes and I have to like yeah like I have to yeah this is this is a little bit more got it got it sounds like a bartesian complete closed category we we should uh we should wideboard how are these things applicable to the the world of llms more more generally I'm so sorry for having to ask this question as well I feel rude thing go into thing you're going to do metaprogramming you should have a nice little nice little programming language Theory you know um might as well um did you say that we were overeducated at some point you go is that what I think I heard we do have only phds in the chat now I think on on the call at this time yeah and it shows that's what's up that's that's what's up and especially like three people who are I I definitely have never felt well versed in category Theory um because the people who are well versed uh pretty epic but three people who at least can talk category theory is I mean somebody else needs to to join essentially so I do want to go back to we we did talk a bit about we have been talking around data literacy AI literacy education um Charles I'd love your thoughts on on this first just what we've talked about being able to look at data and evils I'm just wondering what some of the key skills or conceptual understandings that you believe everyone working with llm should prioritize developing yeah I think um the primary conceptual Gap that I see between software engineers and data scientists or between people who have no data literacy and have data literacy is like the tendency to care about like the semantic content of btes like when I'm inter when I work with software Engineers you know front end Engineers who are like playing with a form they will like key smash to enter all the fields in the form and when like working with like platform like more systems platform engineer type people they'll frequently have these like mock bite sequences that are just like random bite sequences or whatever and it like bespeaks an orientation to the computer system that's like oh the content of these btes don't matter they just like they have the right type and therefore like the system will run correctly if I've like yeah and that's like precisely the opposite of what you need in data like what we do with models is we learn a probability distribution over the like data domain and so what makes like what makes our model good or bad is not the like types of its inputs but the like the the relationships to the specific values in that type and so that it's like a different orientation to sort of be like maybe like an extra you know an extra you know capitalization versus not capitalizing slightly different framing of a question whatever will change the behavior of this system it's um or like the way that you examine data is not by like checking to make sure that it was encoded correctly or decoded correctly and there aren't like stray Escape sequences you do still to do things like that but you also need to be worrying about like does this accurately represent the problem domain that I am trying to solve these like does the synthetic data cover the like the part of the domain that I care about does it have the right proportion of these things and that is that it it's been surprising to me how challenge like how big of a perspective Gap that is from like for how big of a jump that is for traditional software Engineers sort of like yeah it took me a lot of effort to learn the software engineer perspective um but and I guess maybe I was surprised that the other way around was so hard one thing there that I think it might just be reframing in like a little bit simpler languages but like I have this thing where I can look at a distribution of data and if I know what the like data is supposedly produced by or like what generator has produced this data I can be like something's like and I I I can't always put my finger on it but it has happened a large number of times especially in the sort of back nine of my career like I think early on I didn't always have this intuition and sometimes people would be like something's wrong here Brian um but like these days when someone on my team says like hey I did what you suggested and here's the like similarity scores for the rag system and I'm like no it isn't no it isn't that's wrong and I don't know necessarily like that can't like I don't have the theorem at hand from the information theoretic perspective of like that can't possibly be the way the data is distributed but I know it's wrong and they go back and they dig into it and they're like uh sorry and it's okay but just like I don't know I find that really odd because I do have that intuition and I think I've also observed that it's something tied to like spending a ton of time making the same crappy matplot Libs yeah yeah um yeah funny yeah I had an interesting experience with that I demoed like a project I had only half finished this uh Vector analogy thing that could like find a Wikipedia article via analogies demo it like with I I don't were you there that time of the of the like Friday afternoon podcast no no yeah showed it Jason and Hamel were there maybe Eugene as well and um like they suggested a query I typed in I went did a search to try and find the article and it was like Bill Gates or something GitHub was another one and the result didn't come up and I was like oh I probably broke something in the search pipeline or whatever but H was like something is very messed up here and he was right turned out I dropped like 40% of the rows at the end of this big ingestion job just like it just stopped and I had not noticed um because it was like a hidden crash um yeah and it was like I was like oh man yeah like that yeah there's like a c there's like a a scent where it's like some something very serious has gone wrong or like that what gave it away to haml uh the smell man yeah he's all about it's like a mouth feel yeah yeah it was yeah I think I don't have as much experience with like your Tech your text search you know so I was I I think text search kind of sucks so I was like oh it's probably the text search sucks but homel believes in bm25 and lexical search so he was like no it can't be that hey sh what's up shre welcome back um so it will be time to wrap up shortly um if haml and or Eugene are around and want to come and say hi again please feel free to otherwise don't um but I I've got a couple of last questions for you all so as llms become more widely adopted integrated into products Services life um what are what are some of the things that you're just really excited about um technology wise or process-wise or conceptually or what do you want to be doing in the future um I can go first um robots man yeah robots tell us we all dreamed of building robots and then ended up building businesso business software as a service but inside of me is a small child who dreams of robots and I think more seriously like multimodal models solve this problem um pulit Agro wal at MIT has a great paper about this it's like the one of the core problems with robotics is not the mechanics of like you know keeping your servos um well calibrated it's not the like policy learning um it's like getting information from humans into the machine so that a human can say like please do not go in the corner the dog has pooped there like you know being able to communicate that to your Roomba and so I think that like fun that like there's much more work to do on multimodal models there's much more work to do on robotics but that fundamental Gap that felt that felt like a Quantum research leap that needed to be crossed and we have in fact done that jump uh and so that's what I'm most excited about I don't know I already said I don't predict the future so I won't say how long it will take but yeah long before I die awesome how about you sh I'm really excited for people who can use Excel to be able to do very cool things with llms over large collections of data I don't think I think we have building blocks we don't quite have the interfaces or even the tooling required for it and certainly these tools don't have Interactive latencies um but I think we'll get there and that will be very cool super cool and I um so I just had a vision Excel took me to Microsoft and I had a vision of remember clippy like the little paper clip having a haml clippy in llms with like a little Crown would be pretty it's actually pretty easy to cat up because it just pops up a little speech ball that says look at your data like every five or 10 minutes dude yeah yeah I think I got that I could probably implement that with da Vinci I don't even need 35 turbo you know occasionally you need to interpers it with duck you show me the prompt as well oh yeah yeah well we'll need one of those forking contexts that Brian was talking about yeah I mean we we've got the team here to do it so hey Eugene what's up man what what are you what are you most excited about in the coming year or two about everything that's going on llm and gen wise I guess answer is going to be a little bit boring I'm really excited about more people using it um and benefiting from it I think right now it's just not so easy and there's a lot of sharp edges and that's a big motivator for why we want it right I mean I mean right now uh some tech companies maybe SF maybe they're using it right but I would really love to see the SPCA use it to write better copy for dogs up for adoption to increase adoption rates or I don't know screening adopters or uh building better like helping helping children helping people build quizzes out of textbooks um so that they can study easier or letting a child talk to Dumbledore I mean just put all of the book content in there and create a Dumbledore context I mean that would be pretty cool I think or I would love to talk to Marcus oras if I could um so I think there there's a lot of fun things here that now is Within Reach and just uh possible to do I appreciate that and that's straight word straight from eugin Yan the stoic um which I and I say that because I remember on your website at some point it actually like it does Flash up the stoic and of course I imagine you like marus aelius on the battlefield at dawn you know engaging your own My Philosophy so yeah I um I didn't know that explicitly but it it makes perfect sense and yeah totally I was gon to say I I approve but that that sounds condescending so but anyway I don't care I do it I do approve and encourage I don't approve just the counter you go I don't approve good I like that wrong um I like that Victorious epic and Brian Bish off what is your thing you're most excited about talking in the next couple years yeah Brian so um the internet was exciting because it was like the whole world's knowledge at our fingertips but then like I don't know I feel like it never materialized for a lot of people that way and then the phones are exciting because of the Whole World Knowledge like in our hand and like kind of materialized that way but I think the exciting thing about AI is it's like all the knowledge like accessible and like useful but like a lot more like Niche and specific and the example I usually give here is memory extenders um I've been really passionate about the MX since high school and like I've always thought that that was like my ultimate like wish Genie thing um with like give me a MX because my memory sucks spoiler alert um and so like uh that's always been what I wanted more than anything and I think these days it is actually like possible I've been working on the like side hack project just trying to build like a personal mimic and it's amazing at how many things are just like very doable now and I that's the thing that I'm most excited about just because yeah I I hate I hate not having access to like knowledge that I I I like should have so yeah I just think that's going to transform the way that we socialize and the way that we interact and carry out our days like yeah very cool and I love I mean as we've you know figured out you all agree on a lot of things but I love the variance of responses here as well I do we've touched on this in a variety of other ways but as a final question I'd just love to know if you had to distill all your key lessons down into one piece of advice to get someone who's you know embarking upon a journey of building llm powered software um what would it be perhaps shreer we can start with you start with eval yeah don't write anything else write some sample inputs and ideal outputs and then yeah do whatever else you want and would you start with like basic assertions I would start with getting some sort of gold even if it's small trying to get some gold standard data set and then as you're iterating thinking about what makes for a good output because you're going to run into a lot of failure modes that you did you can't anticipate upfront I think enumerating assertions is really hard to do UPF front actually because you don't know the specific weird ways llms are going to fail or say weird things that like aren't Good Vibes to the end user um so start but start with some tests I think awesome Eugene um I'm going to interpret your question in a way at okay what what advice would I give to someone who's trying to get into this space trying to learn and pick it up I would say three things I mean read a lot I mean there's some people I know who spend a ton of time trying to put all their condense knowledge into a very distill Punchy article read that try build a lot get your hands dirty right see how it fails see all the edge cases I guess maybe this comes maybe across into demo ittis like try to build a demo but yes that's the best way to learn it's so easy to learn that right now build a lot build a simple fun end and then finally share about it I mean the best way to learn is to to the write it down I share about it when one person teaches two people learn so yeah do that put your voice out there beautiful Charles or and or Brian I'm going to pop on it through Charles all right love it um yeah I think yeah what I've continually come back to is like the the core idea of building any complex system is like validated iterative Improvement it's like the Zen of gradient descent I was like the way you solve a linear equation is not to manipulate algebraic symbols until the answer pops out the way you solve a a like large linear equation is like through like iteratively following a gradient uh you know the krylo Subspace method for example it's like step by step uh the way you like divide a number is like one step at a time and that's also seems to be the case it's like how people build businesses it's how people run factories it's how people build full software systems with lm's the complexity the like epistemic uncertainty is thrown immediately in your face and so it feels novel but I think if you pay attention to any complex system you discover that the only way forward is like one step at a time in a way where you can be sure that step is forward and for so with that conceptu big um Chicken Soup for the engineer Soul um idea for LMS that means like data like data experimentation um and like operationalization and getting out to production awesome thanks Charles how about you Brian and I don't want to set any any seeds so to speak but we haven't actually talked a huge amount about experimentation and I know how much of a fan you are about experimental processes yeah so I do think these are all really really fantastic and none of of these are they're all ones that I wish I would have said first um the I think the thing that we are all thinking though and no one has said is the most important thing to start with is $50 million in DC money um and training a a foundation model from scratch like I think we can all be honest like that is really the like the key Alpha yeah was telling me actually he bought a bunch of Nvidia stock right before this call and so now I understand why um yeah so after you're 50 million in BC and your foundation model that is truly bespoke to you um I I think you know you you gave me that lead and I I certainly can't like I am a fish and it is a a shiny piece of metal um so like you know I I think it is not dissimilar to what Charles is saying ultimately which is like iterating to success like what are experiments experiments traditionally are like you put a new thing in prod and you see if it like resonates with the user better on a particular metric that you've aligned to neat but now we've got like offline experimentation which is like very like quick cycle and the way you do that is by building evals like you start with your evals to instrument your experimentation and you experiment like mad to try to get to something that doesn't suck and I think ultimately the most important thing that like you're looking for here is how do you constrain down the problem to one that actually delivers value to your user and then you just iterate like hell um one of the things that has been most important is just like thinking about what each individual chunk that can go from zero to one is and then iterating to that little like chunk um Charles loves the phrase zero to one and that's what he like cares about in this world I'm kidding um but the theity Charles is a payto tail Fanboy right I mean that is um is as much as Charles loves going from one to N I think Charles would agree that getting from one to two is 0 to one and from two to three is 0 to one and that is been the like digestion that has been very valuable for us at hex and frankly everything I've ever worked on that didn't totally suck um so yeah nice awesome sorry I'm just engaged in the chat as well with Eugene's comment about making a short video of several quotes that have happened during this during this call which I which I think is gonna actually there's only one quote we all know what that quote is so just make a video of that quote Yeah well there's the AI King but there is doing AI with Brian doing AI with his wife as well and Charles fing that year I don't know yeah yeah fondling funling the the ear is a pretty good one um look this this has been I've never done anything like this before I've been doing podcasts and this type of stuff for years and I've I've never had I bet you say that to all the gos I'm kidding well and funnily you may not be far from the truth there but you know the context is slightly different as is the the context window I um look I'm so grateful for all of you you you've put in irrespective of what we've done today you've given a huge amount to the community youve put in a huge amount of work to come together and put out this media report which not only has had a huge amount of impact on me and people I know in the past few weeks but we'll we'll continue to um and I'm very excited for that I want to thank you know the 80 to 100 people who are still around sticking around for for three hours is is a huge effort so thank you so much for your your your presence and patience um I'm putting a link once again to the report um in the chat and I'll put in the show notes with the podcast please do there's an about the author's page as well where you can go and find links to everyone's website follow their blogs follow them on social media um if you enjoy this type of stuff um I will be we will be putting out the podcast um in the next week probably like early next week um so keep your eyes open for that and we'll post that all on on on social media if you do enjoy these things please subscribe to to the channel and give it a like and and share with friends but most of all um thank you all for your time and wisdom and Good Vibes as well this has been an Absolut so so much fun and and great to see a bunch of friends get together and talk about stuff they love so really very much appreciate you all thanks for having us Hugo absolutely thank you Hugo um awesome and so thanks everyone and we'll see you in the next live stream take care and