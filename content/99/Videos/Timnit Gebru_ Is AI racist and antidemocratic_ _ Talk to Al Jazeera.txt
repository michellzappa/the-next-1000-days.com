the history of humanity has been marked by its technological advances the discovery of fire two million years ago the invention of the wheel in 3500 bc all the way to the industrial revolution in the 18th century throughout the times we've sought to make our lives easier though many argue some of those advancements have proven destructive in modern times our ambition for a better life has taken us to the age of information technology programming and artificial intelligence ai gives machines the ability to do more they can think for themselves learn our preferences and behaviors communicate with us suggest alternatives and even do things only humans once did alexa order more dog food ai has slowly become an essential part of our life its use in social media has brought us closer with our families and friends and has proven valuable at home and at work but some say there's another more sinister side to artificial intelligence ethiopian american computer scientist timit gebru has been one of the most critical voices against the unethical use of ai she's been vocal about issues around bias inclusion diversity fairness and responsibility within the digital space google asked her to co-lead its unit focused on ethical artificial intelligence but six weeks later the tech giant fired her after she criticized the company's lucrative ai work gabru considered one of the 100 most influential people of 2022 by time magazine has now launched an independent research institute focused on the harms of ai on marginalized groups so who's behind artificial intelligence technology and whose interests does it serve and with such a big influence in our lives how democratic is its use computer scientist tim neet gebre talks to al jazeera tim nick thank you for talking to al jazeera so to start with it let's start right at the start and just set the scene a little bit for people who might not think about ai in everyday life as the technology stands right now how much are we using ai in every day-to-day life how embedded is it right now for most people i don't blame the public for being confused about what ai is i think that many researchers like myself who have gotten our phds from ai labs studying ai are also confused because um the cons the conception of ai that we see in pop culture is in my view what really really shapes public opinion about what it is and so it kind of makes me realize that pop culture has this huge um power to shape people's thinking right so i think when people think of ai they're thinking about terminator kind of things these robots that are kind of human-like um and or are gonna destroy the world or are you know or either good or bad something like that but that's really not what is being branded as quote unquote ai right now um anything you can think of that has um any sort of data processed and makes any sort of predictions about people what shoshone as a wealth calls surveillance capitalism is based on what is currently being branded as ai any sort of chat bot that you use for instance um whether it is alexa or siri or um i guess these are voice assistants or or chat bots that a lot of companies use because they want to hire less call center operators or things like that there could be some sort of quote-unquote ai behind it there is a lot of surveillance on in day-to-day life whether it is space recognition or other kinds of uh tracking that go on um and that that has some sort of ai in it and there's recommendation engines uh that that we use that we might not even know exists um when we're watching you know videos on tick tock or something like that or advertise targeted advertisement that we get or music selections um that try to infer what kind of music we want to listen to next based on what we listened to before so it's a very broad kind of uh branding and it wasn't always the case but i think that you know there's always the language du jour that people use in order to kind of sell more products or hype hype up too many uh of their products in my opinion so that is currently in my view what is being branded as artificial intelligence it's really interesting because i guess when you think about using like even face recognition or getting a playlist recommended to you as you say i mean i don't think about that being ai i'm just like opening my phone i guess that's something you know are people thinking about it as they use it or is this just um i guess going under the radar as just the future of what it means to use technology it's very interesting because they're in my opinion there is this deliberate rebranding of artificial intelligence that's going on so that um people are confused by the capabilities of the systems that are being billed as quote-unquote artificial intelligence right so for instance we even see these papers that say um that computers can now identify skin cancer with super human performance that they're better than humans at doing this which is really not true right so scientists themselves are engaging in this kind of hype and corporations themselves are engaging in this kind of hype and what that does is instead of people thinking about a product that is created by human beings whether they're in corporations or government agencies or military agencies like defense contractors right creating autonomous weapons and drones so instead of thinking about people creating artifacts that we are then using we think about quote-unquote ai as this you know some being that has its own agency so what we do then is we don't ascribe the issues that we see to the people or corporations that are creating harmful products we we start derailing the conversation talking about whether you can create a moral being or you can impart your values into air or whatever because now we are kind of ascribing this responsibility away from the creators of these artifacts like machines right to some sort of you know being that we are telling people have their own agency okay so that's what ai is what got you into your line of work the ethics of artificial intelligence because it hasn't always been an easy path it seems initially i was just interested in the technical details face recognition is a is something that is done under the computer vision umbrella or any other kind of thing that tries to make sense of images it seemed really cool that you could infer certain things based on videos and images and that was what i was interested in however there was a confluence of a number of things so first of all when i went to graduate school at stanford i saw the stark the lack of any black person from anywhere in the world in graduate school and especially in with respect to artificial intelligence developing or researching these systems so when i when i was at stanford by then i heard that they had literally only graduated one black person with a phd in computer science ever since the inception of the computer science department and you can imagine the type of influence that this school has had on the world right you can imagine the kinds of companies that came out of there including google um so i that was just such a shock to me so i saw not only the the the lack of uh black people in the field but also the lack of understanding of kind of what we go through and what systems of discrimination we go through in the u.s any and globally really around the same time i also started reading about systems that were being sold to the public and being used in very very kind of scary ways so for instance there was a propublica article that showed that there was a company that purported to determine the likelihood of people committing a crime again and unsurprisingly of course it was heavily biased against black people at the same time you know i see the kind of drones and systems purporting to determine whether somebody's a terrorist or not etc and my own life experience told me you know who would be most likely to be harmed by those systems and who would be most likely to be developing those kinds of systems right so that was the the time uh where i started pivoting from purely studying how to develop these systems and doing research on the technical aspects of this field to being very worried about the the way in which these systems are being developed and who they are negatively impacting learning about the problem the existence of um an algorithm a model that purports to predict someone's likelihood of committing a crime again was such a huge shock for me and by then it had existed for a long time and in addition to to that um you know so this system judges used for sentencing for for setting bail um along with other inputs and there are other systems other predictive policing systems so one example of a predictive policing system was something called pred poll that actually um the la police lapd were using and thanks to a lot of activism from groups like stop lapd spying this software stopped being used by lapd actually my people in my field statistician christine lom and scientist um william isaac did a study that actually reverse engineered pred pool and showed that unsurprisingly it pinpoints neighborhoods with black and brown people and says that these neighborhoods are crime hot spots right even if for instance um drug use isn't one example if you look at the national survey for drug use it's pretty evenly distributed in for instance oakland right but the um these predictive policing systems like pred pool they instead kind of pinpoint black and brown neighborhoods saying that these are hot spots and why is that well those of us who know history and the current realities of the u.s we're not surprised by that because these systems they feed in they have training data that are labeled and the training data does not depend on who commits a crime it depends on who was arrested for committing a crime and obviously that's going to be biased i want to come back to to the issues around the data that you put into ai and what the results are that you get in a minute but let's go back to when you were hired at google what was it that you were hired to do i was hired to do the kind of uh work that i'm talking about with respect to analyzing the the negative societal impact of ai and working on all aspects of mitigating that whether it is technical or non-technical or documentation so i was a research scientist with um you know the freedom to set my own research agenda and i was also co-lead of the ethical ai team with my former co-lead meg mitchell and so our job there was also to create to set the agenda of our small research team which is again focused on minimizing the negative societal impacts of ai and as you say there's a lack of diversity in the industry you knew that you know if you've known that since you got into this so what were the realities then of going into this this mega huge company as a woman of color trying to do that job it was incredibly difficult from day one i faced a lot of discrimination whether it's sexism or racism from day one i tried to raise the issues but it was exhausting you know my my colleague mitchell and i we were just so exhausted and doing research was basically you know working on our papers and discussing ideas felt like such a luxury because we were just always so exhausted by all the other issues that we were dealing with you eventually put out a paper which led to your being dismissed although google says you resigned but put that to the side this this paper looks at the biases being built into ai machines basically reflecting the mistakes that that humanity has made is perpetuating history a foregone conclusion when we when we talk about ai or is there another path i i always believe that we have to believe that there is another path um and this comes to uh back to the way in which we discuss ai as just being its own thing rather than an artifact a tool that is created by human beings that are um in corporations or educational facilities or other institutions right so as long as we have to know that we control what we build and for and who we build it for and what it's used for so there's definitely another path but for for that other path to exist we have to um uncover the issues with the current path that we're going on and and remedy them and also invest in terms of research in those other paths so for instance on this paper that i put out called on the dangers of sarcastic parrots it um talks about this huge race that is going on right now on developing what are called large language models and so these uh models are are trained on massive amounts of data from the internet scraped from the internet right and so you and i are not getting paid for the content that we put out on the internet that that is being um scraped to train these models something just just to make it really simple i mean something that i hadn't even considered what you're talking about is you know giving ai all of the information of the internet and of course it's going to you know spew out some of the the worst parts of the internet which are you know often predominant but if we give it a smaller data set or if we curate the data then we're going to get something that might be more helpful for people is that kind of putting it too simply no actually that is one of you know we discuss so many different kinds of issues in our paper and one of the issues we discussed is exactly what you mentioned in terms of curating data and using you know large um uncurated data from the internet with the assumption that size gives you diversity right and so what we say is size does not give you diversity and we detail so many ways in which that's not the case and one of the suggestions that we make is to make sure that we curate our data and we understand our data and if we believe that the data set that we are using to train these models is too large too daunting too overwhelming for us to understand it document it and curate it then that means we shouldn't be using that data right and so this is this is kind of one of the things that we're talking about another thing that i thought was really fascinating um that i guess we don't consider in our daily lives is that at a macro level the funding for a lot of the technological advances that trickle down to us begin either with the military or with these massive tech giants that you know can they have our best interests at heart this is um precisely uh what i talk about too with um the founding of our new my new institute the distributed air research institute right so a lot of when you look at history in ai whether it is things like machine translation or self-driving cars right self-driving cars are a good example they were very much funded by darpa a defense um funding agency right so it's not because they're interested and on accessibility for for disabled people right they're interested primarily in autonomous warfare so how can we assume that something that starts with that goal and that funding insight will end up being something different um and so i often give this example of you know when people talk about ai for social good right they talk about kind of reorienting some of these things that we already built for quote-unquote social good whereas for me it's kind of saying that okay we built this tank first and then we try to figure out how to use this tank for something other than warfare maybe we could use it for farming maybe we can use it for something else but the thing is we we already made the tank right we designed the systems so that they become a tank like for a specific um goal and outcome which is warfare so that's exactly how we've been designing our technological systems if you look at the history of ai um when it's funded by the government when they fund basic research in this space and when they have all of these collaborations with large tech companies when you look at um really prestigious top schools like mit they're huge military contractors right the lincoln lab so i think that as human beings we have to look at ourselves and say what are we building and where are we going and why are we building this thing and we can have a different path you mentioned governments there i'm wondering in your line of work have you seen governments use data you know in a way that that if we all knew about we would be we would be upset to find out about like our government's in on uh using data for ai in ways that perhaps we we might not be aware of there are you know all sorts of face recognition related uses by law enforcement for example and recently my colleague joy uh bulamini my co-author and friend who also heads the algorithmic justice league had a series of videos and and op-eds and other educational material describing id dot me right this is um the irs is uh was asking people to submit basically biometric information so in order to log in and file taxes and do all sorts of and get all sorts of government services they were using this private company id dot me um to as a verification mechanism so then this company has your your biometric this private company has your biometric information and this is getting proliferated everywhere like if you look at um in airports um they're now using all sorts of you know face recognition related things to to verify um that it's you um and i don't even know exactly who they're using and how this is getting proliferated so every day we're finding out about new uses that we never knew about we never voted on we never learned about we were never educated on the worst example for me is clearview so clearview is this company that has been under fire for using so many people's face data from the internet spring for example i believe facebook um and um you know training all of these uh automated facial analysis tools that are being used by law enforcement and all sorts of groups around the world and it's been sued for a number of things right like for example their use of this kind of data but then you also find out that they have all these partnerships with all sorts of governmental agencies for people um because i think people will be watching this at home going oh my gosh i did give my biometric information or you know i do get scanned at the airport all the time for people watching at home who are concerned about how their data is being used concerned about you know how that's being fed into artificial intelligence and you know the whole thing what can people at home do well what do you do do you avoid certain things or certain places or you know i'm just trying to think of some some helpful things at home for people who are watching going oh no you know i need to do something in my opinion you know the biggest thing we can do is that we need to advocate for regulation that puts the onus on companies to make sure that they keep people safe to make sure that they don't they that they prove to us before they put products out there that they're not harmful the issue right now is that we're assuming that the onus is on the public on each one of us and how many how many times can you do this right like every single thing you click on every single thing you use you have to make sure that you know you have the privacy settings right etc and i think it just you know people say privacy by design right or um we should have something like i guess cyril fiddler saying fairness by design or something like that like justice by design right we where the onus is on the designers and implementers and not on the public um to to go on their daily lives and make sure that they spend all their days reading every terms of service and making sure that they click on you know certain things and not others we've talked a lot about um about you know big tech companies about the vast amounts of data being collected is a.i inherently anti-democratic that's a very interesting question so there are there is a segment of people in ai that whose goal is to create what's called an um artificial general intelligence what does it mean to create artificial general intelligence it means that you're trying to create a this being that knows everything um can automate any task so if you're a corporation then you can have this thing that does all your tasks for you that you don't have to pay um and that you don't have to care about right and so that kind of goal i find extremely strange and inherently undemocratic to me personally any aspect of ai i see it as a tool as a tool that we can build for specific needs of community so if you build a tool based on the needs of specific communities so for example i had talked about um you know people that detect who i believe radio station in new zealand right using a language technology for revitalization of the maori language right and so that to me is is definitely a democratic goal it is a goal that is allowing people to use their language and culture after it has been beaten out of them because of colonization so if we decide that that is what we're going to do we can do that and we can create our funding structures um design our systems and um processes accordingly so again you know i think that there is a world in which we can build ai tools that are beneficial to people but that means we have to do a rethinking of where the money comes from and how we do our research and development process however there is this segment of people in ai who have this weird goal of creating artificial general intelligence which i believe is inherently undemocratic timmy gabriel thank you for talking to al jazeera thank you for having me [Music] you