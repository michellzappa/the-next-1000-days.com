Can you spot the following pattern if you look at this grid here and see how it's been transformed into this grid and likewise how this grid on the left has been transformed into this grid Can you spot that pattern could you in other words predict what would happen to this grid here in the test example well it might shock you to learn that language models like gp4 can't do this they are terrible at noticing that the little squares are being filled in with a darker shade of blue this specific abstract reasoning challenge was not in the training data set of GPT 4 and that model cannot generalize from what it has seen to solve the challenge it's not generally intelligent it's not an artificial general intelligence now you might think that's a minor quipple but it gets to the heart of why current generation AI is not AGI and frankly isn't even close and no neither will this problem be solved by a simple naive scaling up of our models but this video isn't just about picking out one floor orbe it a critical one in our current llms it's more my attempt to address this swirling debate about whether AI is overhyped or underhyped for many AI is nothing but hype and is a giant bubble while for others AGI has already arrived or is just months away but you don't need an opinion what does the evidence show this video will draw upon dozens of papers and reports to give you the best snapshot I can on llms and AI more broadly I'm going to start I'm afraid with so much of what's wrong with the current landscape from delayed releases overpromises and my biggest current concern a tragedy of the commons from AI slop but then I will caution those who might throw the baby out with the bath water giving six detailed evidence-based pathways from the llms we have today to substantially more powerful and useful models including systems that yes perform decently even on that abstract reasoning challenge I showed you earlier but let's start with the dodgy stuff in quote Ai and no I'm not referring to the fact that the creators and funders of this Ark AGI challenge are so confident that current generation LMS cannot succeed that they've put up a prize pool of over a million doar if GPT 40 was a pure reasoning engine well then why is its performance negligible in this challenge but no I'm actually referring to the landscape of over promising and underd delivering you might remember Demis cabis referring to the original Gemini model when it was launched as being as good as human experts but Google has had to roll back its llm powered AI overview feature because there were simply far too many mistakes if Gemini was as good as human experts as some some benchmarks were claiming to show then why wouldn't it be better than a random Google search but surely the newly announced Apple intelligence will be far better though well aside from the fact that we can't actually test it no Tim Cook admitted that it still hallucinates all the time now to some as we'll see that's actually the point of llms we want them to be creative but to others it smells more like BS this particular paper aside from being quite funny gave one clear take-home message language models aren't actually designed to be correct they aren't designed to transmit information so don't be surprised when their assertions turn out to be false of course people are working on that and the paper even makes reference to the let's verify step-by-step paper more on that later but the point remains so we have the hallucinations in Ai and then the hallucinations in the marketing about Ai and the fact that we have ai powered toothbrushes isn't even my primary concern and nor is it actually that we occasionally have those wildly overhyped products like the rabbit R1 and the Humane AI pin then there's the possible breaches in privacy that features like Microsoft recall seem to inevitably invite I don't know about you but it was never appealing to me to have an llm analyze screenshots taken of my desktop every few seconds imagine the poor llm trying to sift through all of the Creations that some are likely to come up with using open AI tools you might think I've reached the end of my dodgy list but I'm not actually even close what about the increase in academics using llms to write or polish papers you can see the recent and dramatic increase in the use of the word delve on papers on PubMed for me as soon as I suspect an article I'm reading is llm generated I just discount it heavily then we get the delayed releases now this one is arguably a bit more forgivable but we were promised GPT 40 within a few weeks I think we all would prefer a tradition when features are announced the moment they're actually available I will come back to GPT 40 in a moment now you might be different but for me the number one concern at the moment is just AI generated slop take this tool where on LinkedIn you can imitate the writing of someone in your field or industry are they going viral while you get little to no engagement copy their tone and address the same topic and I would call this a kind of tragedy of the commons for the individuals using this and I'm not meaning to pick on one individual tool but for the individuals using this it's probably pretty helpful it probably does Boost engagement and help sort out any language issues but as we've seen of late on Facebook it just leads to this General AI generated miasma Bots engaging with Bots gullible people drawn in and fooled a landscape where increasing L you can't trust what you see or even hear and that's before even getting to the topic of deep fakes which of course can acutely affect the individuals concerned you can of course let me know if you agree but for me this is at the moment the number one concern and it's hard to see how it would be stopped at this point you're probably thinking damn this is pretty negative on AI but as you can see the video has plenty of time to go but at this point my rejoiner to sum is that it's all too easy to fall into diametrically opposed camps as we saw last year with accelerationists and doomers we're seeing this year with those who say AI is nothing but hype and those who say AGI is imminent but I think the world is just so much more complex than that I reckon that if I was blind I would forgive the occasional hallucination with GPT 40 and be grateful to have a model that can tell me about the World Around Me Oh and interactively and in real time and tell me exactly what they're doing right now please um right now the Ducks are gently gliding across the water they're moving in a fairly relaxed manner not in a hurry occasionally one of them will get its head under the water probably looking for food and then pop back up I even know when a taxi is coming with its orange light on I think I'll hail it to get home yes I spotted one just now it's heading away on the left side of the road get ready to wave it [Music] down great job hailing that taxi it looks like you're all set to go that's a good dog right there leading the way into the taxi safe travels and remember too that as project Astra from Google demonstrated models are getting better at ingesting more and more tokens more and more context they are increasingly able to help users of any kind locate things not just in text but in the real world now we'll get back to llms in a moment but of course it's worth remembering that there are far more to neural networks than just llms a new study in nature showed how you could use Gans generative adversarial networks to predict the effects of untested chemicals on mice gen AI in this case was able to simulate a virtual animal experiment to generate profiles similar to those obtained from traditional animal studies the tldd is not just that the Gen predictions had less error but they were produced much more quickly and as the BBC reported this is at least one tentative step toward an end to animal testing and yes that study was different to this one released a few days ago with Harvard and Google deepmind where they built a virtual rodent powered by AI again a highly realistic simulation but this time down to the level of neural activity inside those real rats then of course we have the good old con evolutional neural networks for image analysis their use in the brainix E stroke system is as we speak helping stroke victims in the NHS essentially it has enabled diagnosis to be made by clinicians much more quickly which in the case of Strokes is super important of course and that has tripled the number of patients recovering now the only tiny tiny complaint I would make is that as ever the title just uses the phrase AI it took me a disturbing amount of digging to track down the actual technique te used and even then of course for commercial reasons they don't say everything but now I want to return to large language models the central focus of this channel now I have discussed in the past on this channel some of the reasoning gaps that you can find in current generation large language models but the arc abstract reasoning Challenge and prize publicized this week by the legendary franois charet is a great opportunity to clarify what exactly are current models Miss and what is being done to rectify that Gap hopefully at least the following will explain in part why our models can sometimes be shockingly dumb and shockingly smart I'm going to leave in the background for a minute an example of the kind of challenge that models like gp4 fail at consistently so here is in 60 seconds what the current issue is if language models haven't seen a solution to something in their training data they won't be able to give you a solution when you test them that's why models fail at this challenge they simply haven't seen these tests before moreover the models aren't generally intelligent you can train them on millions of these kind of examples and people have tried and they'll still fail on a new Fresh one again if that new fresh example isn't in the training data set they will fail if the mother of Gabriel Mack is in the data set they will output the correct answer if however the data son of that suzan Victoria pulia is not in the data set it will not know it doesn't reason its way to the answer based on other parts of its training data so how can models do so well on certain benchmarks like the math benchmark well they can quote recall from their training data set certain reasoning chains that they've seen before that's enough in certain circumstances to get the answer right so hang on a second they can recall certain reasoning procedures let's call them programs but they can't create them yes it's a good news bad news kind of situation but for the remainder of this video indeed for the remainder of llms it will be important to remember that distinction recalling reasoning procedures or programs versus doing fresh reasoning itself if it's seen it before great if it hasn't seen it before not so great okay so we're almost ready for those six ways which might drag llms closer to that artificial general intelligence but first I want to quickly focus on one thought you may have had in reaction to this framework why not train language models on every reasoning procedure out there feed it data on any scenario it might encounter and wouldn't that be AGI well here's franois sh the author of The Ark challenge he'll explain why memorization isn't enough because we don't don't see the whole map if the world if your life were a static distribution uh then sure you could just Brute Force the space of possible behaviors you can think of intelligence as a paast finding algorithm in future situation space like I don't know if you're familiar with scam development like RTS gam development but you have a map right and and you have it's like a 2d 2D 2D map and um you have partial information about it there is some fog of War on your map there are areas that you haven't explored yet you know nothing about them and then there are areas that you've explored but um you only know how they were like in the past and and now instead of thinking about to I think about the space of possible future situations that you might encounter and how they're connected to each other intelligence is a past finding algorithm so once you set a goal it will tell you uh how to get there optimally um but of course it's it's constrained by the information you have it it cannot pass F in an area that you know nothing about if you had complete information about the map then you could solve the pathf finding Problem by simply memorizing every possible path every mapping from a point A to point B solve the problem with pure memory but where the reason you cannot do that in real life is because you don't actually know what's going to happen in the future so if AI will be encountering novel situations it will need to adapt on the Fly what would make me change my mind about that Al is basically if I start seeing a critical mass of cases where you show the model with something it has not seen before a task that's actually novel from the perspective of its training data something that's not in a training data if it can actually adapt on the Fly and that might be more possible than you think as gnome Brown of open AI thinks he's optimistic that llms will crack it but again it won't be just a naive scaling up of data that alone gets us there without examples or zero shot as we've seen models don't generalize from what they've seen to what they haven't this paper demonstrates that in the visual domain as we've already seen it in the text domain no matter what neural network architecture or parameter scale they tested models were data hungry unlike a human child they didn't learn in a sample efficient manner remember that a child might be shown one image of a camel with the caption camel and retain that term for life however Midway through the paper there there was some tentative evidence that with enough scale you can get decent results on rarely found Concepts those were represented by the let it wag data set referring to the long tail of a distribution anyway as you can see models that performed at over 80% on this traditional well-known image net accuracy test did perform fairly well on this longtail data set as they note the Gap in performance does seem to be decreasing for higher capacity model in other words more data definitely helps especially exponentially more data but it definitely won't be enough but even this paper pointed to some ways that this challenge might be overcome they note possibilities for not only retrieval mechanisms but compositional generalization in other words piecing together Concepts that have been found in the training data set to be able to recognize more complex ones and if you think that's impossible I've got news for you in a moment just before I do though I want to get to one other approach that I don't think will work you may or may not have heard on the Twitter sphere about the situational awareness 165 page report put out by a former open aai researcher I've done a full 45 minute breakdown on my patreon AI insiders but one key takeaway is this he thinks the straight march to AGI will be achieved by scaling up the parameters and data of our current llms but again I think that's far far too simplistic just throwing on more parameters and more data wouldn't resolve the kind of issues you've seen today Leopold Ashen brener also made some other somewhat crazier claims but that's for this video so it's time to get to the first of those six methods that I mentioned that might drag llms closer to something we might call a general intelligence and this paper published late last year in nature is about that compositionality I mentioned just a moment ago perhaps models can't reason but if they can better compose reasoning blocks into something more complex might that be enough well the authors prove that point in principle at least on just a 1.4 million parameter Transformer based model boldly they claim our results show how a standard noral Network architecture optimized for its compositional skills can mimic human systematic generalization in a head-to-head comparison and as the lead author of the paper retweeted the answer to better AI is probably not just more training data but rather diversifying training strategies tldr send the robot to algebra class every so often and the challenge as you can see in the bottom left was this it might even remind you a little bit of the arc challenge the challenge was to work out what this madeup language fragment actually meant based on these rules given enough time humans tend to do fairly well at this challenge like the arc challenge but models like GPT 4 as we'll see flop hard remember GPT 4 has 1.8 trillion parameters versus the model they trained at 1.4 million anyway with enough time in your case and training in the case of their Transformer model it worked out that for example who means to whereas s means green and re means light blue there are other rules to work out of course and then those rules have to be composed together to work out the question answer now remember when they were tested they were tested on a new configuration of words they had to quote understand the madeup rules of a new language and apply them to phrases it hadn't been trained on it was able to do so in other words show the first tentative hints at true reasoning it's only a very small step towards AGI though because when they tested these flickers of compositional reasoning on a new task this tiny model failed okay time for the next approach if as we've seen language models have these reasoning chains or programs within them to solve challenges they're just hard to find what about methods that improve our ability to find those programs within language models that's what in part verifies our about a string of papers came out this week about using verifiers and Monte Carlo treesearch to improve the mathematical reasoning of language models of course covering all of them would be a video in and of itself but in a nutshell it's about this you can train a model to recognize faulty steps in a reasoning chain to pick out the bad programs with let's verify step by step which I've talked about many times before on this channel that required human annotation but Google deepmind came up with approach which was done in an automated fashion by automatically collecting results which led to a correct answer as well as contrasting them with outputs that led to an incorrect output they trained a process reward model think of it like a supervisor analyzing each step of a language model's outputs or to use the analogy we've been using throughout this video the process reward model could alert the language model the moment it's calling a faulty or inappropriate program in theory but at least in this paper there is a limit to this approach even when analyzing and deciding amongst over a 100 Solutions the performance started to Plateau for sure it's still a huge boost on this math benchmark from 50% to around 70% but there's a limit is that because it's not using human annotation like with let's verify or is it a fundamental limitation with the approach perhaps if a language model doesn't have the requisite program in its training data set to solve a math problem it can't do so no matter what supervision it's given nevertheless the principle is clearly established we don't have to rely on the language model itself locating the correct and necessary program to solve a challenge we can at least help it along the way and we know that there are even better verifiers out there namely simulations and the real world as I discussed with Jason Mah the lead author of The Dr Eureka paper it's a feature because that means the LM can sample let's say a 100 different solutions and your simulator in this case serves as external verifier to see which ones are good if your LM doesn't have the ability to hallucinate it's always deterministic then the method will actually not work because you would only be able to generate one candidate per iteration right so it becomes very slow every time the model generates any response it's technically hallucinating it just a sample from the probability distribution right and it's only hallucination if it doesn't agree with what you think it should output right but in my case is I don't care if it agrees with what I think is a good reward I have something external to verify so it's great the model can output a 100 different things that makes the iterative evolutionary process much faster so I think that's actually a blessing that I think if you only think about applications like chatbot agents you may underappreciate but if you think about the use case for large language models or any foundation model for I think Discovery tasks or scientific discoveries what you want is the model to be able to propose 10 different solutions is it possible that we can turn hallucinations from a weakness to a strength Nvidia and others are of course working hard to find out that second approach then can be summed up as using verifiers and other approach is to better locate the requisite programs within a large language model and I will quickly mention another method for locating that latent knowledge many shot give models tons and tons of examples of the kind of task you want it to achieve and it can better learn how to do so it seems obvious but it can lead to significant performance gains but what about teaching language models new programs on the Fly this is what franois Chalet calls active inference and it's responsible for the current state of-the-art score of 34% on the ark AGI prize there are many facets to this approach but I'm going to summarize heavily the key Insight is to use test time fine-tuning when the model sees three examples like those on the left that isn't enough to teach it the way to solve the fourth one it's too minuscule a signal amongst all its many parameters but one of the things that Jack Cole and code did is augment these three examples with many many synthetic examples that mimic the style they then fine-tune the model on those augmented examples I think of it as prioritizing as humans do the thing right in front of its face you can almost think of it like a language model concentrating its parameters are adjusted to focus on the task at hand a bit like a human going into the Flow by the way they also use gp4 to generate many of the synthetic riddles to train their system here's how franois charet describes the approach most of the time when you're when you're using an llm it's just doing static inference the model is frozen and you're just uh prompting it and then you're get you're getting an answer so the model is not actually learning anything on the Fly it's its state is not adapting to the to the task at hand and what jao is actually doing is that for every test problem is on the Fly is fine tune in a version of d llm uh for that task and that's really what's unlocking performance if you don't do that you get like 1% two % so basically something completely completely negligible and if you do test time fine tuning and you add a bunch of tricks on top then you end up with interesting performance numbers so I think what he's doing is trying to address one of the key limitations of LMS today uh which is the lack of active inference is actually adding active Insurance to LMS and that's working extremely well actually so that's fascinating to me in sure even on the Achilles heel of large language models abstract reasoning Jack Cole says this what is clear from our work and from others is that the upper limits of the capabilities of llms even small ones have not yet been discovered by the way his model is just 240 million parameters there is clear evidence that more generally capable models are possible again sticking with the program metaphor as franois Chalet says this is like doing program synthesis another definition we can use is reasoning is the ability to when you're faced with a with a puzzle given that you don't have already a program in memory to solve it you must synthesize on the fly a new program based on bits of pieces of existing programs that you have you have to do on the Fly program synthesis and it's actually dramatically harder than just fetching the right memorized program and replying it now at this point in the video as we get to the fourth approach I'm going to make a confession I have about 20 tabs left on my screen going over other relevant papers on how they improve llms but I am beginning to worry that this video might be getting a little bit too long so for the last few approaches I'm going to be much more brief I hope I'm still conveying that Central message though that llms currently suck at abstract reasoning but that need not be a death sentence nothing in the literature indicates that AGI is at all imminent but neither is AI all hype here is a paper from the arch llm skeptic Professor raal who I interviewed almost a year ago it's a position paper from this week lm's C plan but can help planning in llm modulo Frameworks we had a great discussion almost a year ago which I'll hopefully get to in a different video but the summary of this paper is this earlier work by professor raal and Co had shown that even models like gp4 can't come up with coherent plans they fail in this domain of bloxs world essentially bloxs world is like some of the other reasoning challenges you've seen today in that you have to come up with a coherent plan unstacking blocks and restacking them to meet the required objective definitely not a memorization test as the MML U can sometimes be indeed if you throw off the model and use mysterious words instead of household objects the models perform even worse zero shot gp4 gets one out of 600 challenges however this fourth approach says why do we have to use llms alone why can't we use them with traditional symbolic systems maybe that combination of neural networks and traditional symbolic hardcoded programmatic systems is better than either alone Professor raal is a friend of yan Lun a famed llm skeptic but the paper that he led said this there is unwarranted pessimism about the roles llms can play in planning SL reasoning tasks the key Insight is that llms can act as idea generators those grounded symbolic systems can then check those plans llms as the ideas man with symbolic systems as the kind of accountants llms are great at guessing candidate plans to solve bloxs World challenges and those ideas or you could say retrieve programs aren't all bad even after three or four rounds of feedback from the symbolic system 50% of the final plan retains the elements of the initial large language model plan with that feedback from the symbolic system you back prompt the llm and it comes up with hopefully a better plan not even hopefully though the results are clear GPT 4 can score 82% with this approach still struggles with mysterious languages but can solve five of them this all reminded me of alpha geometry which I have done a separate video on so I won't focus on today but it's that same combination of a neural network and symbolic system for geometry problems specifically in the international math Olympiad it scored almost a gold medal those are incredibly hard reasoning challenges very quickly now the fifth approach instead of calling a separate system how about jointly training on its knowledge for time purposes here is a very quick summary they trained a separate neural network not a language model in this case a graph neural network it learned specialized algorithms then they embedded that fixed optimized knowhow and had a language model train with access to those embeddings in other words a language model fluent in the language of text and algorithms programs that you might need for example for sorting six and finally for this video there's tacit data so much of what humans do and how humans reason is not written down let's hear from Terren to arguably the smartest man on the planet he said so much knowledge is somehow trapped in the head of individual mathematicians and only a tiny fraction is made explicit A lot of the intuition of mathematicians is not captured in the printed papers in journals but in conversations among mathematicians in lectures and in the way we advise students people only publish the success stories the data that are really precious are from when someone tries something and it doesn't quite work but they know how to fix it but they only publish the successful thing not the process and all of this he says simultaneously points to a dramatic way to improve AI but also why we shouldn't expect an intelligence explosion imminently training on that tacet data would unlock notable progress but human mathematicians he says would just in his view move to a higher type of mathematics as we speak open Ai and I'm sure many others are trying to make as explicit as possible that tacit knowledge I'm sure hundreds of phds are writing down their methodologies as they solve problems Millions if not billions of YouTube hours of video are being ingested in the hopes that AI models pick up some of that implicit reasoning while you may see this as the most promising approach of the six I've so far mentioned it wouldn't yield immediate explosive results it would be reliant on us and other human experts to write down with Fidelity our reasoning less a remorseless faceless shogo solving the universe and more a student imitating its teachers of course you will have likely seen the somewhat ambiguous clip from Mira murati the CTO of open aai saying they don't have any giant breakthrough behind the scenes inside the labs we have the these capable models and you know they're not that far ahead from what the public has access to for free and that's a completely different trajectory for bringing technology into the world that what we've seen historically but as I've hopefully shown you today it doesn't have to be an all or nothing AGI imminently or all hype as even franois Chalet says it could be a combination of approaches that solves for example Ark that indeed might be the path to AGI people are going to be uh winning uh the AR competition and that we're going to be making the most progress towards near term are going to be S that manage to merge the Deep planning Paradigm and a discret for search Paradigm into one elegant way so much more to get to that I couldn't get to today but I hope this video has helped you to navigate the current AI landscape as ever the world is more complex than it seems thank you so much as ever for watching and have a wonderful day