[MUSIC PLAYING] HANNAH FRY: Welcome to "Google
DeepMind, the Podcast" with me, your host, Professor Hannah Fry. Now, when we first
started thinking about making this
podcast way back in 2017, DeepMind was this relatively
small, focused AI research lab. They'd just been
bought by Google and given the freedom to do
their own quirky research projects from the safe
distance of London. How things have changed. Because since the
last season, Google has reconfigured its
entire structure, putting AI and the
team at DeepMind at the core of its strategy. Google DeepMind has
continued its quest to endow AI with
human-level intelligence, known as artificial general
intelligence, or AGI. It has introduced a family of
powerful new AI models called Gemini, as well as
an AI agent called Project Astra that can process
audio, video, image, and code. The lab is also
making huge leaps in applying AI to a host
of scientific domains, including a brand new
third version of AlphaFold, which can predict the structures
of all of the molecules that you will find in the
human body, not just proteins. And in 2021, they spun off a
new company, Isomorphic Labs, to get down to the
business of discovering new drugs to treat diseases. Google DeepMind is also working
on powerful AI agents that can learn to perform tasks by
themselves using reinforcement learning, and continuing
that legacy of AlphaGo's famous victory over a
human in the game of Go. Now, of course, you'll all have
been following this podcast since the beginning. You'll all be familiar
with the stories behind all of those changes. But just in case you are
coming to us fresh, welcome. You can find our first
award-winning previous seasons on Google DeepMind's
YouTube channel, or wherever you
get your podcasts. They also, those
episodes go into detail about a lot of the
themes that we're going to hear come up over
and over again from the people here, like reinforcement
learning, deep learning, large language
models and, so on. So have a listen. They are really good, even
if we do say so ourselves. Now, all of the
newfound attention on AI since the last series does
mean that there are quite a few more podcasts out there
for you to choose from. But on this podcast, in just
the same way as we always have, we want to offer you something
a little bit different. We want to take you right to the
heart of where these ideas are coming from to introduce
you to the people who are leading the design of
our collective future-- no hype, no spin, just
compelling discussions and grand scientific ambition. So with all of that
in mind, I am here with the DeepMind
co-founder and now CEO of Google DeepMind,
Demis Hassabis. So with all of that in mind,
do I have to call you Sir Demis now? DEMIS HASSABIS:
No, absolutely not. HANNAH FRY: OK. Well, Demis, welcome
to the podcast. DEMIS HASSABIS: Thank you. HANNAH FRY: Thank you
very much for being here. OK, I want to know, is
your job easier or harder now that there has been this
explosion in public interest? DEMIS HASSABIS: I think
it's double edged. I think it's harder
because there's just so much scrutiny,
focus, and actually quite a lot of noise in
the whole field. I actually preferred it
when it was less people, and maybe a little bit more
focused on the science. But it's also good because it
shows that the technology is ready to impact the real
world in many different ways, and impact people's everyday
lives in positive ways. So I think it's exciting, too. HANNAH FRY: Have
you been surprised by how quickly this has caught
the public's imagination? I mean, I guess you would have
expected that eventually people would have got on board. DEMIS HASSABIS: Yes, exactly. So at some point,
those of us who've been working on it like us for
many years now, even decades, so I guess at some
point the general public would wake up to that fact. And effectively,
everyone's starting to realize how important
AI is going to be. But it's been
quite surreal still to see that actually come
to fruition, and for that to happen. And I guess it is the advent
of the chat bots and language models because everyone,
of course, uses language. Everyone can
understand language. So it's an easy way
for the general public to understand and maybe
measure where AI has got to. HANNAH FRY: I heard you
describe these chat bots as though they were unreasonably
effective, which I really like. And actually, later
in the podcast we are going to be discussing
transformers, which was the big breakthrough,
I guess-- the big advance that gave us those tools. But tell me first, what do you
mean by unreasonably effective? DEMIS HASSABIS:
What I mean by it is I suppose if one were to
wind back 5, 10 years ago, and you were to
say the way we're going to go about this is build
these amazing architectures, and then scale from there,
and not necessarily crack specific things like
concepts or abstractions. These are a lot of
debates we would have 5, 10 years ago is do you
need a special way of doing abstractions? The brain certainly
seems to do that. But yet somehow, the systems,
if you give them enough data-- i.e. The whole internet--
then they do seem to learn this
and generalize from those examples--
not just rote memorize, but actually somewhat understand
what they're processing. And it's a little bit
unreasonably effective in the sense that I
don't think anyone would have thought that it would
work as well as it has done, say, five years ago. HANNAH FRY: Yeah. I suppose it is a
surprise that things like conceptual
understanding and abstraction have emerged rather than been-- DEMIS HASSABIS: Yes,
and we would have been-- probably we discussed last
time things like concepts and grounding-- grounding language in
real world experience, maybe in simulations or as
robots embodied intelligence, would have been necessary
to really understand the world around us. And of course, these
systems are not there yet. They make lots of mistakes. They don't really have a
proper model of the world, but they've got a lot further
than one might expect just by learning from language. HANNAH FRY: I guess we
probably should actually say what grounding is for
those who haven't listened to series 1 and series 2. Because this was a big thing. I mean, we were talking
about this a lot. So do you want to just
give us an overview of what grounding is? DEMIS HASSABIS:
Grounding is when-- one of the reasons the
systems that were built in the '80s and '90s, the
classical AI systems built at places like MIT, they
were big logic systems. So you can imagine them
as huge databases of words connected to other words. And the problem was you could
say something, a dog has legs, and that would be
in the database. But the problem was, as
soon as you showed it a picture of a dog, it had no
idea that collection of pixels was referring to that symbol. And that's the
grounding problem. So you have this
symbolic representation, this abstract representation,
but what does it really mean in the real world-- in the messy real world? And then, of course,
they tried to fix that, but you never get
that quite right. And instead of that, of
course, today's systems, they're directly
learning from the data. So in a way, they're
forming that connection from the beginning. But the interesting thing
was that if you learn just from language, in
theory, there should be missing a lot of the
grounding that you need. But it turns out that a lot
of it is inferrable somehow. HANNAH FRY: Why, in theory? DEMIS HASSABIS:
Well, because where is that grounding coming from? These systems, at least the
first large language models-- HANNAH FRY: Don't exist
in the real world. DEMIS HASSABIS: --don't
exist in the real world. They're not connected
to simulators. They're not connected to robots. They don't have any
access to even-- they weren't multimodal
to begin with, either. They don't have access to
the visuals or anything else. It's just purely they
live in language space. So they're learning
in an abstract domain, so it's pretty surprising they
can then infer some things about the real world from that. HANNAH FRY: Which makes sense if
the grounding gets in by people interacting with the
system and saying that's a rubbish answer,
that's a good answer. DEMIS HASSABIS: Yes. So for sure, part
of that, if the question that they're
getting wrong, the early versions of this,
was due to grounding missing-- actually, the real world
dogs bark in this way or whatever it is-- and it's
answering it incorrectly, then that feedback
will correct it. And part of that feedback is
from our own grounded knowledge. So some grounding is seeping
in like that for sure. HANNAH FRY: I remember
seeing a really nice example about crossing the English
Channel versus walking across the English Channel. DEMIS HASSABIS: Exactly,
those kinds of things. And if it answered wrong,
you would tell it it's wrong. And then it would have
to slightly figure out that you can't walk
across the Channel. HANNAH FRY: So some
of these properties that have emerged that
weren't necessarily expected to be, I want to ask
you a little bit about hype. Do you think that
where we are right now, how things are at this moment,
is overhyped or underhyped? Or is it just hyped, perhaps,
in the wrong direction? DEMIS HASSABIS: Yeah, I
think it's more the latter. So I would say that in the
near term, it's hyped too much. So I think people are claiming
can do all sorts of things it can't. There's all sorts of startups
and VC money chasing crazy ideas that are just not ready. On the other hand, I think
it's still underhyped. HANNAH FRY: Coming
from you, Demis-- DEMIS HASSABIS: Yes, I
know, I know, I know. HANNAH FRY: AI in 2010. DEMIS HASSABIS:
Exactly, exactly. But I think it's still
underhyped or perhaps underappreciated
still even now what's going to happen when we
get to AGI and post-AGI. I still don't feel
like that's people are quite understood how
enormous that's going to be, and therefore, the
responsibility of that. So it's both, really. I think it's a
little bit overhyped in the near term at the moment. We're going through that cycle. HANNAH FRY: I guess, though,
so in terms of all of these potential startups,
and VC funding, and so on, you who have
lived and breathed this stuff for, as you say, decades, are
very well placed to spot which ones are realistic goals
and which ones aren't. But for other people, how
can they distinguish between what's real and what isn't? DEMIS HASSABIS: Yeah, well look,
I think you need to look at-- obviously you've got to do your
technical due diligence, have some understanding
of the technology, and the latest trends. I think also look at, perhaps,
the background of the people saying it, how
technical they are. Have they just arrived in AI
last year from somewhere else? I don't know. They were doing
crypto last year. These might be some clues
that perhaps they're jumping on a bandwagon. And it doesn't mean to
say, of course, they could still have some good
ideas, and many will do. But it's a bit more lottery
ticket like, shall we say. And I think that always happens
when there's a ton of attention suddenly on a place, and
obviously, then the money follows that. And everyone feels like
they're missing out. And that creates a
kind of opportunistic, shall we say, environment,
which is a little bit opposite to those of us
who've been in for decades in a deep technology, deep
science way, which is ideally the way I think we need to
carry on going as we get closer to AGI. HANNAH FRY: Yeah. And I guess one of the
big things that we're going to talk about
in this series is Gemini, which really comes
from that very deep science approach, I guess. In what ways is Gemini different
from the other large language models that are
released by other labs? DEMIS HASSABIS: So from
the beginning with Gemini, we wanted it to be multi-modal
from the start so it could process not just language, but
also audio, video, image, code-- any modality, really. And the reason we wanted
to do that was firstly, we think that's the way to
get these systems to actually understand the world around them
and build better world models. So actually still going
back to our grounding question earlier, still
building grounding in, but piggybacking on top
of language this time. And so that's important. And we also had this
vision in the end of having a universal
assistant, and we prototyped something
called Astro, which I'm sure we'll
talk about, which understands not just what you're
typing, but actually the context you're in. And if you think about something
like a personal assistant or digital assistant, it will
be much more useful the more context it understood about
what you're asking it for or the situation that you're in. So we always thought that would
be a much more useful type of system, and so we
built multi-modality in from the start. So that was one thing,
natively multi-modal. And then at the time, that
was the only model doing that. So now the other models
are trying to catch up. And then the other
big innovations we had are on memory. So long context. So actually holding in mind 1
million-- or 2 million now-- tokens, you can think of them
as more or less like words, in mind. So you can give it "War and
Peace," or even a whole-- because it's multi-modal-- a
whole video now, a whole film, or a lecture, and then get it
to answer questions or find you things within that video stream. HANNAH FRY: OK
Project Astra, that's the new universal AI
agent, the one that can take in video and audio data. At Google. I/O, I think you used
the example of how Astra could help you
remember where you left your glasses, for instance. So I wonder, though,
about the lineage of this stuff because is this
just a fancy, advanced version of those old Google glasses? DEMIS HASSABIS: So, of course,
Google have a long history of developing glass-type
devices actually back to, I think, 2012 or something. So they were way
ahead of the curve. But maybe it was just missing
this kind of technology So you could actually
understand-- a smart agent, or a smart assistant
that could actually understand what it's seeing. And so we're very excited
about that digital assistant to go around with you and
understand the world around you. So it seems a really--
when you use it, it feels a really
natural use case. HANNAH FRY: OK. I want to rewind a tiny
bit to the start of Gemini because it came from
two separate parts of the organization. DEMIS HASSABIS: Yes. So we-- actually, last year
we combined our two research divisions at Alphabet. So obviously, the old DeepMind,
and then Google Brain into one we call it super unit, bringing
all the talent together-- that amazing talent we
have across the company, across the whole of Google,
into one unified unit. And what it meant was that
we combined all the best knowledge that we had
from all the research we were doing, but especially
on language models. So we had Chinchilla, and
Gopher, and things like that, and they were building
things like PaLM, and LaMDA, and early language models. And they had different
strengths and weaknesses, and we pulled them all together
into what became Gemini as the first Lighthouse project
that the combined group would output. And then the other
important thing, of course, was bringing together
all the compute, as well, so that we could, do
these really massive training runs and actually pull the
compute resources together. So it's been great. HANNAH FRY: I guess,
in a lot of ways, the focus of Google Brain and
DeepMind was slightly different. Is that fair to say? DEMIS HASSABIS: Yeah. So I think it was. I mean, we were obviously
focused, both of us, on the frontiers
of AI, and there was a lot of collaborations
already on a individual research level, but maybe not
on a strategic level. Obviously, now the combined
group, Google DeepMind, I describe it as we're the
engine room of Google now. But it's worked really well. I think there were a lot
more similarities, actually, in the way we were working
than there were differences, and we've continued to keep
and double down our strengths on things fundamental research. So where does the next
transformer architecture come from? We want to invent that. Obviously, Google Brain
invented the previous one. We combined it with
deep reinforcement learning that we
pioneered, and I still think more innovations
are going to be needed. And I would back us to
do that just as we've done in the past 10
years collectively, both Brain and DeepMind. So it's been exciting. HANNAH FRY: I want to come
back to that merge in a moment. But I think just
sticking on Gemini for a second, how good is it? How does it compare
to other models? DEMIS HASSABIS: Yeah, well, I
think some of the benchmarks are not-- the problem is
that we need more-- I think this is one thing
the whole field needs is much better benchmarks. HANNAH FRY: Yeah. How do you decide? DEMIS HASSABIS: Well, there
are some well-known benchmarks, academic ones. But they're getting
saturated now, and they don't
really differentiate between the nuances, between
the different top models. I would say there's
three models that are at the top, the frontier. So it's Gemini from us,
OpenAI's, GPT, of course, and then Anthropic with
their Claude models. And then obviously, there's
a bunch of other good models, too, that people like Meta,
and Mistral, and others built, and they're differently
good at different things. It depends what you want-- coding, perhaps that's Claude. And reasoning, maybe that's GPT. And then memory
stuff, long context, and multimodal understanding,
that would be Gemini. Of course, we're
continuing to-- all of us are improving our
models all the time. So given where we started
from, which Gemini as a project only existed for
a year, obviously, based on some of
our other projects, I think our trajectory
is very good. So when we talk next
time, we should hopefully be right at the forefront. HANNAH FRY: Because there
is still a way to go. I mean, there are
still some things that these models
aren't very good at. DEMIS HASSABIS: Yes, for sure. And actually, that's the
big debate right now. So this last set
of things emerged from the technologies that were
invented five, six years ago. The question is, they're still
missing a ton of things-- so their factuality, they
hallucinate, as we know. They also not good
at planning yet. HANNAH FRY: Planning
in what sense? DEMIS HASSABIS: Well,
long term planning. So they can't problem solve. Something long term, you
give it an objective, they can't really do actions
in the world for you. So they're very much
like passive Q&A systems. You put the energy in
by asking the question, and then they give you
some kind of response. But they're not able to
solve a problem for you. You can't say something
like, if you wanted it as a digital assistant, you
might want to say something like, book me that holiday in
Italy, and all the restaurants, and the museums, and whatever,
and it knows what you like, but then it goes out
and books the flights and all of that for you. So it can't do any of that. But I think that's
the next era-- these more agent-based
systems, we would call them,
or agentic systems that have agent-like behavior. But of course, that's
what we're expert in. That's what we used to build
with all our game agents-- AlphaGo and all of
the other things we've talked in
about in the past. So a lot of what we're
doing is marrying that work that we're,
I guess, famous for with the new large
multimodal models. And I think that's going to be
the next generation of systems. You can think of it as
combining AlphaGo with Gemini. HANNAH FRY: Yeah, because I
guess AlphaGo was very, very good at planning. DEMIS HASSABIS: Yes, it
was very good at planning. Of course, only in the
domain, though, of games. And so we need to
generalize that into the general domain of
everyday workloads and language. HANNAH FRY: You
mentioned a minute ago how Google DeepMind is now
the engine room of Google. I mean, that is
quite a big shift since I was last here in the
last couple of years ago. Is Google taking quite
a big gamble on you? DEMIS HASSABIS:
Well, I guess so. I mean, I think Google
have always understood the importance of AI. Sundar, when he
took over as CEO, said that Google was
an AI-first company. And we discussed that very
early on in his tenure, and he saw the potential in
AI as the next big paradigm shift after mobile and internet,
but bigger than those things. But then I think maybe
in the last year or two, we've really started
living what that means-- not just from a
research perspective, but also from products
and other things. So it's very
exciting, but I think it's the right bet for us to
coordinate all of our talents together, and then push
as hard as possible. HANNAH FRY: And then how
about the other way around? Because I guess from DeepMind,
having that very strong research and science focus, does becoming
the engine room for Google now mean that you have to care much
more about commercial interests rather than the
purer stuff that-- DEMIS HASSABIS: Yeah,
well, we do definitely have to worry more
about, and it's in our remit now, the
commercial interests. But actually, there's a
couple things say about that. First of all, we're continuing
on with our science work in AlphaFolds, and you just
saw AlphaFold 3 come out. And we're doubling down
on our investments there. That's, I think, a unique thing
that we do at Google DeepMind now. And even our competitors
point at those things as universal goods, if you
like, that come out of AI. And that's going really well. And we spun out isomorphic
to do drug discovery. So it's very exciting, and
that's all going really well. And so we're going to
continue to do that. And then was all our work on
climate and all of these things. But then, we're
quite a large team, so we can do more than
one things at once. We're also building our large
models, Gemini and et cetera, and then we have a product
team that we're building out that is going to bring all
this amazing technology to all of the surfaces that Google has. So it's an incredible
privilege, in a way, to have that there to
plug in all of our stuff. And we invent something,
it immediately can become useful
to a billion people. And so that's really motivating. And actually, the other thing is
there's a lot more convergence now between the technology we
need to develop for a product to have AI in it and what you
would do for pure AGI research purposes. So there's not really--
five years ago, you'd have had to build some
special case AI for a product. Now, you can branch
off your main research and, of course, you still
need to do some things that are product specific, but maybe
it's only 10% of the work. So there's actually
not that tension anymore between what you would
develop for an AI product and what you would develop
for trying to build AGI. It's 90%, I would say,
the same research program. And then finally, of
course, if you do products, and you get them
out into the world, you learn a lot from that. And people using
it, and you learn a lot about, oh,
your internal metrics don't quite match what
people are saying, so then you can update that. And that's really helpful
for your research. HANNAH FRY: Absolutely. Well, OK, we are going to talk
a lot more in this podcast about those breakthroughs that
have come from applying AI to science, but I want to
ask you about that tension that there is between knowing
when the right moment is to release something
to the public. Because internally at
DeepMind, those tools like large language models
were being used for research rather than being seen as a
potentially commercial thing. DEMIS HASSABIS:
Yeah, that's right. So as you know, we've always
taken responsibility incredibly seriously here, and safety,
right from the beginning, way back when we started
in 2010 and before that. And Google then adopted some
of our, basically, ethics charter effectively into
their AI principles. So we've always been well
aligned with the whole of Google and wanting to be responsible
about deploying this as one of the leaders in this space. And so it's been
interesting now starting to ship real products
with Gen AI in them. Actually there's a lot of
learning that is going on, and we're learning
fast, which is good because we're at
relatively low stakes here with the
current technology. So it's not that powerful yet. But as it gets more powerful,
we have to be more careful. And that's just learning
about the product teams and other groups learning about
how to test Gen AI technologies. It's different from a
normal piece of technology because it doesn't
always do the same thing. It's almost like testing
an open world game. It's almost infinite what
you can try and do with it, so it's interesting to
figure out how do you do the red teaming on it. HANNAH FRY: So red
teaming, in this case, being where you're competing
against yourselves? DEMIS HASSABIS: Yeah. So red teaming is when you set
up a specific separate team from the team that's
developed the technology to stress test it and try and
break it in any way possible. You actually need to use tools
to automate that because nobody can red team-- even if you had
thousands of people doing it, that's not enough compared
to billions of users when you put it out there. They're going to try
all sorts of things. So it's kind of interesting
to take that learning, and then improve our processes
so that our future launches will be as smooth as possible. And I think we got to do
it in stages where there's an experimental phase, then a
closed beta, and then launch-- a little bit,
again, like we used to launch our games
back in the day, and learn at each
step of the way. And then the other
thing we've got to do, and I think we need to do more
on, is use AI itself to help us internally with red
teaming and actually spotting some errors
automatically or triaging that so that, then, our
developers and human testers can actually focus
on those hard cases. HANNAH FRY: You said
something really interesting there about how you're just in
a much more probabilistic space here. And then, if there's even a
very small chance of something happening, if you
have enough tries, eventually, something
will go wrong. And I guess there have
been a couple of mistakes that-- public mistakes. DEMIS HASSABIS: Yeah, so
that's why I think that, as I mentioned, that product
teams are just getting used to the sorts of testing. They tested these
things, but they have this stochastic nature,
probabilistic nature. So in fact, a lot
of cases where if it was a normal piece of
software, you could say I've tested 99.999% of things,
so then extrapolates. So then it's enough
because there's no way of exposing the flaw
that it has if it has one. But that's not the case with
these generative systems. They can do all
sorts of things that are a little bit left
field, or out of the box, out of distribution, in a way,
from what you've seen before if someone clever or adversarial
decides to-- it's almost like a hacker decides to
test push it in some way. And it could even be-- I mean, it's so
combinatorial, it could even be with all the things
that you've happened to have said before to it. And then it's in some
kind of peculiar state which then-- or it's got
its memories filled up with this particular
thing, and then that's why it outputs something. So there's a lot of complexity
there, but it's not infinite. So there's ways to deal with it. But it's just a lot more
nuanced than launching normal technology. HANNAH FRY: I
remember you saying, I think it was in the first
time I interviewed you about how, actually, you have to
think that this is a completely different way of computing. You have to move away from
the things that we completely understand-- the
deterministic stuff-- into this much more messy,
probabilistic error-ridden place, as well as your testers. Do you think the
public slightly has to shift its mindset on the type
of computing that we're doing? DEMIS HASSABIS:
Yeah, I think so, and maybe that's another
thing, interestingly, that we're thinking about
is actually putting out a kind of principles document
or something before you release something to show what is the
expectation from this system. What's it designed for? What's it useful for? What can't it do? And I think there is some sort
of education there needed of, you'll be able to find it useful
if you do these things with it, but don't try and use it
for these other things because it won't work. And I think that
that's something that we need to get better
at clarifying as a field, and then probably users need
to get more experienced on. And actually, this interesting. This is probably why chatbots
themselves came a little bit out of the blue. Even obviously ChatGPT, but even
to OpenAI, it surprised them. And we had our own chat
bots, and Google had theirs. And one of the things was
we were looking at them, and we were looking at all
the flaws they still had, and they still do. And it's like, well, it's
getting these things wrong, and it sometimes hallucinates,
and blah, blah, blah. And there's so many things. But then what we didn't
realize is, actually, there's still a lot of very good
use cases for that even now that people find very valuable--
summarizing documents, and really long
things, or writing-- HANNAH FRY: Awkward emails? DEMIS HASSABIS: --awkward
emails, or mundane forms to be filled in. And there's all these use
cases which, actually, people don't mind if
there's some small errors. They can fix them easily, and
saves a huge amount of time. And I guess that was
the surprising thing. They discovered-- people
discovered when you put it in the hands of everyone, there
were actually these valuable use cases, even though the systems
were flawed in all of these ways we know. HANNAH FRY: Well, OK, so I
think that sort of takes me on to the next
question I want to ask, which is about open source. Because when things are
in the hands of people, as you mentioned, really
extraordinary things can happen. And I know that
DeepMind in the past has open sourced lots of
its research projects, but it feels like
that's slightly changing now as we go forward. So just tell me what your
stance is on open source. DEMIS HASSABIS: Yeah. Well, look, we're
huge supporters of open source and open
science, as you know. I mean, we've given
away and published almost everything we've done,
collectively, including like, things like transformers,
and AlphaGo. We published all these things
in "Nature" and "Science." AlphaFold was open source,
as we covered last time. And these are all good choices,
and you're absolutely right. That's the reason
that all works is because that's the way
technology and science advances as quickly as possible,
by sharing information. So almost always,
that's a universal good to do it like that, and
that's how science works. The only exception is when you-- and AGI and powerful
AI does fall into this-- is when you have
a dual purpose technology. And so then, the
problem is that you want to enable all the
good use cases and all the genuine scientists who are
acting in good faith and so on, technologists, to build
on the ideas, critique the ideas, and so on. That's the way society
advances the quickest. But the problem is how
do you restrict access at the same time for bad actors
who would take the same systems, repurpose them for bad
ends, misuse them-- weapon systems, who knows what? And those general
purpose systems can be repurposed like that. And it's OK today
because I don't think the systems are that powerful. But in two, three,
four years time, especially when you start
getting agent-like systems or agentic behaviors,
then, I think, if something's misused by
someone, or perhaps even a rogue nation, state,
there could be serious harm. So then, I don't have
a solution to that. But as a community, we need
to think about what does that mean for open source? Perhaps the frontier models need
to have more checks on them, and then only after they've been
out for a year or two years, then they can get open sourced. That's the model we're
following because we have our own open models
of Gemini called Gemma because they're smaller. So they're not frontier models. So their capabilities are very
useful still to the developer because they're also
easy to run on a laptop because they're small
numbers of parameters. But the capabilities
they have are well understood at this point. Because they're not
frontier models. So it's just not as
powerful as the latest, say, Gemini 1.5 models. So I think that's
probably the approach that we'll end up taking is
we'll have open source models, but they'll be lagging maybe
one year behind the most cutting edge models just so
that we can really assess out in the open by users what those
models can do-- the frontier ones can do. HANNAH FRY: And you
can really, I guess, test those boundaries
of the stochastic-- DEMIS HASSABIS: Yeah, and
we'll see what those are. The problem with open source
is if something goes wrong, you can't recall it. With a proprietary
model, if your bad actor starts using it in a bad way,
you can just close the tap off. In the limit, you
could switch it off. But once you open
source something, there's no pulling it back. So it's a one way door, so
you should be very, very sure when you do that. HANNAH FRY: Is it
definitely possible to contain an AGI,
though, within the walls of an organization? DEMIS HASSABIS: Well, that's
a whole separate question. I don't think we know
how to do that right now. So when you start talking
about AGI level powerful, like human level AI-- HANNAH FRY: Well, what
about intermediary? DEMIS HASSABIS: Well,
intermediary, I think, we have good ideas
of how to do that. So one would be things
like secure sandboxing. So you test--
that's what I'd want to test the agent behaviors
in is in a game environment, or a version of
the internet that's not quite fully connected. So there's a lot
of security work that's done and known in
this space, and in fintech, and other places. So we'd probably
borrow those ideas, and then build those
kinds of systems. And that's how we would test
the early prototype systems. But we also know
that's not going to be good enough to contain
an AGI, something that's potentially smarter than us. So I think we got to understand
those systems better so that we can design the
protocols for an AGI. When that time comes,
we'll have better ideas for how to contain
that, potentially also using AI systems and tools
to monitor the next versions of the AI system. HANNAH FRY: So on the
subject of safety, because I know that you are a
very big part of the AI Safety Summit at Bletchley Park in
2023, which was, of course, hosted by the UK government. And from the outside,
I think a lot of people just say the word regulation
as though it's just going to come in and fix everything. But what is your view on how
regulation should be structured? DEMIS HASSABIS: Well,
I think it's great that governments are getting
up to speed on it and involved. I think that's one
of the good things about the recent explosion of
interest is that, of course, governments are
paying attention. And I think it's been great. The UK government
specifically, who I've talked to a
lot, and US, as well, they've got very smart
people in the civil service staff that understand the
technology now to a good degree. And it's been great to see
the AI safety institutes being set up in the
UK and US, and I think many other countries
are going to follow. So I think these are all
good precedents and protocols to settle into, again, before
the stakes get really high. So this is a proving
stage, again, as well. And I do think international
cooperation is going to be needed, ideally around
things like regulation, and guardrails, and
deployment norms. So because AI is a digital
technology, very much so, it's hard to contain it
within national boundaries. So if the UK or Europe does
something, or even the US, but China doesn't, does
that really help the world? When we start getting
closer to AGI, not really. So I think my view
on it is you've got to be, because the
technology is changing so fast, we've got to be very nimble and
light-footed with regulation so that it's easy to adapt it to
where the latest technology is going. If you'd regulated
AI five years ago, you'd have regulated something
completely different to what we see today, which is Gen AI. But it might be different
again in five years. It might be these
agent-based systems that are the ones that
carry the highest risk. So right now, I would recommend
to beef up existing regulations in domains that already have
them-- health, transport, so on. I think you can update
them for an AI world just like they were updated
for mobile and internet. That's probably
the first thing I'd do, while doing a watching brief
on making sure you understand and test the frontier systems. And then as things become
clear and more clearly obvious, then start regulating
around that. Maybe in a couple of years
time would make sense. One of the things we're missing
is, again, the benchmarks-- the right tests for
capabilities that-- what we'd all want
to know, including the industry in the field, is
at what point are capabilities posing some big risk? And there's no answer to that at
the moment beyond what I've just said, which is agent-based
capabilities is probably our next threshold. But there's no
agreed-upon test for that. One thing you might imagine
is testing for deception, for example, as a capability. You really don't want
that in the system because then you can't
rely on anything else that it's reporting. So that would be my number one
emerging capability that I think would be good to test for. But there's many-- ability
to achieve certain goals, the ability to replicate. And there's quite a lot of
work going on on this now. And I think the safety
institutes, which are basically government agencies, I think
it would be great for them to push on that, as well. As well as the labs, of course,
contributing what we know. HANNAH FRY: I wonder, in
this picture of the world that you're describing, what's
the place for institutions in this? I mean, if we get
to the stage where we have AGI that's supporting
all scientific research, is there still a place
for great institutions? DEMIS HASSABIS:
Yeah, I think so. There's the stage
up to AGI, and I think that's got
to be a cooperation between civil society,
academia, government, and the industrial labs. So I really believe
that's the only way we're going to get to
the final stages of this. Now, if you're asking
after AGI happens, maybe that is what you're asking,
then AGI, of course, one of the reasons I've always
wanted to build it is then we can use it to start
answering some of the biggest, most fundamental
questions about the nature of reality, and physics,
and all of these things, and consciousness, and so on. It depends what form
that takes, whether that will be a human-expert
combination with AI. I think that will be the
case for a while in terms of discovering
the next frontier. So like right now, these
systems can't come up with their own
conjectures or hypotheses. They can help you
prove something, and I think we'll
be able to prove get gold medals on
international maths Olympiad, things like that. But maybe even solve
a famous conjecture. I think that's within reach now. But they don't have
the ability to come up with Riemann hypothesis
in the first place, or general relativity. So that, really,
was always my test for maybe a true artificial
general intelligence is it will be able to
do that, or invent Go. And so we don't
have any systems. We don't really know how we
would design, in theory, even, a system that could do that. HANNAH FRY: You know
the computer scientist, Stuart Russell? So he told me that he was a bit
worried that once we get to AGI, it might be that we all
become like the royal princes of the past-- the ones who never had to ascend
the throne or do any work, but just got to live this
life of unbridled luxury and have no purpose. DEMIS HASSABIS: Yeah, so that is
the interesting question, is it? Maybe it's beyond AGI. It's more like artificial
superintelligence or something-- sometimes people call it ASI. But then we should
have radical abundance. And assuming we make sure
we distribute that fairly and equitably, then we
will be in this position where we'll have more
freedom to choose what to do. And then meaning will be a
big philosophical question. And I think we'll
need philosophers, perhaps theologians
even, to start thinking as social scientists. They should be thinking
about that now. What brings meaning? I mean, I still think there's,
of course, self-actualization, and I don't think we'll all just
be sitting there meditating. But maybe we'll be
playing computer games. I don't know. But is that a bad
thing even, or not? Who knows? HANNAH FRY: I don't think
the princes of the past came off particularly well. DEMIS HASSABIS: No. Traveling the stars. But then there's also
extreme sports people do. Why do they do them? I mean, climb
Everest, all these. I mean, there'll
be-- but I think it's going be very interesting. And that I don't
know, but that's what I was saying
earlier about it's underappreciated what's going
to happen going back to the hype near-term versus far-term. So if you want to
call that hype, even, it's definitely
under hyped, I think, the amount of
transformation that will happen. I think it will be
very good in the limit. We'll cure lots of diseases,
and/or all diseases, solve our energy problems,
climate problems. But then the next question
comes is, is there meaning? HANNAH FRY: So bring us
back slightly closer to AGI rather than superintelligence. I know that your big mission is
to build artificial intelligence to benefit everybody,
but how do you make sure that it does benefit everybody? How do you include all
people's preferences rather than just the designers? DEMIS HASSABIS:
Yeah, I think what's going to have to happen is-- I mean, it's impossible
to include all preferences in one system. Because by definition,
people don't agree. We can see that
in, unfortunately, in the current
state of the world. Countries don't agree. Governments don't agree. We can't even get
agreement on obvious things like dealing with the
climate situation. So I think that's very hard. What I imagine will
happen is that we'll have a set of safe
architectures, hopefully, that personalized
AIs can be built on top of. And then everyone will
have, or different countries will have their own preferences
about what they use it for, what they deploy it for, what
can and can't be done with them. But overall-- and that's fine. That's for everyone
to individually decide or countries to
decide themselves, just like they do today. But as a society,
we know that there's some provably safe things
about those architectures. And then you can let them
proliferate, and so on. So I think that we've got to
get through the eye of a needle in a way where as we
get closer to AGI, we've probably got to cooperate
more, ideally internationally, and then make sure we build
AGIs in a safe architecture way. Because I'm sure
there are unsafe ways, and I'm sure there are
safe ways of building AGI. And then once we
get through that, then we can open
the funnel again, and everyone can have their
own personalized pocket AGIs, if they want. HANNAH FRY: What a
version of the future. But then, in terms of
the safe way to build it, I mean, are we talking
about undesirable behaviors here that might emerge? DEMIS HASSABIS: Yes, undesirable
emergent behaviors, capabilities that-- HANNAH FRY: Deception. DEMIS HASSABIS: Deception is
one example that you don't want. Value systems. We got to understand
all of these things better-- what kind of guardrails
work, not circumventable. And there's two
cases to worry about. There's bad uses by bad
individuals or nations, so human misuse, and then
there's the AI itself as it gets closer to
AGI going off the rails. And I think you need different
solutions for those two problems. And so, yeah, that's
what we're going to have to contend
with as we get closer to building these technologies. And also, just going back to
your benefiting everyone point, of course, we're showing
the way with things like AlphaFold and isomorphic. I think we could cure most
diseases within the next decade or two if AI drug design works. And then they could be
personalized medicines where it minimizes the side
effects on the individual because it's mapped
to the person's individual illness, and
their individual metabolism, and so on. So these are amazing things-- clean energy, renewable
energy sources, fusion, or better solar power,
all of these types of things. I think they're
all within reach. And then that would
sort out water access because you could do
desalination everywhere. So I just feel like
an enormous good is going to come from
these technologies, but we have to mitigate
the risks, too. HANNAH FRY: And one
way that you said that you would want
to mitigate the risks was that there would be a
moment where you would basically do the scientific version
of Avengers assemble. DEMIS HASSABIS: Yes, sure. HANNAH FRY: Terence Tao,
get him on the phone. DEMIS HASSABIS: Exactly. HANNAH FRY: Bring him on down. DEMIS HASSABIS: Yeah, exactly. HANNAH FRY: Is that
still your plan? DEMIS HASSABIS: Yeah,
well, I think so. I think if we can get the
international cooperation, I'd love there to be a
international CERN, basically, for AI. Where you get the top
researchers in the world, and you go, look, let's focus on
the final few years of this AGI project, and get
it really right, and do it scientifically,
and carefully, and thoughtfully at every step-- the final steps. I still think that
would be the best way. HANNAH FRY: How do you know when
is the time to press the button? DEMIS HASSABIS: Well,
that's the big question. Because you can't do it
too early because you would never be able to
get the buy-in to do that. A lot of people would disagree. I mean, today people
disagree with the risks. You see very famous people
saying there's no risks, and then you have people
like Jeff Hinton saying there's lots of risks. And I'm in the middle of that. HANNAH FRY: I want
to talk to you a bit more about neuroscience. How much does it still
inspire what you're doing? Because I noticed the other
day that DeepMind had unveiled this computerized rat
with an artificial brain that helps to change
our understanding of how the brain controls movement. But in the first
season of the podcast, I remember we talked
a lot about how DeepMind takes
direct inspiration from biological systems. Is that still the
core of your approach? DEMIS HASSABIS: No,
it's evolved now because I think we've
got to a stage now. In the last, I would
say, two to three years, we've gone more into
an engineering phase-- large scale systems, massive
training architectures. So I would say that the
influence of neuroscience on that is a little bit less. It may come back in. So any time where you
need more invention, then you want to get as
many sources as possible. And neuroscience would be one
of those sources of ideas. But when it's more
engineering heavy, then I think that takes a
little bit more of a backseat. So it may be more applying
AI to neuroscience now like you saw with the
virtual rat brain. And I think we'll see that
as we get closer to AGI using that to
understand the brain. I think it would be
one of the coolest use cases for AGI and science. HANNAH FRY: I guess
this stuff goes through phases of the
engineering challenge, the intervention challenge. DEMIS HASSABIS: So it's
done its part for now, and it's been great. And we still obviously
keep a close track of it and take any other ideas, too. HANNAH FRY: OK. All of the pictures of the
future that you've painted are still anchored
quite in reality. But I know that you've
said that you really want AGI to be able to peer into
the mysteries of the universe down at the Planck scale. DEMIS HASSABIS: Yes! HANNAH FRY: Like
subatomic, quantum worlds. Do you think that there are
things that we have not even yet conceived of that might
end up being possible? I'm talking wormholes here. DEMIS HASSABIS: Completely, yes. I'd love wormholes
to be possible. I think there is a lot of
probably misunderstanding, I would say, still things we
don't understand about physics and the nature of reality. And obviously, the quantum
mechanics, and unifying that with gravity, and
all of these things, and there's all these problems
with the standard model. So I think there's-- and string theory, I
mean, I just think-- HANNAH FRY: There's giant
gaping holes in physics. DEMIS HASSABIS: Yes, in
physics, all over the place. And I talk to my physics
friends about this, and there's a lot of things
that don't fit together. I don't really like the
multiverse explanation. So I think that it would
be great to come up with new theories and then test
those on massive apparatus, perhaps out in space,
at these tiny-- the reason I'm obsessed
with Planck scale things-- Planck time, Planck space-- is because that seems to be the
resolution of reality in a way. That's the smallest quanta
you can break anything into. So that feels like
the level you want to experiment on if you had
powerful apparatus perhaps designed or enabled by having
AGI and radical abundance. You would need both to be
able to afford to build those types of experiments. HANNAH FRY: The
resolution of reality. What a phrase. What, so as in the resolution
that we're at the moment? Human level is just an
approximation of reality. DEMIS HASSABIS:
Yes, that's right. And then we know there's
the atomic level, and below that is the Planck
level, which as far as we know is the smallest resolution one
can even talk about things. And so that, to me,
would be the resolution one wants to experiment
on to really understand what's going on here. HANNAH FRY: I wonder
whether you're also envisaging that there
will be things that are beyond the limits
of human understanding AGI will help us to uncover-- that actually, we're just not
really capable of understanding. And then I wonder if
things are unexplainable or ununderstandable, are
they still falsifiable? DEMIS HASSABIS: Yeah
well, look, I mean, these are great questions. I think there will be a
potential for an AGI system to understand higher level
abstractions than we can. So again, going back
to neuroscience, we know that it's your
prefrontal cortex that does that. And there's up to about six or
seven layers of indirection one could take. This person's
thinking this, and I'm thinking this about that person
thinking this, and so on. And then we lose track. But I think an AI system could
have an arbitrarily large prefrontal cortex effectively. So you could imagine
higher levels of abstraction and
patterns that it will be able to see
about the universe that we can't really comprehend
or hold in mind at once. And then I think in terms of
explainability point of view, the way I think
that is a little bit different to other philosophers
who've thought about this, which is we'll be closer to an ant and
then the AGI, in terms of IQ. But I don't think that's
the way to think of it. I think it's we are
Turing complete, so we're a full general
intelligence as ourselves, albeit a bit slow because
we run on slow machinery. And we can't infinitely
expand our own brains. But we can, in theory, given
enough time and memory, understand anything
that's computable. And so I think it will be more
like Garry Kasparov or Magnus Carlsen playing an
amazing chess move. I couldn't have come up with it,
but they can explain it to me why it's a good move. So I think that's what an AGI
system will be able to do. HANNAH FRY: You said that
DeepMind was a 20-year project. How far through are we? Are you on track? DEMIS HASSABIS: I think we're
on track, yeah, crazily. Because usually 20-year
projects stay 20 years away. But yeah, we're a good way
in now, and I think we're-- HANNAH FRY: 20 years
is 2030 for AGI. DEMIS HASSABIS: 2030, yeah. So I think the way I say
is I wouldn't be surprised if it comes in the next decade. So I think we're on track. HANNAH FRY: That matches
what you said last time. You haven't updated your priors. [LAUGHTER] DEMIS HASSABIS: Exactly HANNAH FRY: Amazing. Demis, thank you so much. Absolute delight. Absolute delight, as always. DEMIS HASSABIS: Very fun to
talk, as always, as well. Thank you. HANNAH FRY: OK, I think there
are a few really important things that came out of that
conversation, especially when you compare it to what
Demis was saying last time we spoke to him in 2022. Because there have definitely
been a few surprises in the last couple of years. The way that these
models have demonstrated a genuine conceptual
understanding is one-- this real world grounding
that came in from language and human feedback alone. We did not think
that would be enough. And then how interesting
and useful, imperfect AI has been to the everyday person. Demis himself there
admitted that he had not seen that one coming. And that makes me wonder
about the other challenges that we don't yet
know how to solve like long-term planning, and
agency, and robust, unbreakable safeguards. How many of those-- which we're going to cover
in detail in this podcast, by the way-- are we going to come back
to in a couple of years and realize that they were
easier than we thought? And how many of them
are going to be harder? And then as for
the big predictions that Demis made, like cures for
most diseases in 10 or 20 years, or AGI by the end of
the decade, or how we're about to enter
into an era of abundance, I mean, they all sound like
Demis is being a bit overly optimistic, doesn't it? But then again, he hasn't
exactly been wrong so far. You've been listening to "Google
DeepMind, the Podcast" with me, Professor Hannah Fry. If you have enjoyed this
episode, hey, why not subscribe? We've got plenty more
fascinating conversations with the people at
the cutting edge of AI coming up on topics ranging
from how AI is accelerating the pace of
scientific discoveries to addressing some
of the biggest risks of this technology. If you have any feedback, or you
want to suggest a future guest, then do leave us a
comment on YouTube. Until next time. [MUSIC PLAYING]