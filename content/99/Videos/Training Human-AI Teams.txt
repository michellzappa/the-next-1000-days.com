great well uh we W warmly welcome Hussein mosar who is a PhD student at MIT advised by David sag research focuses on using AI to help people complete tasks more efficiently uh please go ahead all right thank you sirg um welcome everyone so yeah I'm Hussein I'm PhD C at MIT and today I'm going to talk about training human ey thems and please feel free to interrupt me with questions I'll I'll try to monitor the chat if I if there's a question on chat please let me know Sergey and I can try to answer it on the spot and I'll try to leave a few minutes at the end for questions okay all right so let's get started my vision is that in the very near future almost most of decisions we take or even actions will be either assisted or performed by an AI agent and this is already starting to happen in a lot of domains so in particular AI is being used to assist radiologist for breast cancer detection among other Radiology tasks large language models are really changing the way we write code with popular products like GitHub co-pilot being used to assist programmers inside their uh integrated uh editors and this is really a major shift in how humans perform their works and my goal as a researcher is to try to maximize ai's benefit to humans and the way I try to do this is by modeling the human and thei as a team as single entity and try to directly optimize for performance so we want to think of the human and the ey not as two separate things but as a single team and then optimize directly for that performance so to drive home this point really the main motivation behind this is that there's a fundamental mismatch in the way we opt optimize AI models and how they are deployed currently so this bar is showing you the level of current human AI performance the level of the steam at making decisions and really the current state is only very optimizing the potential only halfway through and this is because the way AI models are trained and integrated is without considering that there's a human in that Loop and this is causes many failure points including things like over Reliance or under Reliance so not properly using thei to its current full ability and to really bridge that Gap and get that potential that's left for us to optimize we really need to train the team with this human AI performance objective directly so to make this things more concrete we going to draw things in a figure so I have a human and they're performing their work on the computer I can train an AI model that is with the objective of mimicking the human on their task the CI model can be used to provide suggestions to the human and these suggestions can get displayed directly in the human's interface so again this humanite team is this integration of the human Ai and the interface in which they communicate with them and this clearly present three opport unities to optimize this tee the interface the human and AI so in this talk I'll talk about three opportunities to optimize the human itam the first one being is how can we train the interface and adapted from feedback of the human interacting with the model the second opportunity is training the human itself to better know how to interact with thei model to make sure the human's mental model of thei is accurate and the third opportunity is training AI models that complement rather than mimic human behavior and this talk is going to focus on all of these three things so just as a helpful outline of today we're going to talk about these three directions and for reference each of these directions is based on the following papers published in machine learning and HCI conferen so let's start with the first part of our talk it's going to be about adapting interfaces from Human feedback the main tool I'm going to use today is modeling the interaction explicitly so to make any progress we need a model of how the human is interacting with AI once we have that model of the interaction we can figure out what's wrong and use that model as a basis to design interventions to improve their inefficiencies so explicitly our approach is going to be to build a taxonomy a set of different human activities where the human is interacting with the eye once we have that taxonomy that list of states we can model the human as kind of a stochastic process over these states and that's going to enable us to find out what's going wrong and try to fix things and we're going to use a concrete application to illustrate this approach so specifically I'm going to focus on GitHub co-pilot so co-pilot is a large language model that's integrated into your code editor and at each moment in time in which you stop writing code copile is going to show you a suggestion that's going to be engra and you can accept that suggestion into your code by pressing tab or rejected by continuing to write code or pressing the Escape key and copile 3D is one of the most used llm software as of now and I think one of the most useful softwares and it's really used by millions of programmers on a daily basis the first burning question is is copilot useful and one of the best evidence for this comes from the copilot team itself and they found with a randomized control trial that programmers writing code with copilot versus without are 55% faster and really all of the state of AI development right now is about making models bigger and larger and we think that by making these models bigger hopefully we can improve human productivity so to try to investigate that question of if we keep making the ey larger where are we heading I ran my own study with collaborators at CMU where we built our own version of cop added but instead of using the gpt3 model we swapped in the llm with three different models of increasing capabilities so the first one is cod Lama 7 billion the second is cod Lama 34 billion and the third is g3.5 now Cod Lama 34 billion is much better at all of these static benchmarks marks like human eval then Cod Lama 7B and GP 3.5 as well so these are models of increasing size and abilities on these programming benchmarks but once we try to understand how humans are interacting with these models for example we find that with Cod Lama 7B people are only 7.5 faster versus no AI at all then code with Cod Lama 34b people are 90% faster than without AI but once you move to this much bigger mod hypothetically much bigger model but at least much better performing at all of these benchmarks you don't get any Improvement in terms of human productivity so really even if GP 3.5 is twice as good at these benchmarks it's really not much helpful to humans so maybe the path right now is not to optimize the eye to be as good as it can on its own but rather thinking about this human ey team and optimizing for the performance and we might have better opportunities to improve human productivity there so another question is how can we improve calet without improving the AI model so again to do this we need to understand how people are using it and try to build that taxonomy I alluded to so to understand this use I'm going to play for you a video here on the left of me trying to call this regation function and on the right I'm going to try to track my behavior with a state diagram so now as the video starts playing on the left I'm writing code and I'm trying to rewrite code for L regression function so I'm in the state purple State writing new code new functionality as I'm writing copile is interrupt me and showing me a suggestion but once it shows me the suggestion I'm not thinking about the suggestion it's showing me I'm still thinking what I want my function signature to be so I transition to the state of thinking about code I want to write and I quickly reject that suggestion and go back to writing my function signature and once I finished writing my function signature copil is now going to show me suggestion but here clearly it's not useful at all it's just an empty suggestion and so I'm going to verify it and and see that's not useful for me and then reject it and then I'm going to try to explicitly WR a prompt telling co-pilot that oh I want you to initialize the weight and bies parameters of the model so I transition to this purple state of prompt crafting after I write my prompt I'm going to go back and verify the suggestion and here in this setting I'm going to accept it and then I'm going to go in this Loop of waiting for suggestions verifying them and accepting them as I'll show in the video so here I get this so kind of a sequence of accepts that I perform until there comes a point where I don't like the suggestion so I'm going to go back to writing a prompt and then cile is going to show me a huge suggestion and it's basically the optimization Loop of the code however it's too it's too long for me to quickly verify so what I will do is I'm going to defer my verification for later I'm going to accept the suggestion without verifying it after I accept it I'm going to go ahead and verify it kind of you can see my cursor L my on going through it and seeing if there's something I don't like and then editing it so kind of this video really tries to show you how people are behaving with Cil and there's many insights that we're going to try to more formally describe you know for instance the fact that I accepted or rejected suggestion is not indication of my preference for that suggestion it depends also on my latent State what was I thinking what was I doing when I was shown that suggestion and you can also see the different types of activities that I'm doing when i'm coding so to formalize these insights in a more structured manner we derived this taxonomy which we called Cups short for co-pilot user programming States and really think of it as a profiler for the interaction so it consists of these 12 12 unique latent states that include things that I just showed you like verifying suggestions on the top prompt crafting in purple in the kind of bottom right edting suggestions and kind of my favorite state the not thinking State and so really to better understand um Behavior we collected data of 21 programmers at Microsoft so they write code for a given task then they try to self-label their coding session as we did in the exercise just previously with these cup state so they have their video of their coding and they label fragments of that interaction with what cup state is most appropriate and that gives us data of labeled coding sessions with according cups labels and perhaps the biggest Insight we can derive from this is understanding where do programmers Ender time so on this graph I'm showing you on the left for each of the 12 unique cup States and I'm highlighting in light green those which are co-pilot interaction States like verifying suggestions adding suggestions prompt crafting the fraction of a coding session that is spent in that state so you can see kind of the top state is that of verifying suggestion so almost 22% of your time coding with copilot and that user study is just verifying the suggestions that you get shown it's not surprising to see that the second most common state is writing code and the third most common state is thinking about what you want to write and so it's kind of a really change in the type of activities that we're performing and the amount of time we're spending doing them once we dug deeper into this data we we also found that programmers only accepted 21% of suggestions so if you do the r calculus you have 22% of your time going into verifying suggestions and that time goes into 80% of suggestions that get rejected and so if I multiply these two numbers together I can get the time that spent verifying rejected suggestions and that's at most probably time that's going to waste but it's going because it's going into verifying stuff that you never use inside your code you may argue that sometimes you reject suggestions but can still gain value from them and that's why I'm saying it's at most that time please feel free to interrupt me since we're a small group with questions at any time and I'm happy to answer and so now given this number we can clearly see there's an inefficiency in the interaction and how can we remedy this well the main design decision in building a system like copilot is when do you interrupt the user and show a suggestion it's basically copilot is a large language model and on top of it is should be this logic of when do you optimally drop the user and intuitively the way you would Design This is as follows you want to see if showing the suggestion would help the user in completing whatever task they're doing faster so if I don't note by the function T the time to complete the task and I try to figure out what's the time to complete the task if the suggestion was not shown versus if the suggestion was shown and if I take this difference and compare it to some threshold tow that's going to give me a way to control when suggestions are shown in a very fundamental manner so to illustrate this let's see to extreme of setting this threshold tab if I set to to minus infinity that means I don't care if discression helps you in the task I'm still going to show it so basically this equation is always showing a suggestion and you can clearly see on this video you get this very annoying interaction where the model is is constantly interrupting you and so that's not how you want to design this let's see the Other Extreme where you set to equal to zero basically you only show a suggestion if it reduces the time for you to code and here co-pilot is going to wait for me to write my function signature and only after I'm done writing that function signature it's going to go ahead and interrupt me with suggestion and in theory that should have a higher probability of me using that suggestion and being useful for me so how do we design an interface that can accomplish this unfortunately if you really want to estimate that you know time difference if something was shown this really a counterfactual estimation problem and you need counter factual estimators and you need a lot of this cups data that tells you about the time that people you know users take and really requires tons of that data so instead we're going to think about a slightly different design decision which is to consider the probability of the user accepting a suggestion given supposingly if it was shown and the way I'm going to motivate this is as follows if you assume that you only gain value from suggestion if you accept it then a necessary condition for it to be useful is for you to accept it so if you show suggestions that have a high probability of acceptance user accepting them then hopefully they're going to be useful to the user so we're going to try to build an estimator of given the suggestion given the context that the user is in the code they've written how long they've been writing code for their probability of accepting assertion suggessions and comparing that to a thre tab now how do we build such an estimator well what we're going to do is we're going to collect a lot of raw Telemetry data from 500 Microsoft users that's going to Total about 5,000 coding sessions with 200k suggestions and for each suggestion that appears in the data we're going to extract features about the suggestion itself so it's length the number of lines that suggestion an embedding of that suggestion features about the prompt that's shown in blue so how long was the prompt you know occurrence of certain keywords and embedding and features about the suggestion the session itself so you know the file you're writing code in what were your previous five action so that's showing in green and I'm going to take all of these features concatenate them and I'm going to feed them into an XJ boost model that strain on this binary signal of accept and reject and so what we can show is that with this model we can achieve that interface intervention where we can selectively figure out which suggestion we want to show and which we want to hide and so in particular we can have 30% of suggestions that were shown in retrospective evaluation and guarantee that 95% of them would have been rejected rejected if they were show so on this graph on the right I'm showing you as I vary the parameter Tow on the Y AIS I'm showing you how many of the suggestions I can hide and for each parameter setting to I know how many of those that I hid were actually rejected or not so if I set to to be a very high value I'm going to go to the bottom right where I'm going to hide the small fraction of these suggestions but I'm going to guarantee that most of them would have been rejected if they were sh and so this was work that I did that microsof soft which owns GitHub and it got implemented in co-pilot in lightweight version and the specific setting of that parameter enabled them to eliminate 4.5% of suggestions and almost most of them would have gotten rejected so really the takeaways here is that we can understand the interaction by building this taxonomy of states and it allows us to identify inefficiencies and we can use feedback data of the interaction to fix these inici icies so that kind of concludes the first part of my talk and now we're going to try to transition to understanding how do people know how to interact with the model basically the users how do the users even know when to accept the suggestion that's kind of something we skipped over how do they know when to accept the suggestion or reject the suggestion um can I ask a Qui question about the first part yes um you had something about uh prompt writing as one of your categories yes did you measure prompt rewriting when you get a suggestion you don't like it you rewrite the prompt I do that a lot myself that's also a kind of waste of time yeah that's a great uh that's a great question so we have in our paper kind of the sequences of behaviors that people do and one of the most prominent ones is this kind of loop of you write a prompt you get a suggestion you don't like it you go back to prompt crafting you verify and then you don't like it and you kind of keep iterating uh and so we do identify that as like a sequence of states right so kind of look at you know end grams of States and what we show in kind of one of the paper is that sometimes when you when you go through this effort of writing prompt Ting writing prompt you almost surely eventually accept something maybe not because it's useful because you spend all that effort writing your prompt so one of the challenges here is if you let's say your model correctly identifies that a certain prompt I wrote I'm very likely to reject the output then you don't show the output so I can't rewrite my prompt anymore so you have these counterfactual cascading effects how do you deal with that that's a super that's super interesting question so here we're now thinking about this game of you know you know how initially the model behaves to you and if I built my estimator correctly I should know that whenever you rewrite your prompt that you want a suggestion to be shown so that's kind of a limitation of this approach where if you only look at what gets s accepted then that's not the correct optimization if you also had another signal that says this is when people request suggestions then you should also incorporate that into the model um but eventually I think that specific case you have where people are requesting there is a shortcut in co-pilot where you can just request suggestion and so that in theory should fix it in the short term with in long term we need something better than just you know predicting acceptance and using other these signals like the state you are in to know when you display a suggestion does that make a bit of sense it does I mean I use that shortcut myself it's just a it's it's like a last resort like where is it where's my thing and then exactly so that's why you know just using that signal is not good enough and you need that Insight of what was the user state to know when to make this decisions thanks all right so now we're going to transition into slightly different setting of AI assisted decision- making instead of co-pilot and so in this setting AI is going to provide an answer and the human is making a one shot decision of either using the eyes answer or making the diagnosis on of their own so here I have chest x-ray and the human can make an immediate diagnosis of saying this patient is healthy or has pneumonia and the ey is making a prediction that the patient is healthy and providing a confidence score and there's a button that says you can use the eye answer and basically how does the user know when to use this button or not first I'm going to show you a figure that's adapted from Kristof G it's going to show you what would a word look like where the user knows exactly when to rely on the eye and when not to so suppose I have the human at this level of accuracy suppose the eye is surpassing that level you would expect if I put the human and the eye together to get something that's better that's what a word would look like where the human knows exactly when to r on the AI but I think maybe as many of you are familiar that's not often what happens in reality we often get situations where this human ey theme is performing worse than the best of the human or theyi and one of the most recent example of this is is in a study by agal where they had radiologist predicting within without the eye and even though the eye model was better than two-thirds of these radiologist radiologist performance did not increase when you gave them the ey model so even if the blindly all ideologist used the ey they would have done better they were not doing that and it was not the case that they were never using it they were just using it in suboptimum ways in different settings and this is sadly even become a conjecture in lure with people you know stating things like the performance of the human itam is worse than the maximum of they are the human and really what I want to try to see is how I can I flip that in quality how can I ensure that the human I is better than either they are the human and the way we're going to do this is not by optimizing the eye because we know the ey is may be better than the human it's by optimizing the human here and trying to teach them when you should and when you should not rely on the eye ideally what I want to create is a set of natural language rules that describe how should the person interact with the AI and once I've created these rules I can teach them to the person and hopefully the person is convinced of them and they can use these rules in the future and kind of update their mental model and appropriately rely on the AI so how can we get to a state where we have these natural language rules which are nicely written uh for the human so concretely I'm going to consider very miror task to the Radiology but uh where basically I'm asking people is there a traffic light in this picture and the picture is very blurry and so it's kind of very hard for people and for thei and thei is going to provide you know a bounding box if it sees where the traffic light is and it's also going to give them a confidence for so concretely I'm going to formalize things I'm going to formalize this decision of the human to rely on or not rely on the ey by this function I call integrator so this is the humans mental model it's a function R takes his input X the image and a which is the AI suggestion so that in that traffic light I think it's going to be yes there is a traffic light with this percent probability and the human can either decide two things either they ignore thei or they use the ai's answer so either zero or one and so this is kind of an abstraction and here I'm going to make an assumption of how this function looks like how the mental model the human looks like I'm going to assume it decomposes into a prior R subscript zero and this prior mental model can be arbitrary suppose you know the human always relied on the eye or always ignored the eye but it can take arbitrary shape on each simple example and I'm going to assume as a human observes data as they kind of interact to the model they're going to update that prior so they're going to have a posterior is going to consist of local updates in around points that they observe in an a meding space so kind of suppose the human sees this specific example zero um as a specific with specific input and specific AI suggestion and here they figure out that I should ignore thei model on this specific example and so kind of they update that prior in a local region around this point they observe and okay then this region I'm going to ignore the ey and as they observe more points they're going to patch their prior with the data they're learning and so basically my goal is to get the humans integrator and shape their posterior to a better um end end outcome so the way I will do this is I'm going to first try to understand what would an optimal human do how would they optimally rely on on the AI so I'm going to assume I have a data set of examples and I'm going to try to label each point in my data set with a zero or one whether the human should rely or ignore the on this specific example so I have my data set I have labeled it with the optimal decision that a a theoretical human should make and basically onboarding is going to train this person to follow what the optimal integrator would do and so kind of the first step if you want to teach someone you want to understand where they're starting from they're prior and so here I'm just going to assume in this figure that the humans prior is to always trust the eye so they would label all the points in this figure as a one and clearly there's two region where the human makes mistakes so this cluster of zero on the top right and this cluster of Z on the bottom right and I've identified these two clusters so we have a kind of a discovery algorithm to find these clusters and I really want to say is like human look at these two clusters you're making mistakes you should ignore the eye on these regions and kind of the way I'm going to communicate that to them is by describing these clusters so we're going to take each of these regions where the human collaborates incorrectly with their eye and describe them with a natural language rule so I'm going to assume each example can be described individually and my problem is to go from single descriptions to describing a region if I want to describe a region of points it's not enough to describe what's inside the region because I have to figure out where does the region start and stop I have to figure out its boundary so I need to contrast what's inside the region versus what's outside inside the region so what I'm going to do is I'm going to take a random sampling of points from inside the region so the set S Plus and sampling from outside the set s minus I'm going to ask a language model to contrast these two two sets and I'm going to come up with a description so that's my initial description this region consists of Highways during the night but because the language model cannot fit all of the points in the region and what's outside in its context and the clever way to select that set s+ and S minus that that's not just random so my procedure is going to be exertive I'm going to start with something random and then I'm going to continuously improve on it and the way I'm going to do this is as follows I'm going to take this initial description so this description in Gold I'm going to plug it into my embedding space once I plug it into my embedding space I can try to find counter examples to that description so that region description should match everything inside and should not match everything outside and so in theory if there's a point outside the region that has very close embedding similarity with it I should ask the model to make sure to contrast to that point so at point in red I'm showing you and if there's a point inside that region but that's has very similarity with that description I should tell the model to make sure to describe that point in particular so that point in green and so I can add these two points to my sets and then ask the model to describe again so I get a better description that's better refined so now actually the region is high during the night but without cars and I can keep doing this alterative procedure until I reach the maximum of my context L and so we have one synthetic experiment that I'm showing you here with the DAT set Ms Coco so a DAT set of images and captions and there's objects in that data set from over 100 classes and I'm going to try to describe each class on its own so for example I want to describe the set of points that have set of examples that have an apple inside of them you know usually the scenes that contain an apple also contain an orange but really the description I want to find is Apple exactly so that's my metric I want to I have the description which is apple I want to find that exact description of the region and so here I'm showing with a Rouge metric for captioning if I don't do any contrasting I get 25% if I add contrasting I'm at 32% if I cleverly select the examples not just randomly but with this counter examples I can boost performance further more by 3% kind of have we now have this clever procedure to figure out how to describe regions by cleverly selecting which points should the model pay attention to so at this point we have our regions and each region has a description and we have points from each of these regions what's left is to teach them to the person and so the way we're going to teach te these guidelines is we're going to take an example from each region show it to the person the person's going to make a prediction on that example then we're going to tell them if they were right or wrong and what should have been their decision on that example and what's the description of this region so we're going to take an example and help them journalize from example to region and so we have kind of these SK lines that we've created and so we did a user study on that object detection traffic light detection task where some people received on boing and some people did not in a random manner so back to kind of these initial figures I was showing you we can Benchmark Human Performance without AI we can Benchmark AI performance and it's kind of at the same level of the human at 79% is the human augmented with the eyes device but without on boarding is at 77% so it does not exceed the human or the eye alone but now once you do onboarding the you can boost the performance of The Human by 5% points and that increases setly significant and it's not only because of exposure to the task because in the control we also have the human interact with the model before we you know test them but it's really because of these rules that they've learned in this onboarding phase and so kind of the takeaways here is that we can train the human when to trust AI via these natural language rules and can improve their performance um can I ask another question here yes so you said in the control the human got to uh interact with the AI yes so they would just get predictions and then tell you tell them if they were right or wrong and it was random if effectively yeah so I you know I expose them to the same number of examples but without telling them the region descriptions so I have them you know practice in the beginning and but this examples are chosen randomly we also have a Bas where they're chosen a bit more smartly with kind of um a lime way of choosing examples you know to describe the eyes errors and that's still not sufficient okay thanks um one one related question um an alternative to the to the named regions would be to give the human um some kind of Black Box predictor of when it ought to trust the AI more than it normally would or you know or less um with no interpretation just like you know this kind of image for some reason that's opaque is one where you should trust the AI more L do you think that would work great great question so we had another arm in this study where we did very something very similar to what you were mentioning but we also gave a reason why so we would tell them the users don't trust Ai and because the I observed to be behave badly on those examples and what we found is that that not help at all so that users took more time to make decision and they didn't do better than just without that recommendation and I think there were two reasons for that um which I'm hypo making a hypothesis about is that you're just confusing people like you're showing them they ey and then you're telling them oh don't trust theyi and so they you're making them doubt themselves and doubt you know what they are so they're just more confused so that's my primary I think reason why this didn't work in that specific experiment we did another task where we had it it was a different study where we had a question answering task you you have a question multiple choice answers and you had g3.5 tell you what the right answer was um once you enable G 3.5 to not just tell you its answer but explain it often what GPT will tell you it will give you a reasoning for its explanation and and would also tell you I don't know how to answer this question so so the basically the model refuses to answer the question and you can and by enabling the model to tell you itself that I don't know how to answer that enables you to know when to trust it better because you can verify that the model doesn't know what it's talking about because once you ask it to verify you know it tells you I don't know and so that mitigated any Improvement due to onboarding now what's the difference between that setting of question answering versus the object detection in question answering you can verify the explanation from thei easily at least when it tells you I don't know how to answer you just know not to trust it in the object detection setting often the the AI will give you like a bonding box okay but you can't verify the bonding box either because it's too blurry for you to figure out so you cannot easily verify the explanation of thei and so that's why you know these recommendations were not as helpful because they were not easily verifiable and just confused the user um so a better maybe interface that would work is just you know either hide or show the eyes enter based on that prediction does does that make a bit of sense yes yes it does thanks so let's take what doc suggested even further um and that's going to be the next part of my talk um so what if you know the human didn't have much time to look at all the examples and that happens a lot in content moderation or Healthcare triage can we still get a team that's better than either the AI or the human so can we still get an effective human team so the way we're going to do things now in this final part of talk is with something I call a deferral system it's going to be very much analogous to before but now ai is going to make decisions on our behalf so suppose we have our input X or just X right here we're going to build an i model that's going to be called the rejector r ofx on each example this rejector is going to make a decision either this patient should be routed to the radiologist h of Z so the rejector would say oh this person should go to the human it's going to defer that person to the human or this rejector is going to say actually let my classifier predict so my classifier m ofx is going to make a decision and it's going to make a final decision Y and it's going to compare that to a ground truth so here the ejector is analogous to that integrator but here it's a i decision so for each patient X they're only seen by either the radiologist or the classifier so now we kind of have this similar you know intervention that I mentioned before but now the human is doesn't even see the example the I just makes decision on their behalf and kind of how do we build such a system where we have this optimal delegation and we have that classifier so I'm going to assume I have data that's a tuple of the following we have the input X the humans prediction H and the ground truth label Y and I want to optimize the loss function this loss of the feral of function of the classified m rejector R and it's going to compare what's the out what the output of the system to a ground Ro label y so if R is one it's going to be H if R is zero it's going to be M and compare that to a ground Ro label y the naive way to learn such a system is to train your classifier ignoring the human so as you do in machine learning and then for knowing when to defer threshold the confidence of that classifi to a threshold tow and say when that model is has low confidence let the human predict and there's two you know Sub aats in this approach one is that the rejector does not consider what the human confidence is so even though your model might have low confidence the human might have lower confidence and so you're not considering that so that's been pointing out BYU the second suboptimal thing is that the model does not adapt to the fact that it's only making prediction on a subset of the data since it's making prediction only a subset it should optimize its performance for the set of which the human is not very good at so it adapt to human weaknesses and so kind of to figure fix both of these things we need to jointly learn how to classify and differ I'm going to go over this bit quickly but basically if you were just going to take this objective function and optimize it it would be computationally hard even in the simplest simplest setting you can consider and that's not the same and that's kind of the setting where if you were doing just classification you could do it in polinomial time but if you're doing now joint learning it becomes computationally hard so now how do I practically learn such a system and the way I'm going to consider this problem is as follows instead of making a two State decision of defer classify I'm going to w a one stage decision of either predict class one class two class three or theer so I'm adding this extra class and I'm going to call it theer this is an instance of a more General problem called cost sensitive learning where for each input example I I have a vector of costs for each class I'm considering and basically the goal is to pick the class with the lowest cost so in the language of the feral pick whether it's less costly for the classier to predict or the human to predict and at the high level we show that any algorithm for cence of classification I can transform it into an algorithm for learning to defer and guarantee that minimize the objective we care about so basically you take your standard machine learning setup you have a neural network and has you know C output heads for C classes it's trained with cost entropy loss I can transform it by adding one more output head swapping out cross entropy loss for the learning to defer cross entropy loss and now I have a machine learning model that's human at work and defer optimate and kind of very quickly in experimental evaluation on this check s data sets we can show that by using this joint learning approach so this kind of last column we can get better accuracy than these previous baselines and better than their JS or they ey on their own by only using the human 50% of the time and so kind of the takeaway here is that incorporating the human into the machine learning process can really improve Downstream performance in these last two minutes I'm going to go over future directions and we can chat more about them and really all my future research going to focus how can I we enable you know AI um to help us make decisions every day and you the first problem is about Dynamics um how they should adapt from the humans feedback so you know I prev spoke about these myopic signals like accepting suggestions can we think about other feedback signals that can tell us about longer term outcomes the other side of the coin is humans adapting as they eye improves so again how do we ensure that their mental model is up to date so kind of this is a game between the human and the eye as they evolve over time the other side of things is about the interaction mechanism and so there's three opportunities that I want to explore first is AI that automatically steps in when it knows it's helpful the second is allowing the eye to figure out what's the hum intent without them explicitly writing down long text that tells they I what they want to do and enabling the human to control the generations with kind of these nice knobs instead of just text and I think this leads to a vision of developing you know agents that take actions rather than just advice and how do we build such agents how do we build groups of these agents to solve tasks and kind of the last part is about evaluating you know are we on the right track is this human team you know performing well in domains um I'm going to skip the slide which might be interesting um and kind of atantic scholar what I would Envision those directions looking like um as kind of trying to think of what would the I assistant for the research proc look like so very similar to cilet work how can think about assistants that maybe have augmented access to apis other you know knowledge bases that automatically step in as you're in the process multi-agent teams to help you in the writing or review process and kind of an agent observing you in the research process and making suggestions and trying to understand how can we incorporate human feedback and even social aspects of research into the model and kind of even for fun trying to think about how do we simulate the entire scientific process you know with these different AI agents in the community so I'm going to stop here and acknowledge um all my collaborators at MIT MSR IBM and tmu and thank you so much for your time today sorry that I went a bit overtime let's thank the speaker thanks a lot thank you um I've already asked a bunch of questions I have more but if i' I'd invite other people to step in please uh if you have any questions we have eight minutes okay I may sorry maybe I'll ask one although I agree with surgery be great to hear from other folks um can you go back to slide 17 really quick 17 yes okay so this this curve I mean it is good that you can kind of trade off how much you can hide with um you know that that you get better at it's as you hide less you those are more likely to be um to be rejected but I guess you know this curve doesn't bend that much right it's it's almost diagonal um and so for for a lot of problems you know you could get like 80% of the reduction in uh in suggestions at you know only 20% of the um you know loss of of good things um so like the 8020 rule I'm not sure I totally said that right but at this but this curve is almost diagonal right so it's um you know you're you're not getting points where you can actually hide 80% of the stuff and um and get you know uh a much smaller fraction hidden so I guess do are you happy with this curve or do you think you could drive up the accuracy this or is there some reason why it's especially hard um so this curve is really so you know it's doing much much better than randomly hiding stuff so if I was to draw like what would the random line be it would be horizontal right um that's sorry vertical yeah not not horizontal vertical basically if I hide 30% it would hide you know I would still will be randomly so since 21% of suggestion accepted I would be on this kind of line so I'm doing significantly better than that now what's the opportunity to improve that curve the opportunity comes from incorporating the latent state so um so if you know I showed you these different states if I had these labeled data which I have for this specific study I could improve the accuracy of my model significantly better because I I know where the user like if you just went to the bathroom I know you're not going to accept the suggestion I could never figure that out because it's just unobservable so if I had this latent data then I can significantly improve my model and and we show that in kind of when we're evaluating on this specific subset of the data um so that's the most problemis thing Avenue for me at least does that make sense yeah yeah it does um and it's a good point that you're doing much better than the vertical line thanks um I don't see any hands up so I guess I'll proceed oh Joseph please sorry um can you get a 44 yeah and great talk by the way I really really enjoyed it um thank you yes so there's a lot of stuff here that I find very interesting yes um specifically about AI being Auto stepping in so I feel like that's both a machine learning problem and also an HCI problem yes especially when you have either a single user or multiple user with an agent um and then the other one that I was really interested about is this like interpretable knobs for control generation yes I guess I I just want to hear a little more about each of the two um what your thoughts on expl yeah so I'm starting to think about this problem of Auto stepping in so C is is a way to do that right like it's but it's a bit more simpler now I'm thinking about it with a chat agent so again in the context of writing code so I have my e that's open and I'm writing code and I have a chat model that's observing me writing code now there's two ways to design that agent that steps in the first one I'm trying to explore is that I try to predict what question you would have asked so I have a bunch of data where people people you know have I can see their code I can see what questions they're asking based on that code so I can try to predict what question they ask answer that question and then I need to figure out if it's useful to show that answer or not so that's one way to design such a system right another way to design it is to think about you know you have this code I can try to infer what's the task you're soling and then try to give you the suggestion that gets you as close as possible to solving the task that's a different way to design such a system um so those are kind of the design choices that we I'm trying to think about and then there's the other angle of like is it good to interrupt the person or not and what you know what data can we use to you know figure out what to optimally interrupt so that's about the stepping in that's kind of some concrete stuff I'm thinking about for the interpretable knobs that's that's something I've been trying to think about but finding it very hard honestly so the easiest setting of this is an image generation where you know you have a prompt you get an image and then sometimes for images there's very easy ways to control things like you know um you know if you ask for an image um you know that's a blend of two artists right you can imagine there's a slider that tells you oh you want it to be more like this artist or that artist and what does it look like for text or code you know for code you can think of it make it more documented less mented so but the idea is to create these knobs and on instance basis not having like prefixes and how can we get to that Vision um so I don't have very concrete ideas but that's kind of the realm of things I'm thinking about there I great I would love to J more Jonathan thank you thank you j i you also really enjoyed the talk I I was wondering about the um the work where you were modeling the internal states of the human and it seemed like that involved a lot of manual labeling um but it would be cool to be able to deploy agents in like new domains that you know without having that step just wondering um on your thoughts on how to how to reduce the cost of that yeah that's a great question so in that Copo study we had people self-label their data because I don't know what their intent is so they would code and then we designed like a video labeling tool and it's kind of you know easy navigated there shortcuts it's kind of pretty quick to do this for like a 30 minute session um and the the hardest thing but which is made easy in copal is there's natural fragments so there's natural like scenes you need to label which is basically when does it interrupt you and when Once something is shown um to make it faster uh I had two directions which is one of them is uh a decoding problem so maybe if I have this small data I can good job at decoding maybe not a good job at prediction and these are two different things because decoding I can see the future and then the other one is using kind of weak labels so I can develop heris stics like I cix my taxonomy I build a set of heris sticks for some of the labels and that you know labels for me 5% of data and then basically I only ask the user like when you know the you know after I build my my model from week labels if there's segment I'm un sure about so that can speed up labeling a bit uh I'm not sure where else to make it faster like we can create keyboard shortcuts for you know specific things we care about but that's the realm of things I've considered thanks yeah also wondering if if you could do it without any human uh labeling at all but thanks yeah that that would be the weak labels basically that you htics you would develop but still human somewhere I guess um looks like we're at time here um well uh let's thank the speaker again thanks for the excellent talk really appreciate it thank you so much thank you Hussein