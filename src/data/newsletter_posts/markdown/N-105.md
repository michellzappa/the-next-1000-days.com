# 105

#

It is difficult to picture how different AIs display biases in their output data.

We assume our models to be neutral, yet itâ€™s trivial to pinpoint examples of forced behavior, like Golden Gate Claude, the purpose-built variant of Anthropicâ€™s assistant that [smuggled](https://venturebeat.com/ai/anthropic-tricked-claude-into-thinking-it-was-the-golden-gate-bridge-and-other-glimpses-into-the-mysterious-ai-brain/) San Franciscoâ€™s iconic bridge ðŸŒ‰ into almost every reply, regardless of topic. Biases go unnoticed.

Spotting these traits in models is challenging, and we have only recently started developing the skills necessary to notice [biases](https://www.envisioning.io/vocab/bias) when we interact with AIs.

Last week Grok, an AI model created by xAI, began shoe-horning references to a conspiratorial â€œwhite genocideâ€ in South Africa into unrelated conversations. The [episode](https://www.theguardian.com/technology/2025/may/18/musks-ai-bot-grok-blames-its-holocaust-scepticism-on-programming-error), blamed on a rogue prompt injection, is a textbook reminder that a modelâ€™s worldview can be bentâ€”sometimes clumsilyâ€”by anyone with access to its levers. ï¿¼

Such mishaps are not mere curiosities.

LLMs are trained on oceans of text, but each ocean is charted differently.

OpenAIâ€™s GPT is steeped in Anglo-American discourse; Googleâ€™s Gemini is tethered to the live web; Anthropicâ€™s Claude is tuned to err on the side of safety; Mistralâ€™s models reflects a more European palette; Grok is marinated in the brash vernacular of X; Metaâ€™s Llama rests on open-source ideals.

Ask the same question and their answers diverge like newspaper editorials. In that divergence lie both risk and opportunity. Sensible strategist therefore treat models like any other asset: diversify, compare, rebalance.

**Signals** , our ensemble AI research tool, does just that: running identical prompts through multiple engines, stripping out duplicates, and identifying coherence between their answers.

Agreement across models raises confidence; conflict highlights implicit assumptions. The result is not a single synthetic truth but a spectrum of possibilities that can be weighed by your judgment.

For foresight and innovation teams, the question is not how to police the models but how to press them into service today.

Signals lets you assemble an overview of multiple AIs in a few clicks: choose a scan type, input your organization, region and hit Generate.

Within minutes the app collates, de-duplicates and visualizes the data, handing you a briefing of emerging signals ready for scenario work, design sprints or that afternoonâ€™s strategy meeting.

If foresight is the art of thinking several futures at once, nothing aids the craft more than a chorus of mutually biased AIs. The trick is to keep them arguing long enough for the truth, or something close to it, to emerge.

We are rolling out access to our platform to foresight and innovation practitioners through a series of quick demos. The next one is happening next Thursday **[May 29](https://lu.ma/guvdnlau)**. Join us for a showcase of the platform, the reasoning behind it, and to receive a custom scan for any organization or industry you want insight on.

Until next week,
MZ

* * *

#### Insightful interview with Altman (30 min)

[Donâ€™t miss the Q&A](https://youtu.be/ctcMA6chfDY).

> _I mean, in some sense, I think the Platonic ideal state is a very tiny reasoning model with a trillion tokens of context that you put your whole life into. The model never retrains; the weights never customize. But that thing can, like, reason across your whole context and do it efficiently. And every conversation youâ€™ve ever had in your life, every book youâ€™ve ever read, every email youâ€™ve ever readâ€”everything youâ€™ve ever looked atâ€”is in there, plus all your data from other sources. And, you know, your life just keeps appending to the context, and your company just does the same thing for all your companyâ€™s data. We canâ€™t get there today, but I think of anything else as a compromise off that Platonic ideal, and that is how I would eventuallyâ€”I hopeâ€”we do customization._

* * *

#### Act like a Cold War era Russian Olympic Judge

Excellent interview with Nicholas Thompson of The Atlantic. This whole conversation is gold. Don't miss it.

* * *

#### AGI by 2030? (60 min)

[Amazing overview](https://youtu.be/-sk6_HFYM8c) by Benjamin Todd. [Article version](https://80000hours.org/agi/guide/when-will-agi-arrive/).

* * *

#### Deep Research (9 min)

Continuing the excellent series of talks from Sequoia, [Isa Fulford from OpenAI](https://youtu.be/jFZ9hJKJKtw) explains their Deep Research product.

* * *

* * *

* * *

#### Quick Links from the community

* [NYT is not convinced AGI is imminent](https://www.nytimes.com/2025/05/16/technology/what-is-agi.html?unlocked_article_code=1.Hk8.Z2GG.YBdZmoJs_RG&smid=url-share).

* [Semantic calculator](https://calc.datova.ai/): a smart way of learning about embeddings and word vectors.

* [Coders are the new managers](https://x.com/OpenAI/status/1923416746150523221).

* [This is the equivalent of doing search engine optimization in the early 2000s before everyone else caught on](https://www.wsj.com/articles/walmart-is-preparing-to-welcome-its-next-customer-the-ai-shopping-agent-6659ef18). Prepare your infra for agents. (WSJ)

* * *

* * *

* * *

* * *